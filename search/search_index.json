{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"punkst","text":""},{"location":"#pixel-level-factor-analysis","title":"Pixel level factor analysis","text":"<p>The punkst toolkit provides a pipeline for efficient pixel level factor analysis, which achieves the same result as FICTURE (2024) with improved efficiency. (Previous python package is still functioning.)</p> <p>Check the quick start page for help on generating the full workflow using Makefile or running each command step by step.</p> <p>Check the Input for help on preparing your data. Current we have worked exampled for 10X Genomics Visium HD, Xenium, NanoString CosMx SMI, and Vizgen MERSCOPE data. We've also applied punkst to Seq-scope and Stereo-seq data, we are working on providing more information.</p>"},{"location":"#workflow-overview","title":"Workflow Overview","text":"<ol> <li>pts2tiles: Group pixels to tiles for faster processing</li> <li>tiles2hex: Group pixels into non-overlapping hexagons</li> <li>lda4hex: Run factorization on the hexagon data</li> <li>pixel-decode: Annotate each pixel with the top factors and their probabilities</li> <li>Visualization: Create a high resolution visualization of the results</li> </ol> <p>All analyses are parallelized and some steps need to write temporary files. Choose the number of threads and the temporary directory (must be empty or can be created) to match your system capabilities.</p> <p>Check the Modules for detailed documentation of each command.</p>"},{"location":"install/","title":"Install","text":""},{"location":"install/#installation-guide-for-punkst","title":"Installation Guide for punkst","text":"<p>This guide walks you through building punkst on Linux and macOS, including environments without root access.</p>"},{"location":"install/#build","title":"Build","text":"<p>Prerequisites</p> <ul> <li>Git</li> <li>CMake: 3.15 to 3.23</li> <li>C++17 compiler* (GCC \u22658, Clang \u22655, MSVC 2017+)</li> <li>TBB, OpenCV</li> </ul> <p>*We do assume your compiler properly supports C++17. Consider updating your compiler if you encounter issues.</p> <pre><code># 1) Clone the repository\ngit clone --recursive https://github.com/your-org/punkst.git\ncd punkst\n# 2) Create and enter a build directory\nmkdir build &amp;&amp; cd build\n# 3) Configure\ncmake ..\n# 4) Build\ncmake --build . --parallel # or make\n</code></pre> <p>If you did not clone the submodule (Eigen) initially, you can do <pre><code>git submodule update --init\n</code></pre></p> <p>If TBB is not found, you can either install it yourself (see below) or add a flag <code>cmake .. -DFETCH_TBB=ON</code> to let cmake fetch and build it from oneTBB (which will take a while).</p> <p>If you installed some dependencies locally, you may need to specify their paths like <pre><code>cmake .. \\\n  -DOpenCV_DIR=$HOME/.local/lib/cmake/opencv4 \\\n  -DCMAKE_PREFIX_PATH=\"$HOME/.local\"\n  ```\n(On mac, if CMake fails to locate OpenCV (installed with brew), pass: `-DOpenCV_DIR=$(brew --prefix opencv)/lib/cmake/opencv4` or wherever OpenCV is installed.)\n\nThe `punkst` binary will be placed in `bin/` under the project root.\n\nVerifying the Build\n\n```bash\npunkst/bin/punkst --help\n</code></pre></p> <p>You should see a message starting with <pre><code>Available Commands\nThe following commands are available:\n</code></pre></p>"},{"location":"install/#required-libraries","title":"Required Libraries","text":"<ul> <li>TBB</li> </ul> <p>System: <code>sudo apt-get install libtbb-dev</code> or <code>yum install tbb-devel</code> on linux and <code>brew install tbb</code> on macOS.</p> <p>Local: <code>git clone</code> from oneTBB then build locally.</p> <ul> <li>OpenCV</li> </ul> <p><code>sudo apt-get install libopencv-dev</code>or <code>sudo yum install opencv-devel</code> on linux, <code>brew install opencv</code> on macOS. See OpenCV installation guide for more details on how to install from source.</p> <ul> <li>Other dependencies</li> </ul> Library Ubuntu / Debian CentOS / RHEL macOS (Homebrew) zlib <code>sudo apt-get install zlib1g-dev</code> <code>sudo yum install zlib-devel</code> <code>brew install zlib</code> BZip2 <code>sudo apt-get install libbz2-dev</code> <code>sudo yum install bzip2-devel</code> <code>brew install bzip2</code> LibLZMA <code>sudo apt-get install liblzma-dev</code> <code>sudo yum install xz-devel</code> <code>brew install xz</code>"},{"location":"blog/","title":"Blog","text":""},{"location":"input/","title":"Notes on processing data from different technologies/platforms","text":"<p>Here are instructions on how to convert raw data from different platforms to the generic input format for FICTURE</p> <p>Except for Visium HD (see below), we provide a pair of template <code>Makefile</code> and <code>config_prepare.json</code> files in the <code>examples/format_input</code> directory for each platform. You can copy <code>config_prepare.json</code> to your directory and modify the parameters, then generate the concrete <code>Makefile</code> by <pre><code>python /path/to/punkst/ext/py/generate_workflow.py \\\n  -t /path/to/punkst/examples/format_input/cosmx/Makefile \\\n  -c config_prepare.json -m Makefile\n</code></pre> then run <code>make</code>.</p> <p>After the conversion, you can follow the standard workflow as described in the quick start page to run the pipeline (and specify all coordinate/size related parameters in microns). For platforms that provide cell coordinates, we also extracted the cell centers and you can try the experimental workflow in <code>examples/with_cell_centers</code>.</p>"},{"location":"input/#visium-hd","title":"Visium HD","text":"<p>First we need to locate the \"binned output\" directory and the subdirectory with the original resolution. For the data downloaded from 10X website it is called <code>binned_outputs/square_002um</code>. Let's call it <code>RAWDIR</code>. In this directory, you should find the subdirecotries, <code>spatial</code> and <code>filtered_feature_bc_matrix</code> (I guess you could also use data in <code>raw_feature_bc_matrix</code> but I've not tried it yet).</p> <p>In the <code>spatial</code> directory, you should have a json file that contains the scaling factor of the coordinates, named as <code>scalefactors_json.json</code>. Let's grep the scaling factor (or set it manually). <pre><code>microns_per_pixel=$(grep -w microns_per_pixel ${RAWDIR}/spatial/scalefactors_json.json | perl -lane '$_ =~ m/.*\"microns_per_pixel\": ([0-9.]+)/; print $1' )\n#  \"microns_per_pixel\": 0.2737129726047599 in an example data\n</code></pre></p> <p>The spatial coordinates for each barcode are stored in <code>parguet</code> format, we do not support this format directly. Let's convert it to plain tsv file. duckdb seems fast and easy to use:</p> <pre><code>cd ${RAWDIR}/spatial/\nduckdb -c \"COPY (SELECT * FROM read_parquet('tissue_positions.parquet')) TO 'tissue_positions.tsv' (HEADER, DELIMITER '\\t');\"\n</code></pre> <p>Alternatively, you can try to use pyarrow and pandas in python.</p> <p>Next, punkst has a command to merge the 10X style dge files (and the spatial coordinates) into a single file as our standard input:</p> <pre><code>brc_raw=${RAWDIR}/spatial/tissue_positions.tsv # the one converted from parquet\nmtx_path=${RAWDIR}/filtered_feature_bc_matrix # path to the 10X style dge files\npunkst convert-dge \\\n--microns-per-pixel ${microns_per_pixel} \\\n--exclude-regex '^mt-' --in-tissue-only \\\n--in-positions ${brc_raw} \\\n--in-dge-dir ${mtx_path} \\\n--output-dir ${path} \\\n--coords-precision 4\n</code></pre> <p>Here the optional flag <code>--exclude-regex</code> takes a regular expression to exclude genes matching with the regex. In the above example we exclude all mitochondrial genes.</p> <p>The optional flag <code>--in-tissue-only</code> will exclude all barcodes that are labeled as not in the tissue.</p> <p>The command writes <code>transcripts.tsv</code> with coordinates in microns.</p>"},{"location":"input/#cosmx-smi","title":"CosMx SMI","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/cosmx</code> to conver CosMx raw output files to the generic input format. Alternatively, see the bash commands below. Copy the config file to your directory and set the raw file names. For example, here is an example for the public mouse half brain data:</p> <pre><code>{\n    \"workflow\": {\n      \"raw_tx\" : \"Run1000_S1_Half_tx_file.csv\",\n      \"raw_meta\": \"Run1000_S1_Half_metadata_file.csv\",\n      \"microns_per_pixel\": 0.12,\n      \"datadir\": \"/output/test\"\n    }\n  }\n</code></pre> <p>You can find <code>\"microns_per_pixel\"</code> in the ReadMe.html, it may say something like \"To convert to microns multiply the pixel value by 0.12 um per pixel\".</p> <p>The following are the commands ran in the <code>Makefile</code>:</p> <pre><code># Extract cell coordinates\ncut -d',' -f 7-8 ${RAW_META} | tail -n +2 | awk -F',' -v OFS=\"\\t\" \\\n    -v mu=${MICRONS_PER_PIXEL} \\\n    '{printf \"%.2f\\t%.2f\\n\", mu * $1, mu * $2 &gt; out;}' &gt; cell_coordinates.tsv\n\n# Extract transcripts\nawk -F',' -v mu=${MICRONS_PER_PIXEL} '\\\nNR==1{gsub(/\"/, \"\", $0); print $3, $4, $8, \"count\", $7, $9 }\\\nNR&gt;1{gsub(/\"/, \"\", $8); gsub(/\"/, \"\", $9); printf \"%.2f\\t%.2f\\t%s\\t%d\\t%d\\t%s\\n\", mu*$3, mu*$4, $8, 1, $7, $9 } ' ${RAW_TX} &gt; transcripts.tsv\n</code></pre>"},{"location":"input/#merscope","title":"MERSCOPE","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/merscope</code> to conver MERSCOPE raw output files to the generic input format.</p> <p>Set <code>\"rawdir\"</code> to be the path that contains the MERSCOPE output files. We will need the following files: <code>cell_metadata.csv.gz</code> and <code>detected_transcripts.csv.gz</code>. If your data is compressed, set <code>\"compressed\"</code> to 1, otherwise (plain csv) set it to 0. Set <code>\"datadir\"</code> to the output directory.</p> <p>The following are the commands ran in the <code>Makefile</code>:</p> <pre><code># Extract cell coordinates\nzcat ${RAWDIR}/cell_metadata.csv.gz | cut -d',' -f 4-9 | tail -n +2 | awk -F',' -v OFS=\"\\t\" '{ print $1, $2; }' &gt; cell_coordinates.tsv\n# Extract transcripts\nzcat ${RAWDIR}/detected_transcripts.csv.gz \\\n  | cut -d',' -f2-5,9 \\\n  | sed \\\n      -e '0,/barcode/{s/barcode/#barcode/}' \\\n      -e 's/,/\\t/g' \\\n      -e 's/$/\\t1/' \\\n      -e '0,/barcode/{s/\\t1$/\\tcount/}' \\\n    &gt; transcripts.tsv\n</code></pre>"},{"location":"input/#xenium","title":"Xenium","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/xenium</code> to conver Xenium raw output files to the generic input format.</p> <p>In the <code>config.json</code>, you need specify <code>\"raw_transcripts\"</code> as the path of the transcript file <code>transcripts.csv.gz</code> and <code>\"raw_cells\"</code> as the path of the cell metadata <code>cells.csv.gz</code>.</p> <p>The following are the commands ran in the <code>Makefile</code>:</p> <pre><code># Extract transcripts\nzcat transcripts.csv.gz \\\n  | cut -d',' -f4-6 | sed 's/\"//g' \\\n  | awk -F',' -v OFS=\"\\t\" '{ print $2, $3, $1, \"1\" }' \\\n  &gt; transcripts.tsv\n\n# Extract cell coordinates\nzcat cells.csv.gz \\\n  | cut -d',' -f2-3 \\\n  | tail -n +2 \\\n  | awk -F',' -v OFS=\"\\t\" '{printf \"%.4f\\t%.4f\\n\", $1,$2;}' &gt; cell_coordinates.tsv\n</code></pre>"},{"location":"modules/","title":"Punkst Modules","text":"<p>Punkst provides several command-line tools for analyzing high resolution spatial (transcriptomics) data. Each module can be used individually or as part of a pipeline.</p>"},{"location":"modules/#available-modules","title":"Available Modules","text":"<ul> <li>pts2tiles: Groups pixels to tiles for faster processing</li> <li>tiles2hex: Groups pixels into non-overlapping hexagons for spot level analysis</li> <li>lda4hex: Runs LDA on the spot level data</li> <li>pixel-decode: Annotates each pixel with the top factors and their probabilities</li> <li>Visualization: Visualizes the pixel level analysis results</li> </ul>"},{"location":"modules/#input-data-format","title":"Input Data Format","text":"<p>The input is a tsv file with the following columns: X, Y, feature, count. Whether the file contains headers or other columns is not relevant, as long as the above four columns are present.</p> <ul> <li> <p>X, Y coordinates can be either integer or float vlaues. (If your coordinates are integers and you would like to keep the original coordinates in the pixel level inference output, set <code>--coords-are-int</code> in <code>punkst pixel-decode</code>). The coordinates can be in arbitrary units, just make sure all scale/size related parameters you later provide should be in the same unit.</p> </li> <li> <p>\"feature\" can be a string or a nonnegative integer corresponding to the index in a feature list.</p> </li> <li> <p>\"count\" is a nonnegative integer. You could apply gene-specific non-negative real valued weights to the count later in analysis.</p> </li> </ul>"},{"location":"modules/lda4hex/","title":"lda4hex","text":"<p><code>lda4hex</code> runs LDA on the hexagon data.</p>"},{"location":"modules/lda4hex/#input-format","title":"Input format","text":"<p>The input data is a plain text file where each line containing the sparse encoding of the gene counts for one unit (hexagon, cell, etc.). The orders of the units should be randomized.</p> <p>If generated hexagon data from <code>tiles2hex</code> you probably don't need to know the following details.</p>"},{"location":"modules/lda4hex/#required-data","title":"Required data","text":"<p>The required structure of each line is as follows (entries are separated by tabs): - one integer (m) for the number of unique genes in this unit - one integer for the total count of all genes in this unit - followed by m pairs of integers, each pair consisting of a gene index (0-based) and the count of that gene in this unit (separated by a single space). In a cell-by-gene count matrix, the pairs are the (column, value) pairs of all non-zero entries in one row corresponding to a cell.</p> <p>There could be other fields in the input before the above required (m+2) fields, the number of data fields before the required fields should be specified under the key \"offset_data\" in the json metadata file.</p>"},{"location":"modules/lda4hex/#required-metadata","title":"Required metadata","text":"<p>We require a json file with at least the following information: - \"dictionary\": a dictionary that contains key: value pairs where each key is a gene name and each value is the corresponding index of that gene in the sparse encoding in the input data file. (You could skip this dictionary if you provides all and only the present genes' information in the order consistent with the indices (the column names and column sums in a cell-by-gene matrix) by <code>--features</code> in <code>lda2hex</code> (see below)) - \"offset_data\": an integer that specifies the number of fields before the required fields in the input data file. - \"header_info\": a list of size <code>offset_data</code> that contains the names of the fields before the required fields in the input data file. We will carry over these fields to the output files.</p>"},{"location":"modules/lda4hex/#usage","title":"Usage","text":"<pre><code>punkst lda4hex --in-data ${path}/hex_12.randomized.txt --in-meta ${path}/hex_12.json \\\n--n-topics 12 --out-prefix ${path}/hex_12 --transform \\\n--min-count-train 50 --minibatch-size 512 --threads ${threads} --seed 1\n</code></pre>"},{"location":"modules/lda4hex/#required","title":"Required","text":"<p><code>--in-data</code> - Specifies the input data file (created by <code>tiles2hex</code> then shuffled).</p> <p><code>--in-meta</code> - Specifies the metadata file created by <code>tiles2hex</code>.</p> <p><code>--n-topics</code> - Specifies the number of topics to learn.</p> <p><code>--out-prefix</code> - Specifies the prefix for the output files.</p>"},{"location":"modules/lda4hex/#optional","title":"Optional","text":""},{"location":"modules/lda4hex/#feature-filtering","title":"Feature Filtering","text":"<p><code>--features</code> - Required and used only when either of the following three parameters are specified. Path to a file where the first column contains gene names and the second column contains the total count of that gene.</p> <p><code>--min-count-per-feature</code> - Minimum total count for features to be included. Require <code>--features</code> to be specified. Default: 1.</p> <p><code>--include-feature-regex</code> - Regular expression (modified ECMAScript grammar) to include only features matching this pattern. Default: include all features.</p> <p><code>--exclude-feature-regex</code> - Regular expression (modified ECMAScript) to exclude features matching this pattern. Default: exclude no features.</p> <p>Feature Selection Logic: the above three filters are applied jointly, so only genes with at least the minimum count, matching the include regex (if provided), and not matching the exclude regex (if provided) will be included in the model.</p>"},{"location":"modules/lda4hex/#feature-weighting","title":"Feature Weighting","text":"<p><code>--feature-weights</code> - Path to a file containing a weight for each gene. Format should be gene name (first column) and weight (second column). If the json metadata file does not contain a dictionary, the first column should be the gene index (0-based) instead.</p> <p><code>--default-weight</code> - Default weight for features not present in the weights file. Set to 0 to ignore features not in the weights file. Default: 1.0.</p>"},{"location":"modules/lda4hex/#lda-training-parameters","title":"LDA Training Parameters","text":"<p><code>--threads</code> - Number of threads to use. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p> <p><code>--minibatch-size</code> - Size of the minibatches to use during training. Default: 512.</p> <p><code>--min-count-train</code> - Minimum total count for a hexagon to be included in the training set. Default: 20.</p> <p><code>--n-epochs</code> - Number of epochs to train for. Default: 1.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in the e-step in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--max-iter</code> - Maximum number of iterations for each document. Default: 100.</p> <p><code>--kappa</code> - Learning decay parameter for online LDA. Default: 0.7.</p> <p><code>--tau0</code> - Learning offset parameter for online LDA. Default: 10.0.</p> <p><code>--alpha</code> - Document-topic prior. Default: 1/K (where K is the number of topics).</p> <p><code>--eta</code> - Topic-word prior. Default: 1/K (where K is the number of topics).</p>"},{"location":"modules/lda4hex/#model-initialization","title":"Model Initialization","text":"<p><code>--model-prior</code> - File that contains the initial model matrix.</p> <p><code>--prior-scale</code> - Scale the initial model matrix uniformly by this value. Default: use the matrix as is.</p>"},{"location":"modules/lda4hex/#output-control","title":"Output Control","text":"<p><code>--transform</code> - Transform the data to the LDA space after training. If set, an output file <code>&lt;prefix&gt;.results.tsv</code> will be created.</p> <p><code>--projection-only</code> - Transform the data using the prior model without further training. Implies <code>--transform</code>.</p> <p><code>--verbose</code> - Control the verbosity level of output messages.</p>"},{"location":"modules/pixel-decode/","title":"pixel-decode","text":""},{"location":"modules/pixel-decode/#overview","title":"Overview","text":"<p><code>pixel-decode</code> takes a trained LDA model and tiled pixel-level data to annotate each pixel with the top factors and their probabilities. This module enables spatial mapping of gene expression patterns at single-pixel resolution.</p> <pre><code>punkst pixel-decode --model ${path}/hex_12.model.tsv \\\n--in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--temp-dir ${tmpdir} --out-pref ${path}/pixel.decode \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 \\\n--pixel-res 0.5 --threads ${threads} --seed 1 --output-original\n</code></pre> <p>The pixel-level inference result (in this case <code>${path}/pixel.decode.tsv</code>) contains the coordinates and the inferred top factors and their posterior probabilities for each pixel. The module also creates a pseudobulk file (<code>${path}/pixel.decode.pseudobulk.tsv</code>) where each row is a gene and each column is a factor.</p>"},{"location":"modules/pixel-decode/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - Specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> - Specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - Specify the columns with X and Y coordinates (0-based).</p> <p><code>--icol-feature</code> - Specifies the column index for feature (0-based).</p> <p><code>--icol-val</code> - Specifies the column index for count/value (0-based).</p> <p><code>--model</code> - Specifies the model file where the first column contains feature names and the subsequent columns contain the parameters for each factor. The format should match that created by <code>lda4hex</code>.</p> <p><code>--temp-dir</code> - Specifies the directory to store temporary files.</p> <p>Output specification - One of these must be provided: <code>--out-pref</code> - Specifies the output prefix for all output files.</p> <p><code>--out</code> - (Deprecated, for backward compatibility) Specifies the output file.</p> <p>Hexagon grid parameters - One of these must be provided: <code>--hex-size</code> - Specifies the size (side length) of the hexagons for initializing anchors.</p> <p><code>--hex-grid-dist</code> - Specifies center-to-center distance in the axial coordinate system used to place anchors. Equals <code>hex-size * sqrt(3)</code>.</p> <p>Anchor spacing parameters - One of these must be provided: <code>--anchor-dist</code> - Specifies the distance between adjacent anchors.</p> <p><code>--n-moves</code> - Specifies the number of sliding moves in each axis to generate the anchors. If <code>--n-moves</code> is <code>n</code>, <code>anchor-dist</code> equals <code>hex-grid-dist</code> / <code>n</code>.</p>"},{"location":"modules/pixel-decode/#optional-parameters","title":"Optional Parameters","text":""},{"location":"modules/pixel-decode/#input-parameters","title":"Input Parameters","text":"<p><code>--coords-are-int</code> - If set, indicates that the coordinates are integers; otherwise, they are treated as floating point values.</p> <p><code>--feature-is-index</code> - If set, the values in <code>--icol-feature</code> are interpreted as feature indices. Otherwise, they are expected to be feature names.</p> <p><code>--feature-weights</code> - Specifies a file to weight each feature. The first column should contain the feature names, and the second column should contain the weights.</p> <p><code>--default-weight</code> - Specifies the default weight for features not present in the weights file (only if <code>--feature-weights</code> is specified). Default: 0.</p> <p><code>--anchor</code> - Specifies a file containing anchor points to use in addition to evenly spaced lattice points.</p>"},{"location":"modules/pixel-decode/#data-annotation-parameters","title":"Data Annotation Parameters","text":"<p><code>--ext-col-ints</code> - Additional integer columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-floats</code> - Additional float columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-strs</code> - Additional string columns to carry over to the output file. Format: \"idx1:name1:len1 idx2:name2:len2 ...\" where 'idx' are 0-based column indices and 'len' are maximum lengths of strings.</p>"},{"location":"modules/pixel-decode/#processing-parameters","title":"Processing Parameters","text":"<p><code>--pixel-res</code> - Resolution for the analysis, in the same unit as the input coordinates. Default: 1 (each pixel treated independently). Setting the resolution equivalent to 0.5-1\u03bcm is recommended, but it could be smaller if your data is very dense.</p> <p><code>--radius</code> - Specifies the radius within which to search for anchors. Default: <code>anchor-dist * 1.2</code>.</p> <p><code>--min-init-count</code> - Minimum total count within the hexagon around an anchor for it to be included. Filters out regions outside tissues with sparse noise. Default: 10.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--threads</code> - Number of threads to use for parallel processing. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p>"},{"location":"modules/pixel-decode/#output-parameters","title":"Output Parameters","text":"<p><code>--output-original</code> - If set, the original data including the feature names and counts will be included in the output. If <code>pixel-res</code> is not 1 and <code>--output-original</code> is not set, the output contains results per collapsed pixel.</p> <p><code>--use-ticket-system</code> - If set, the order of pixels in the output file is deterministic across runs (though not necessarily the same as the input order). May incur a small performance penalty.</p> <p><code>--top-k</code> - Number of top factors to include in the output. Default: 3.</p> <p><code>--output-coord-digits</code> - Number of decimal digits to output for coordinates (only used if input coordinates are float or <code>--output-original</code> is not set). Default: 4.</p> <p><code>--output-prob-digits</code> - Number of decimal digits to output for probabilities. Default: 4.</p> <p><code>--verbose</code> - Increase verbosity of output messages.</p> <p><code>--debug</code> - Enable debug mode for additional diagnostic information.</p>"},{"location":"modules/pixel-decode/#process-multiple-samples","title":"Process multiple samples","text":"<p>If you want to project the same model onto multiple datasets/samples, you can use <code>--sample-list</code> to pass a tsv file containing all samples' information. (In this case, <code>--in-tsv</code> and <code>--in-index</code> are ignored.) Note: all other parameters are shared across samples, so the input files should have the same structure.</p> <p>If your multi-sample data are generated by <code>punkst multisample-prepare</code>, it has created a file named <code>*.persample_file_list.tsv</code>. You can just pass this file to <code>--sample-list</code> and optionally use <code>--out-pref</code> to specify an identifier (e.g., the model information) to add to each output file name.</p> <p>If you created the input file for each sample manually, you can create a tsv file with at least three columns: sample_id, path to the transcript file created by <code>pts2tiles</code>, path to the index file created by <code>pts2tiles</code>.</p> <p>Optional fourth column: output prefix.</p> <p>Optional fifth column: anchor file path.</p> <p>If there are headers, all header lines should start with \"#\".</p>"},{"location":"modules/pixel-decode/#output-files","title":"Output Files","text":"<p>The following files are generated (with prefix specified by <code>--out-pref</code>):</p> <p><code>&lt;prefix&gt;.tsv</code> - Main output file with pixel-level factor assignments</p> <p><code>&lt;prefix&gt;.pseudobulk.tsv</code> - Gene-by-factor matrix showing feature distribution across topics</p>"},{"location":"modules/pixel-decode/#example-usage-scenarios","title":"Example Usage Scenarios","text":""},{"location":"modules/pixel-decode/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8\n</code></pre>"},{"location":"modules/pixel-decode/#with-data-annotations","title":"With Data Annotations","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8 \\\n--ext-col-ints 4:celltype 5:cluster --ext-col-strs 6:sample_id:20 --output-original\n</code></pre>"},{"location":"modules/pixel-decode/#with-multiple-input-created-by-punkst-multisample-prepare","title":"With multiple input created by <code>punkst multisample-prepare</code>","text":"<pre><code>punkst pixel-decode --model model.tsv --sample-list multi.persample_file_list.tsv \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8\n</code></pre>"},{"location":"modules/pts2tiles/","title":"pts2tiles","text":""},{"location":"modules/pts2tiles/#group-pixels-to-tiles-for-faster-processing","title":"Group pixels to tiles for faster processing","text":"<p><code>pts2tiles</code> creates a plain tsv file that reorders the lines in the input file so that coordinates are grouped into non-overlapping square tiles. The ordering of lines within a tile is not guaranteed. It also creates an index file storing the offset of each tile to support fast access.</p> <p>Example usage <pre><code>punkst pts2tiles --in-tsv ${path}/transcripts.tsv \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 0 --tile-size 500 \\\n--temp-dir ${tmpdir} --out-prefix ${path}/transcripts.tiled --threads ${threads}\n</code></pre></p>"},{"location":"modules/pts2tiles/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - The input TSV file containing spatial data points. It could be a gzipped file ending with <code>.gz</code>. If the input is a stream from stdin, use <code>-</code> as the filename.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - The column indices for X and Y coordinates (0-based).</p> <p><code>--tile-size</code> - The size (side length) of the square tiles. The unit is the same as the coordinates in the input file.</p> <p><code>--out-prefix</code> - The prefix for all output files.</p> <p><code>--temp-dir</code> - The directory for storing temporary files during processing.</p> <p><code>--icol-feature</code> - The column index for feature names/IDs (0-based). If provided, the module will generate a file including feature names and counts. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features.)</p> <p><code>--icol-int</code> - Column indices for integer values to aggregate per feature. Can be specified multiple times to track multiple integer columns. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features, preferably excluding the extremely low count features.)</p>"},{"location":"modules/pts2tiles/#optional-parameters","title":"Optional Parameters","text":"<p><code>--skip</code> - The number of lines to skip in the input file (if your input file contains headers, set it to the number of header lines). Default: 0.</p> <p><code>--tile-buffer</code> - The per-thread per-tile buffer size in terms of the number of lines before writing to disk. Default: 1000. If the number of tiles may be huge and you are using a large number of threads so that the total memory usage is too high, choose a smaller number.</p> <p><code>--threads</code> - The number of threads to use for parallel processing. Default: 1.</p> <p><code>--verbose</code> - Controls the verbosity level of output messages.</p> <p><code>--debug</code> - Enables additional debug output.</p>"},{"location":"modules/pts2tiles/#output-files","title":"Output Files","text":"<ul> <li><code>prefix.tsv</code>: the tiled tsv file.</li> <li><code>prefix.index</code>: an index file that stores the offsets of each tile in the tiled tsv file. This will be used for fast access.</li> <li><code>prefix.coord_range.tsv</code>: a text file that contains the range of coordinates (xmin, xmax, ymin, ymax).</li> <li><code>prefix.features.tsv</code>: a tsv file containing the feature names and their aggregated values. This file is only generated if <code>--icol-feature</code> is specified.</li> </ul>"},{"location":"modules/tiles2hex/","title":"tiles2hex","text":"<p><code>tiles2hex</code> groups pixels into nonoverlapping hexagons for spot level analysis.</p> <p>The input is the tiled data created by <code>pts2tiles</code>. The output is a plain tab-delimited text file, each line representing one hexagon intended for internal use. It also writes metadata to a json file.</p>"},{"location":"modules/tiles2hex/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--feature-dict ${path}/features.txt \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n--min-count 20 --hex-grid-dist 12 \\\n--out ${path}/hex_12.txt --temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code>, <code>--icol-feature</code> specify the column indices corresponding to X and Y coordinates and feature (0-based).</p> <p><code>--icol-int</code> specifies the column index for count/value (0-based). You can specify multiple count columns with <code>--icol-int</code>, separated by space.</p> <p><code>--hex-size</code> specifies the side length of the hexagons. The unit is the same as the coordinates in the input file.</p> <p><code>--out</code> specifies the output file.</p>"},{"location":"modules/tiles2hex/#optional-parameters","title":"Optional Parameters","text":"<p><code>--feature-dict</code> specifies a file with the names of features, one per line. It is used only if the values in <code>--icol-feature</code> are to be interpreted as feature names not indices. Features not present in the file will be ignored. (If the input file contains feature indices instead of names, all features will be included in the output)</p> <p><code>--min-count</code> specifies the minimum count for a hexagon to be included in the output.</p> <p><code>--randomize</code> if set, the order of hexagons in the output will be randomized.</p> <p><code>--temp-dir</code> specifies the directory for temporary files.</p> <p><code>--threads</code> specifies the number of threads to use.</p>"},{"location":"modules/tiles2hex/#output-format","title":"Output Format","text":"<p>The output is a plain tab-delimited text file. It is not a table: each line contains data for one unit and lines have different number of tokens.</p> <p>The first element in each line of the output is a random key, which can be used to shuffle the data before model training. When <code>--randomize</code> is not set when you run <code>tiles2hex</code>, you can do the following <pre><code>sort -k1,1 --parallel ${threads} -S 1G ${path}/hex.txt -o ${path}/hex.txt\n</code></pre> If you use <code>lda4hex</code>, you should always shuffle the ordering in the hexagon file.</p> <p>The remaining of each line is structured as follows:</p> <p>In the basic case, the next two integers after the random key are coordinates (horizontal and vertical) in the axial hexagonal coordinate system.</p> <p>The next 2K tokens (K pairs of non-negative integers) are the number of unique features (\\(M_k\\)) and the total count (\\(C_k\\)) for each modality. The number of modalities (K) is the same as the number of column indices specified in <code>--icol-int</code>.</p> <p>Then there are K chunks of feature values, the k-th chunk containing \\(2M_k\\) (or \\(M_k\\) values) of non-negative integers where \\(M_k\\) is what you read from the previous tokens. The first number in each pair is the indices of the feature, the second is the count of that feature in the hexagon. The indices are 0-based and correspond to the order of features in the <code>--feature-dict</code> file. If <code>--feature-dict</code> is not provided (so the input already codes features as indices), the indices are the same as those in the input file.</p>"},{"location":"modules/tiles2hex/#advanced-usage-spatial-stratification-by-anchor-points","title":"Advanced Usage: Spatial Stratification By Anchor Points","text":"<p><code>tiles2hex</code> can also create multiple sets of units that group pixels that are close to user-provided anchor points. This is useful for creating units stratified by known biological structures for downstream clustering or factorization. A tested use case is to provide nuclear centers as anchor points so likely-nuclear and likely-cytoplasmic pixels are grouped separately.</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index transcripts.tiled.index --feature-dict ${path}/features.txt --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --min-count 20 --hex-size ${hex_size} --anchor-files ${path}/anchors1.txt ${path}/anchors2.txt --radius ${radius1} ${radius2} --out ${path}/hex.txt --temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#additional-parameters-for-anchor-based-analysis","title":"Additional Parameters for Anchor-Based Analysis","text":"<p><code>--anchor-files</code> specifies one or more files containing anchor points. Each anchor file should contain coordinates (x, y) separated by space, one anchor point per line. You can provide multiple anchor files to define different sets of anchor points, separated by space.</p> <p><code>--radius</code> specifies the radius around each anchor point within which pixels will be associated with that anchor. The unit is the same as the coordinates in the input file. You must provide one radius value for each anchor file, in the matched order.</p> <p><code>--ignore-background</code> if set, pixels that are not within the radius of any anchor point will be ignored. By default, these background pixels are included as a separate layer.</p>"},{"location":"modules/tiles2hex/#output-format-for-anchor-based-analysis","title":"Output Format for Anchor-Based Analysis","text":"<p>The output format is similar to the basic usage, but each hexagon also includes a non-negative index as the second token, indicating which anchor set it belongs to. The metadata JSON file includes an additional integer field <code>n_layers</code> recording the number of layers, or the number of anchor sets used (plus one if background is included).</p>"},{"location":"modules/visualization/","title":"Visualization","text":""},{"location":"modules/visualization/#high-resolution-image-of-pixel-level-factorization-results","title":"High resolution image of pixel level factorization results","text":"<p><code>draw-pixel-factors</code> visualizes the results of <code>pixel-decode</code></p> <pre><code>punkst draw-pixel-factors --in-tsv ${path}/pixel.decode.tsv --header-json ${path}/pixel.decode.json --in-color ${path}/color.rgb.tsv --out ${path}/pixel.png --scale 100 --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre> <p><code>--in-tsv</code> specifies the input data file created by <code>pixel-decode</code>.</p> <p><code>--header-json</code> specifies the header created by <code>pixel-decode</code>.</p> <p><code>--in-color</code> specifies a tsv file with the colors for each factor. The first three columns will be interpreted as R, G, B values in the range \\(0-255\\). The valid lines will be assigned to factors in the order they appear in this file.</p> <p><code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code> specify the range of the coordinates.</p> <p><code>--scale</code> scales input coordinates to pixels in the output image. <code>int((x-xmin)/scale)</code> equals the horizontal pixel coordinate in the image.</p> <p><code>--out</code> specifies the output png file.</p> <p>If your specified <code>--transform</code> in <code>lda4hex</code>, one way to create the color table is to use the helper python script <pre><code>python punkst/ext/py/color_helper.py --input ${path}/prefix.results.tsv --output ${path}/color\n</code></pre></p>"},{"location":"modules/visualization/#pseudobulk-differential-expression-analysis","title":"Pseudobulk differential expression analysis","text":"<p><code>de_bulk.py</code> performs naive differential expression analysis on pseudobulk count data</p> <p>The script performs chi-square tests to identify genes that are significantly enriched in each factor compared to the background, outputting results with fold changes, p-values, and chi-square statistics.</p> <p>Note that the statistics are not calibrated when the pseudobulk table is generated by <code>punkst pixel-decode</code>, it is only meant for exploratory analysis.</p> <pre><code>python ext/py/de_bulk.py --input ${path}/pseudobulk.tsv --output ${path}/de_bulk.tsv --feature_label Feature --thread 4\n</code></pre> <p><code>--input</code> specifies the input pseudobulk count table with genes as rows and factors as columns. <code>punkst pixel-decode</code> generates one such file with suffix <code>pseudobulk.tsv</code>. This file has to have a header row with column names. It has to have one column with gene names and one column for each factor.</p> <p><code>--output</code> specifies the output file for differential expression results.</p> <p><code>--feature_label</code> specifies the column name for feature names (default: \"Feature\").</p> <p><code>--min_ct_per_feature</code> minimum total count for a feature to be included in analysis (default: 50).</p> <p><code>--max_pval_output</code> maximum p-value threshold for output (default: 1e-3).</p> <p><code>--min_fold_output</code> minimum fold change threshold for output (default: 1.5).</p> <p><code>--min_output_per_factor</code> minimum number of top genes to output per factor even if not significant (default: 10).</p> <p><code>--thread</code> number of threads for parallel processing (default: 1).</p> <p><code>--use_input_header</code> if specified, all columns except for the column named <code>--feature_label</code> will be viewed as factors and the column names will be preserved as factor IDs in the output. Otherwise (default), only the columns where the column names contain integers will be considered as factors and the numbers will be extracted to be used as factor IDs.</p> <p><code>--feature</code> optional file with features to restrict analysis to. The column containing feature names should have the column name as specified by <code>--feature_label</code>.</p>"},{"location":"modules/visualization/#html-report-for-factor-weights-and-top-genes","title":"HTML report for factor weights and top genes","text":"<p><code>factor_report.py</code> generates HTML reports summarizing factor characteristics and top genes</p> <p>The script generates an interactive HTML report (<code>${output_pref}.factor.info.html</code>) and a TSV summary (<code>${output_pref}.factor.info.tsv</code>) containing factor weights, top differentially expressed genes, and visualization colors.</p> <pre><code>python ext/py/factor_report.py --de ${path}/de_bulk.tsv --pseudobulk ${path}/pseudobulk.tsv --color_table ${path}/color.rgb.tsv --output_pref ${path}/report\n</code></pre> <p><code>--de</code> specifies the differential expression results file from <code>de_bulk.py</code>.</p> <p><code>--pseudobulk</code> specifies the pseudobulk count table.</p> <p><code>--color_table</code> specifies the RGB color table for factors. This probably should be the same file as that used for <code>punkst draw-pixel-factors</code>.</p> <p><code>--output_pref</code> specifies the output prefix for generated files.</p> <p><code>--feature_label</code> specifies the column name for features (default: \"Feature\").</p> <p><code>--n_top_gene</code> maximum number of top genes to include in report (default: 20).</p> <p><code>--min_top_gene</code> minimum number of top genes to show per factor (default: 10).</p> <p><code>--max_pval</code> maximum p-value threshold for significant genes (default: 0.001).</p> <p><code>--min_fc</code> minimum fold change threshold for significant genes (default: 1.5).</p> <p><code>--annotation</code> optional file with factor annotations to display instead of the factor IDs. It is a tsv file where the first column contains factor IDs as appear in the header of the pseudobulk table, and the second column contains the annotation.</p> <p><code>--anchor</code> optional file with anchor genes chosen to represent each factor. It is a tsv file where the first column contains factor IDs as appear in the header of the pseudobulk table, and the second column contains the anchor gene names (separated by things other than tabs).</p>"},{"location":"workflows/","title":"Pixel Level Factor Analysis Example","text":"<p>This example demonstrates how to perform pixel level factor analysis with punkst, achieving similar results to FICTURE (2024) with improved efficiency.</p> <p>We will explain how to generate a full workflow using a template Makefile, then explains the steps taken inside the workflow.</p>"},{"location":"workflows/#generic-input-format-and-example-data","title":"Generic input format and example data","text":"<p>There is a small example data <code>transcripts.tsv.gz</code> in <code>punkst/examples/data</code>.</p> <p>See Input for details on starting from raw data from different platforms.</p> <p>We only need one input file storing the pixel/transcript information: a TSV file with X coordinate, Y coordinate, feature, and count columns. (<code>example_data/transcripts.tsv.gz</code>)</p> <p>It can be gzipped or uncompressed. It can have other columns (which will be ignored in analysis but can be optionally carried over to the pixel level output), and it may or may not have headers (see Step 1). We will only use this file directly in step 1.</p>"},{"location":"workflows/#use-the-example-makefile-template","title":"Use the example Makefile template","text":"<p>We provide a template Makefile and config file in <code>punkst/examples</code> to generate the full workflow of FICTURE.</p> <p>You can copy <code>punkst/examples/basic/config.json</code> to your own directory modify the data path and parameters, then use <code>punkst/ext/py/generate_workflow.py</code> to generate a data-specific Makefile for your task.</p> <p>The python script also generates a bash script that can be submitted as a slurm job. If you are not using slurm just ignore the parameters in the \"job\"  section of the config and run the generation script without the <code>-o</code> option.</p> <pre><code># set repopath to the path of the punkst repo\npython ${repopath}/ext/py/generate_workflow.py \\\n  -c config.json -o run.sh -m Makefile \\\n  -t ${repopath}/examples/basic/Makefile\n</code></pre> <p>You can check the generated workflow before execution by <pre><code>make -f Makefile --dry-run\n</code></pre></p> <p>Then <code>make -f Makefile</code> exectutes the workflow.</p>"},{"location":"workflows/#parameters-in-configjson","title":"Parameters in config.json","text":"<p>(The parameters in the example config file works for the example data, where the coordinates are in microns.)</p> <p><code>\"datadir\"</code>: the path to store all output</p> <p><code>\"tmpdir\"</code>: the path to store temporary files (those files will be deleted automatically by the program). This directory must be empty or creatable.</p> <p><code>\"transcripts\"</code>: a tsv file with X coordinate, Y coordinate, gene/transcript name, and count columns. There could be other columns but they will be ignored.</p> <p>Specify the 0-based column indices in \"transcripts\" for X coordinate, Y coordinate, feature, and count: <code>\"icol_x\"</code>, <code>\"icol_y\"</code>, <code>\"icol_feature\"</code>, and <code>\"icol_count\"</code>. If the input file contains headers, set \"skip\" to the number of lines to skip.</p> <p><code>\"exclude_feature_regex\"</code>: a regular expression to exclude features from the analysis. For example, to exclude negative control probes and/or mitochondrial genes.</p> <p><code>\"tilesize\"</code>: we store and process data by square tiles, this parameter specifies the size length of the tiles in the same unit as your coordinates. Tile sizes affect the memory usage and (perhaps less so) run time, we've been using 500\\(\\mu\\)m for all of our experiments.</p> <p><code>\"hexgrids\"</code> (list): this is center-to-center distance of the hexagonal grid used for training the model. The best value depends on your data density. We've been using \\(12\\sim 18\\mu m\\) for most dataset, but you might want to use a larger value if your data has very low transcript density.</p> <p><code>\"topics\"</code> (list): the number of topics (factors) to learn.</p> <p><code>\"pixhex\"</code>: often set to be the same as \"hexgrids\" or slightly smaller.</p> <p><code>\"nmove\"</code>: \"pixhex\" divided by \"nmove\" is the distance between adjacent anchor points in the algorithm. We recommend pixhex/nmove to be around \\(4~6\\mu m\\) for high resolution results.</p> <p><code>\"res\"</code>: the resolution for pixel level inference (pixels within this distance will be grouped together in inference). We've been using \\(0.5\\mu m\\).</p> <p><code>\"scale\"</code>: this only controls the visualization of pixel level results. The coordinate values divided by scale will be the \"pixel\" indices in the image. If your coordinates are in microns and you want \\(0.5 \\mu m\\) to be one pixel in the image, set scale to 0.5. For Visium HD where the data resolution is \\(2 \\mu m\\), you probably want to set scale to 2.</p> <p>Section <code>\"job\"</code>: only for slurm users. Those are just slurm job parameters to create a job script to wrap aroun the Makefile. You probably don't need this, just for convenience. You can include additional commands by setting \"extra_lines\".</p>"},{"location":"workflows/#step-by-step","title":"Step by step","text":""},{"location":"workflows/#setup","title":"Setup","text":"<p>First, set up the environment variables:</p> <pre><code>threads=4 # Number of threads for parallel processing\ntmpdir=/path/to/tmp # Directory for temporary files (must be empty or creatable)\npath=/path/to/your_data # Path to your data directory\n</code></pre>"},{"location":"workflows/#step-1-group-pixels-to-tiles","title":"Step 1: Group pixels to tiles","text":"<p>Group pixels into non-overlapping square tiles for faster processing:</p> <pre><code>punkst pts2tiles --in-tsv transcripts.tsv \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 1 \\\n  --tile-size 500 \\\n  --temp-dir ${tmpdir} --threads ${threads} \\\n  --out-prefix ${path}/transcripts.tiled\n</code></pre> <p>Key parameters:</p> <p><code>--icol-x</code>, <code>--icol-y</code>: Column indices for X and Y coordinates (0-based)</p> <p><code>--skip</code>: If your input file has a header, use <code>--skip 1</code> to skip the first (or more) lines</p> <p><code>--tile-size</code>: Size (side length) of the square tiles</p> <p>Detailed documentation for pts2tiles</p>"},{"location":"workflows/#step-2-create-hexagonal-units","title":"Step 2: Create hexagonal units","text":"<p>Group pixels into non-overlapping hexagons:</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --feature-dict ${path}/transcripts.tiled.features.tsv \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n  --min-count 20 --hex-size 7 \\\n  --out ${path}/hex_12.txt \\\n  --temp-dir ${tmpdir} --threads ${threads}\n</code></pre> <p>Key parameters:</p> <p><code>--icol-feature</code>, <code>--icol-int</code>: Column indices for feature and count(s)</p> <p><code>--hex-size</code>: Side length of the hexagons</p> <p><code>--min-count</code>: Minimum count for a hexagon to be included</p> <p>Shuffle the output for training: <pre><code>sort -k1,1 --parallel ${threads} ${path}/hex_12.txt &gt; ${path}/hex_12.randomized.txt\nrm ${path}/hex_12.txt\n</code></pre></p> <p>Detailed documentation for tiles2hex</p>"},{"location":"workflows/#step-3-run-lda-on-hexagon-data","title":"Step 3: Run LDA on hexagon data","text":"<p>Perform Latent Dirichlet Allocation on the hexagon data:</p> <pre><code>punkst lda4hex --in-data ${path}/hex_12.randomized.txt \\\n  --in-meta ${path}/hex_12.json \\\n  --n-topics 12 \\\n  --n-epochs 2 --min-count-train 50 \\\n  --out-prefix ${path}/hex_12 --transform \\\n  --threads ${threads} --seed 1\n</code></pre> <p>Key parameters:</p> <p><code>--n-topics</code>: Number of topics (factors) to learn</p> <p><code>--transform</code>: Generate transform results after model fitting</p> <p>Detailed documentation for lda4hex</p>"},{"location":"workflows/#step-4-decode-pixels-with-the-model","title":"Step 4: Decode pixels with the model","text":"<p>Annotate each pixel with top factors and their probabilities:</p> <pre><code>punkst pixel-decode --model ${path}/hex_12.model.tsv \\\n  --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n  --hex-grid-dist 12 --n-moves 2 \\\n  --pixel-res 0.5 \\\n  --out-pref ${path}/pixel.decode \\\n  --temp-dir ${tmpdir} \\\n  --threads ${threads} --seed 1 --output-original\n</code></pre> <p>Key parameters:</p> <p><code>--model</code>: Model file created by lda4hex</p> <p><code>--hex-grid-dist</code>: Center-to-center distance of the hexagonal grid</p> <p><code>--n-moves</code>: Number of sliding moves to generate anchors</p> <p><code>--pixel-res</code>: Resolution for the analysis (in the same unit as coordinates)</p> <p><code>--output-original</code>: Write each transcript/input pixel as a separate line in the output. This will be slower and generates a bigger file, so only use it if matching the inference with the original input is useful. (Excluding this flag for Visium HD data is more sensible)</p> <p>Detailed documentation for pixel-decode</p>"},{"location":"workflows/#step-5-visualize-the-results","title":"Step 5: Visualize the results","text":"<p>Visualize the pixel decoding results:</p> <p>Optional: choose a color table based on the intermediate results. Otherwise, you need to create a RGB table with the following columns: R, G, B (including the header). So each row represents the RGB color for a factor, with integer values from 0 to 255. (Python dependency: jinja2, pandas, matplotlib.)</p> <pre><code>python punkst/ext/py/color_helper.py --input ${path}/hex_12.results.tsv --output ${path}/color\n</code></pre> <p>Generate an image for the pixel level factor assignment <pre><code>punkst draw-pixel-factors --in-tsv ${path}/pixel.decode.tsv \\\n  --in-color ${path}/color.rgb.tsv \\\n  --out ${path}/pixel.png \\\n  --scale 1 \\\n  --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre></p> <p>Key parameters:</p> <p><code>--in-color</code>: TSV file with RGB colors for each factor</p> <p><code>--scale</code>: Scales input coordinates to pixels in the output image (2 means 2 coordinate units = 1 pixel in the image). If the coordinates are in microns, 1 or 0.5 is suitable for high resolution data (imaging-based, Stereo-seq, Seq-scope, etc.); 2 is suitable for Visium HD.</p> <p><code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code>: Range of coordinates to visualize. If you specified <code>--icol-feature</code> and <code>--icol-int</code> in <code>pts2tiles</code>, you can either find the range in the <code>transcripts.tiled.coord_range.tsv</code> file or pass it directly with <code>--range transcripts.tiled.coord_range.tsv</code>.</p> <p>Compute naive differential expression statistics <pre><code>python punkst/ext/py/de_bulk.py --input ${path}/pixel.decode.pseudobulk.tsv \\\n  --output ${path}/de_bulk.tsv --thread ${threads}\n</code></pre></p> <p>Generate a html to display the color and top enriched genes for each factor <pre><code>python punkst/ext/py/factor_report.py --de ${path}/de_bulk.tsv \\\n  --pseudobulk ${path}/pixel.decode.pseudobulk.tsv \\\n  --color_table ${path}/color.rgb.tsv \\\n  --output_pref ${path}/report\n</code></pre></p> <p>Detailed documentation for visualization</p>"},{"location":"workflows/multisample/","title":"Multi-Sample Analysis Utilities","text":"<p>There are two main utilities for handling multiple transcriptomics inputs to help multi-sample analysis.</p> <p><code>multisample-prepare</code> processes raw data from multiple samples in a unified way. The output are ready for model training and later sample-specific pixel level projection. You also have the option to run only the first or the second step of the pipeline, see below for details.</p> <p><code>merge-units</code> merges multiple binned datasets, for example single cells or hexagons, in our customized sparse matrix format to a single dataset while harmonizing the sample-specific feature lists.</p> <p>(<code>pixel-decode</code> also allows processing multiple samples using the same model and parameters (see option <code>--sample-list</code> in <code>pixel-decode</code>) )</p>"},{"location":"workflows/multisample/#processing-multiple-samples-from-raw-data","title":"Processing multiple samples from raw data","text":"<p>The <code>multisample-prepare</code> command processes multiple raw spatial transcriptomics datasets into a merged hexagonal binned data suitable for joint model training (by <code>punkst topic-model</code>) and sample-specific tiled pixel level data for pixel level projection.</p> <p>It runs <code>pts2tiles</code> and <code>tiles2hex</code> for each sample then merge the binned level data.</p> <p>(if neither <code>--hex-grid-dist</code> nor <code>--hex-size</code> is provided, it only runs <code>pts2tiles</code>; if <code>--tiles2hex-only</code> is set (see the third section below) it only runs <code>tiles2hex</code> and merges the output files.)</p>"},{"location":"workflows/multisample/#usage","title":"Usage","text":"<pre><code>punkst multisample-prepare --in-tsv-list input_file_list.tsv \\\n    --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 1 \\\n    --tile-size 500 \\\n    --min-total-count-per-sample 100 \\\n    --hex-grid-dist 12 --min-count 10 \\\n    --out-dir ./out --out-joint-pref merged \\\n    --temp-dir ./tmp --threads ${threads}\n</code></pre>"},{"location":"workflows/multisample/#input-file-list-in-tsv-list","title":"Input File List (<code>--in-tsv-list</code>)","text":"<p>The pipeline requires an input TSV file that lists the information for each sample to be processed. Each line should contain two tab-separated columns: 1.  A unique Sample ID. 2.  The path to the raw transcript file for that sample.</p> <p>The raw transcript file should be in the format expected by <code>pts2tiles</code>. The raw transcript file should be in the format expected by <code>pts2tiles</code>.</p> <p>Example <code>input_file_list.tsv</code>: <pre><code>sample_A    /path/to/sample_A_transcripts.tsv\nsample_B    /path/to/sample_B_transcripts.tsv\n</code></pre></p>"},{"location":"workflows/multisample/#key-options","title":"Key Options","text":"<p>(Other options are available, see <code>pts2tiles</code> and <code>tiles2hex</code> for details.) (Other options are available, see <code>pts2tiles</code> and <code>tiles2hex</code> for details.)</p> <p><code>--in-tsv-list &lt;file&gt;</code>: (Required) Path to the input TSV file describing the samples.</p> <p><code>--min-total-count-per-sample &lt;int&gt;</code>: The minimum sample-specific total count a feature must have (across all samples) to be included in the merged file. Default: 1. Setting it to 0 to use the union of features across samples.</p> <p><code>--exclude-feature-regex</code>: Regular expression (modified ECMAScript grammar) to exclude features matching this pattern. Default: exclude no feature.</p> <p><code>--include-feature-regex</code>: Regular expression to include only features matching this pattern. Default: include all features. (e.g. to exclude features that contain \"Unassigned\" or \"NegControl\" as substrings in any part of the feature name, you can use <code>--exclude-feature-regex \".*(Unassigned|NegControl).*\"</code>)</p> <p><code>--threads &lt;int&gt;</code>: Number of threads to use. [Default: 1]</p> <p><code>--out-dir &lt;dir&gt;</code>: (Required) The base output directory where all results will be stored. Merged outputs will be placed directly under this directory, sample-specific outputs will be in subdirectories named <code>samples/[sample_id]/</code>.</p> <p><code>--out-joint-pref &lt;prefix&gt;</code>: (Required) A prefix for all merged output (the merged hexagon file and merged feature list). For example, the</p> <p><code>--temp-dir &lt;dir&gt;</code>: (Required) A directory for storing temporary files (will be created if it doesn't exist).</p> <p><code>--overwrite</code>: If set, overwrite existing sample-specific output files.</p> <p><code>pts2tiles</code> options:</p> <p><code>--icol-x &lt;int&gt;</code>, <code>--icol-y &lt;int&gt;</code>, <code>--icol-feature &lt;int&gt;</code>, <code>--icol-int &lt;int&gt;</code>: (Required) 0-based column indices for X/Y coordinates, the feature name, and the count/value.</p> <p><code>--skip</code>: If your input file has a header, use <code>--skip 1</code> to skip the first (or more) lines.</p> <p><code>--tile-size &lt;int&gt;</code>: (Required) The size of the square tiles for pre-processing. Should be big enough, say 500 microns.</p> <p><code>tiles2hex</code> options:</p> <p><code>--hex-grid-dist &lt;float&gt;</code>: The center-to-center distance for the hexagonal grid. Alternatively, provide <code>--hex-size &lt;float&gt;</code>, side length of the hexagons (exactly one of the two options must be provided). Multiple values can be provided, separated by spaces.</p> <p><code>--min-count &lt;int&gt;</code>: The minimum total count for a hexagon (unit) to be included in the output.</p>"},{"location":"workflows/multisample/#output-files","title":"Output Files","text":"<p>All outputs are under the specified <code>--out-dir</code></p> <p>In the main output directory (<code>--out-dir</code>):</p> <p><code>[--out-joint-pref].persample_file_list.tsv</code>: A list of paths to the tiled pixel level files for each sample. These are the input for <code>pixel-decode</code>.</p> <p><code>[--out-joint-pref].union_features.tsv</code>: A list of all features found in any of the samples, with total and sample-specific counts.</p> <p><code>[--out-joint-pref].features.tsv</code>: The final list of features used for the merged output.</p> <p><code>[--out-joint-pref].hex_[dist].txt</code> and <code>.json</code>: The final merged hexagon data and its corresponding metadata file, ready for <code>lda4hex</code>.</p> <p>In per-sample subdirectories (<code>--out-dir/samples/[sample_id]/</code>):</p> <p>Intermediate tiled transcript files (<code>.tiled.tsv</code>, <code>.tiled.index</code>).</p> <p>Per-sample feature counts (<code>.tiled.features.tsv</code>).</p> <p>Per-sample randomized hexagon data (<code>.hex_[dist].txt</code>, <code>.hex_[dist].json</code>).</p>"},{"location":"workflows/multisample/#merge-hexagon-units-from-pre-processed-samples","title":"Merge hexagon units from pre-processed samples","text":"<p>The <code>merge-units</code> command merges multiple bin level data in the format of the output from <code>punkst tiles2hex</code>.</p> <p>The input files can have different extra information, as long as the metadata (<code>.json</code>) are recognized and includes a key <code>offset_data</code> indicating the starting index (0-based) of the sparsely coded count data.</p> <p>(In each row, tokens are separate by tabs. Starting from the index specified by <code>offset_data</code>, each row contains two integers for the number of unique features and the total count of all features, followed by feature_index and count (separated by a single space) pairs. See <code>tiles2hex</code> for more details.) (In each row, tokens are separate by tabs. Starting from the index specified by <code>offset_data</code>, each row contains two integers for the number of unique features and the total count of all features, followed by feature_index and count (separated by a single space) pairs. See <code>tiles2hex</code> for more details.)</p>"},{"location":"workflows/multisample/#usage_1","title":"Usage","text":"<p>This example merges two pre-processed samples.</p> <p>Two optional input specifications are demonstrated: It tells the tool to either use the existing random keys or generate new random keys based on the input data (<code>-2</code> on the 5-th column) and to carry over the data from column index (0-based) <code>4</code> of sample 1 and column <code>3</code> from sample 2 into the new \"info\" column.</p> <pre><code># Create the input specification\ninput_list=\"input.tsv\"\necho -e \"1\\t./1/1.tiled.features.tsv\\t./1/1.hex_12.txt\\t./1/1.hex_12.json\\t-2\\t4\" &gt; ${input_list}\necho -e \"2\\t./2/2.tiled.features.tsv\\t./2/2.hex_12.txt\\t./2/2.hex_12.json\\t-2\\t3\" &gt;&gt; ${input_list}\n\n# Run the merge command\npunkst merge-units \\\n    --in-list ${input_list} \\\n    --out-pref ./merged.hex_12 \\\n    --min-total-count-per-sample 100 \\\n    --temp-dir ./tmp --threads 4\n</code></pre>"},{"location":"workflows/multisample/#input-file-in-list","title":"Input File (<code>--in-list</code>)","text":"<p>The command requires a TSV file specifying the input for each sample. Each line must contain at least four columns:</p> <ol> <li> <p>Sample ID: A unique identifier for the sample.</p> </li> <li> <p>Features Path: Path to the sample-specific feature file. (TSV with feature name and count, like that created by <code>pts2tiles</code>. We only read the first two columns; lines where the second token is not a non-negative integer are ignored.)</p> </li> <li> <p>Hexagon Data Path: Path to the sample's hexagon data file (<code>.txt</code>).</p> </li> <li> <p>Hexagon Metadata Path: Path to the sample's hexagon metadata file (<code>.json</code>).</p> </li> <li> <p>Random Key Column Index (Optional): An integer specifying how to handle the random key for each unit.</p> <ul> <li> <p><code>&gt;= 0</code>: The column index in the input hexagon file to use as the random key for shuffling the merged output.</p> </li> <li> <p><code>-1</code>: Generate a new random key for each unit.</p> </li> <li> <p><code>-2</code>: Try to find the key column from the metadata (<code>.json</code>) file first. (Default)</p> </li> </ul> </li> <li> <p>Info Columns (Optional): A comma-delimited string of column indices (e.g., <code>2,5,6</code>) from the input hexagon file to carry over into a single \"info\" column in the merged output. Each sample can have different info columns. Put an \".\" to indicate no info columns to carry over for that sample.</p> </li> </ol> <p>Example <code>input.tsv</code>: <pre><code>1   /path/to/1/1.tiled.features.tsv /path/to/1/1.hex_12.txt /path/to/1/1.hex_12.json    0   4\n2   /path/to/2/2.tiled.features.tsv /path/to/2/2.hex_12.txt /path/to/2/2.hex_12.json    -2  3\n</code></pre></p>"},{"location":"workflows/multisample/#options","title":"Options","text":"<p><code>--in-list &lt;file&gt;</code>: (Required) Path to the input TSV file listing the pre-processed samples.</p> <p><code>--out-pref &lt;prefix&gt;</code>: (Required) Prefix for all output files (e.g., <code>/path/to/output/merged</code>).</p> <p><code>--temp-dir &lt;dir&gt;</code>: (Required) A directory for storing temporary files (will try to create it if it doesn't exist).</p> <p><code>--min-total-count-per-sample &lt;int&gt;</code>: Minimum per-sample count a feature must have across all samples to be included in the final merged feature set. (Default: 1. Set to 0 to use the union of features).</p> <p><code>--min-count-per-unit &lt;int&gt;</code>: Minimum total count a unit/hexagon must have after feature filtering to be included in the merged output. [Default: 1]</p> <p><code>--threads &lt;int&gt;</code>: Number of threads. [Default: 1]</p>"},{"location":"workflows/multisample/#output-files_1","title":"Output Files","text":"<p><code>[--out-pref].txt</code> and <code>.json</code>: The merged hexagon data file and its corresponding metadata, ready for <code>lda4hex</code>.</p> <p><code>[--out-pref].features.tsv</code>: The list of features and their total counts in the merged dataset.</p> <p><code>[--out-pref].union_features.tsv</code>: A list of all features found across all samples and their per-sample counts.</p>"},{"location":"workflows/multisample/#generate-sample-specific-and-merged-hexagons","title":"Generate sample-specific and merged hexagons","text":"<p>The <code>multisample-prepare</code> command has another mode to only perform the second step: run <code>tiles2hex</code> for each sample then merge the binned level data. This mode is activated by <code>--tiles2hex-only</code>, and a different input should be provided in <code>--in-tsv-list</code>. Most likely use case is when you have already run <code>pts2tiles</code> for each sample separately.</p>"},{"location":"workflows/multisample/#usage_2","title":"Usage","text":"<pre><code>punkst multisample-prepare --in-tsv-list input_file_list.tsv \\\n    --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n    --tiles2hex-only \\\n    --min-total-count-per-sample 100 \\\n    --hex-grid-dist 12 --min-count-per-unit 10 \\\n    --out-dir ./out --out-joint-pref merged \\\n    --temp-dir ./tmp --threads ${threads} \\\n</code></pre>"},{"location":"workflows/multisample/#input-file-list-in-tsv-list_1","title":"Input File List (<code>--in-tsv-list</code>)","text":"<p>The pipeline requires an input TSV file that lists the information for each sample to be processed. Each line should contain two tab-separated columns: 1.  A unique Sample ID. 2.  The path to the tiled transcript file for that sample. 3.  The path to the corresponding index file. 4.  The path to the per-sample feature count file.</p> <p>The three input files per sample should be in the same formats as those output by <code>pts2tiles</code>.</p> <p>Example <code>input_file_list.tsv</code>: <pre><code>sample_A    /path/to/sample_A_transcripts.tiled.tsv /path/to/sample_A_transcripts.tiled.index   /path/to/sample_A.features.tsv\nsample_B    /path/to/sample_B_transcripts.tiled.tsv /path/to/sample_B_transcripts.tiled.index   /path/to/sample_B.features.tsv\n</code></pre></p>"}]}