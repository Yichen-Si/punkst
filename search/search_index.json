{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"punkst","text":"<p>punkst provides a set of tools for analyzing high resolution spatial transcriptomics data. See Modules for available operations.</p> <p>Check the Installation page to install punkst.</p> <p>Check the Input page for help on preparing your data from vairous platforms.</p>"},{"location":"#pixel-level-factor-analysis","title":"Pixel level factor analysis","text":"<p>punkst includes a pipeline for efficient pixel level factor analysis (FICTURE (2024)) improved efficiency and flexibility. (Previous python package is still functioning.)</p> <p>Check the quick start page for help on generating a full pipeline using Makefile or running each command step by step.</p>"},{"location":"#basic-workflow","title":"Basic workflow","text":"<ol> <li>pts2tiles: Group pixels to tiles for faster processing</li> <li>tiles2hex: Group pixels into non-overlapping hexagons</li> <li>topic-model: Run factorization on the hexagon/single cell data</li> <li>pixel-decode: Annotate each pixel with the top factors and their probabilities</li> <li>Visualization: Create a high resolution visualization of the results</li> </ol> <p>Check the Modules for detailed documentation of each command.</p>"},{"location":"install/","title":"Install","text":""},{"location":"install/#installation-guide-for-punkst","title":"Installation Guide for punkst","text":"<p>This guide walks you through building punkst on Linux and macOS, including environments without root access.</p>"},{"location":"install/#build","title":"Build","text":"<p>Prerequisites</p> <ul> <li>Git</li> <li>CMake: 3.15 to 3.23</li> <li>C++17 compiler* (GCC \u22658, Clang \u22655, MSVC 2017+)</li> <li>TBB, OpenCV</li> </ul> <p>*We do assume your compiler properly supports C++17. Consider updating your compiler if you encounter issues.</p> <pre><code># 1) Clone the repository\ngit clone --recursive https://github.com/your-org/punkst.git\ncd punkst\n# 2) Create and enter a build directory\nmkdir build &amp;&amp; cd build\n# 3) Configure\ncmake ..\n# 4) Build\ncmake --build . --parallel # or make\n</code></pre> <p>If you did not clone the submodule (Eigen) initially, you can do <pre><code>git submodule update --init\n</code></pre></p> <p>If an error occurs due to TBB not found, see below for installation options.</p> <p>If you installed some dependencies locally, you may need to specify their paths like <pre><code>cmake .. \\\n  -DOpenCV_DIR=$HOME/.local/lib/cmake/opencv4 \\\n  -DTBB_DIR=$HOME/user/opt/tbb/lib/cmake/tbb \\\n  -DCMAKE_PREFIX_PATH=\"$HOME/.local\"\n</code></pre> (On mac, if CMake fails to locate OpenCV (installed with brew), pass: <code>-DOpenCV_DIR=$(brew --prefix opencv)/lib/cmake/opencv4</code> or wherever OpenCV is installed.)</p> <p>The <code>punkst</code> binary will be placed in <code>bin/</code> under the project root.</p> <p>Verifying the Build</p> <pre><code>punkst/bin/punkst --help\n</code></pre> <p>You should see a message starting with <pre><code>Available Commands\nThe following commands are available:\n</code></pre></p>"},{"location":"install/#required-libraries","title":"Required Libraries","text":"<ul> <li>TBB</li> </ul> <p>System: <code>sudo apt-get install libtbb-dev</code> or <code>yum install tbb-devel</code> on linux and <code>brew install tbb</code> on macOS.</p> <p>Local: install oneTBB (or find more information here).</p> <ul> <li>OpenCV</li> </ul> <p><code>sudo apt-get install libopencv-dev</code>or <code>sudo yum install opencv-devel</code> on linux, <code>brew install opencv</code> on macOS. See OpenCV installation guide for more details on how to install from source.</p> <ul> <li>Other dependencies</li> </ul> Library Ubuntu / Debian CentOS / RHEL macOS (Homebrew) zlib <code>sudo apt-get install zlib1g-dev</code> <code>sudo yum install zlib-devel</code> <code>brew install zlib</code> BZip2 <code>sudo apt-get install libbz2-dev</code> <code>sudo yum install bzip2-devel</code> <code>brew install bzip2</code> LibLZMA <code>sudo apt-get install liblzma-dev</code> <code>sudo yum install xz-devel</code> <code>brew install xz</code>"},{"location":"install/#build-options-for-performance-and-portability","title":"Build Options for Performance and Portability","text":"<p>By default, punkst builds a portable binary. You can customize the build using the following CMake flags:</p> Goal CMake Command Description Local Performance <code>cmake -DENABLE_NATIVE_ARCH=ON ..</code> Optimizes the binary for your specific CPU (AVX2, AVX-512, etc.) Maximum Portability <code>cmake -DENABLE_NATIVE_ARCH=OFF ..</code> (Default) Ensures the binary runs on any 64-bit machine Modern CPU Fleet <code>cmake -DENABLE_X86_64_V3=ON ..</code> Targets x86-64-v3 (Haswell/2013+). Supports AVX2 but remains portable to most modern machines"},{"location":"blog/","title":"Blog","text":""},{"location":"input/","title":"Notes on processing data from different technologies/platforms","text":"<p>Here are instructions on how to convert raw data from different platforms to the generic pixel/transcript level input format for punkst.</p> <p>(For spot/single cell level data, <code>topic-model</code> now accepts 10X style DGE files directly, but you can also convert them to our custom format (see the last section below))</p> <p>After the conversion, you can follow the standard workflow as described in the quick start page to run the pipeline (and specify all coordinate/size related parameters in microns). For platforms that provide cell coordinates, we also extracted the cell centers and you can try the experimental workflow in <code>examples/with_cell_centers</code>.</p> <p>(We currently documented examples for 10X Genomics Visium HD, Xenium, NanoString CosMx SMI, and Vizgen MERSCOPE data. We've also applied punkst to Seq-scope, Stereo-seq, and other similar platforms, we are working on providing more information.)</p>"},{"location":"input/#visium-hd","title":"Visium HD","text":"<p>First we need to locate the \"binned output\" directory and the subdirectory with the original resolution. For the data downloaded from 10X website it is called <code>binned_outputs/square_002um</code>. Let's call it <code>RAWDIR</code>. In this directory, you should find the subdirecotries, <code>spatial</code> and <code>filtered_feature_bc_matrix</code> (I guess you could also use data in <code>raw_feature_bc_matrix</code> but I've not tried it yet).</p> <p>In the <code>spatial</code> directory, you should have a json file that contains the scaling factor of the coordinates, named as <code>scalefactors_json.json</code>. Let's grep the scaling factor (or set it manually). <pre><code>microns_per_pixel=$(grep -w microns_per_pixel ${RAWDIR}/spatial/scalefactors_json.json | perl -lane '$_ =~ m/.*\"microns_per_pixel\": ([0-9.]+)/; print $1' )\n#  \"microns_per_pixel\": 0.2737129726047599 in an example data\n</code></pre></p> <p>The spatial coordinates for each barcode are stored in <code>parguet</code> format, we do not support this format directly. Let's convert it to plain tsv file. duckdb seems fast and easy to use:</p> <pre><code>cd ${RAWDIR}/spatial/\nduckdb -c \"COPY (SELECT * FROM read_parquet('tissue_positions.parquet')) TO 'tissue_positions.tsv' (HEADER, DELIMITER '\\t');\"\n</code></pre> <p>Alternatively, you can try to use pyarrow and pandas in python.</p> <p>Next, punkst has a command to merge the 10X style dge files (and the spatial coordinates) into a single file as our standard input:</p> <pre><code>brc_raw=${RAWDIR}/spatial/tissue_positions.tsv # the one converted from parquet\nmtx_path=${RAWDIR}/filtered_feature_bc_matrix # path to the 10X style dge files\npunkst convert-dge \\\n--microns-per-pixel ${microns_per_pixel} \\\n--exclude-regex '^mt-' --in-tissue-only \\\n--in-positions ${brc_raw} \\\n--in-dge-dir ${mtx_path} \\\n--output-dir ${path} \\\n--coords-precision 4\n</code></pre> <p>Here the optional flag <code>--exclude-regex</code> takes a regular expression to exclude genes matching with the regex. In the above example we exclude all mitochondrial genes.</p> <p>The optional flag <code>--in-tissue-only</code> will exclude all barcodes that are labeled as not in the tissue.</p> <p>The command writes <code>transcripts.tsv</code> with coordinates in microns.</p> <p>The first column of the output file is the row index (0-based) of the barcode in the input barcode file (<code>${mtx_path}/barcodes.tsv.gz</code>), in case we want to recover the original barcode ID later.</p>"},{"location":"input/#cosmx-smi","title":"CosMx SMI","text":"<p>&lt;!-- You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/cosmx</code> to conver CosMx raw output files to the generic input format. Alternatively, see the bash commands below. Copy the config file to your directory and set the raw file names. For example, here is an example for the public mouse half brain data:</p> <pre><code>{\n    \"workflow\": {\n      \"raw_tx\" : \"Run1000_S1_Half_tx_file.csv\",\n      \"raw_meta\": \"Run1000_S1_Half_metadata_file.csv\",\n      \"microns_per_pixel\": 0.12,\n      \"datadir\": \"/output/test\"\n    }\n  }\n``` --&gt;\n\nFind `\"microns_per_pixel\"` in the ReadMe.html, it may say something like \"To convert to microns multiply the pixel value by 0.12 um per pixel\".\n\nLocate two files, `RunXXXX_tx_file.csv` (transcript file) and `RunXXXX_metadata_file.csv` (cell metadata file). Let's call them `RAW_TX` and `RAW_META`.\n\nRun the following commands to extract relevant information from the raw csv files:\n\n```bash\n# Extract cell coordinates\ncut -d',' -f 1,2,3,7,8 ${RAW_META} | tail -n +2 | sed 's/\"//g' | \\\n  awk -F',' -v OFS=\"\\t\" -v mu=${MICRONS_PER_PIXEL} \\\n  '{printf \"%.2f\\t%.2f\\t%s_%s\\t%s\\n\", $4*mu, $5*mu, $1, $2, $3}' &gt; cell_centers.tsv\n\n# Extract transcripts\nawk -F',' -v mu=${MICRONS_PER_PIXEL} '\\\nNR==1{gsub(/\"/, \"\", $0); print \"#\" $3, $4, $8, \"count\", $7, \"CellID\", $9 }\\\nNR&gt;1{gsub(/\"/, \"\", $8); gsub(/\"/, \"\", $9); printf \"%.2f\\t%.2f\\t%s\\t%d\\t%s\\t%s_%s\\t%s\\n\", mu*$3, mu*$4, $8, 1, $7, $1, $2, $9 } ' ${RAW_TX} &gt; transcripts.tsv\n</code></pre>"},{"location":"input/#merscope","title":"MERSCOPE","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/merscope</code> to conver MERSCOPE raw output files to the generic input format.</p> <p>Set <code>\"rawdir\"</code> to be the path that contains the MERSCOPE output files. We will need the following files: <code>cell_metadata.csv.gz</code> and <code>detected_transcripts.csv.gz</code>. If your data is compressed, set <code>\"compressed\"</code> to 1, otherwise (plain csv) set it to 0. Set <code>\"datadir\"</code> to the output directory.</p> <p>The following are the commands ran in the <code>Makefile</code>:</p> <pre><code># Extract cell coordinates\nzcat ${RAWDIR}/cell_metadata.csv.gz | cut -d',' -f 4-9 | tail -n +2 | awk -F',' -v OFS=\"\\t\" '{ print $1, $2; }' &gt; cell_coordinates.tsv\n# Extract transcripts\nzcat ${RAWDIR}/detected_transcripts.csv.gz \\\n  | cut -d',' -f2-5,9 \\\n  | sed \\\n      -e '0,/barcode/{s/barcode/#barcode/}' \\\n      -e 's/,/\\t/g' \\\n      -e 's/$/\\t1/' \\\n      -e '0,/barcode/{s/\\t1$/\\tcount/}' \\\n    &gt; transcripts.tsv\n</code></pre>"},{"location":"input/#xenium","title":"Xenium","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/xenium</code> to conver Xenium raw output files to the generic input format.</p> <p>In the <code>config.json</code>, you need specify <code>\"raw_transcripts\"</code> as the path of the transcript file <code>transcripts.csv.gz</code> and <code>\"raw_cells\"</code> as the path of the cell metadata <code>cells.csv.gz</code>.</p> <p>The following are the commands ran in the <code>Makefile</code>:</p> <pre><code># Extract transcripts\nzcat transcripts.csv.gz \\\n  | cut -d',' -f4-6 | sed 's/\"//g' \\\n  | awk -F',' -v OFS=\"\\t\" '{ print $2, $3, $1, \"1\" }' \\\n  &gt; transcripts.tsv\n\n# Extract cell coordinates\nzcat cells.csv.gz \\\n  | cut -d',' -f2-3 \\\n  | tail -n +2 \\\n  | awk -F',' -v OFS=\"\\t\" '{printf \"%.4f\\t%.4f\\n\", $1,$2;}' &gt; cell_coordinates.tsv\n</code></pre>"},{"location":"input/#10x-single-cell","title":"10X Single cell","text":"<p>If you would like to apply topic modeling or matrix factorization to your single cell data, you can convert the 10X Genomics single-cell DGE files to files used by <code>punkst topic-model</code> and <code>nmf-pois-log1p</code> etc. by the following command:</p> <p><pre><code>punkst convert-10X-SC \\\n--in-dge-dir /path/to/dge/folder --sorted-by-barcode \\\n--randomize --sort-mem 1G \\\n--out /path/to/output/prefix\n</code></pre> The input folder should contain the standard 10X files: <code>barcodes.tsv.gz</code>, <code>features.tsv.gz</code>, and <code>matrix.mtx.gz</code> (gziped). Alternatively, you can specify the three files directly by <code>--in-barcodes</code>, <code>--in-features</code>, and <code>--in-matrix</code>. Since normally the matrix is sorted by barcode indices, adding <code>--sorted-by-barcode</code> enables streaming mode though this is optional.</p> <p>The output includes three files: the pair <code>${out}.tsv</code>, <code>${out}.json</code> (the main count matrix) and <code>${out}.features.tsv</code>. Since the gene names are not necessarily unique in the 10X DGE (multiple Ensembl IDs may map to the same gene symbol), the program replace duplicate gene names with the Ensembl IDs while keeping the one with the highest total count as the gene symbol. So the first column in the output feature file contains unique feature names (mostly gene symbols). The second column contains the total counts.</p> <p>For the downstream topic model to run in minibatch mode efficiently, the flag <code>--randomize</code> ensures that the order of cells is randomized in the output. The randomization is done by sorting a random key assigned to each cell. With <code>--sorted-by-barcode</code>, the maximum memory usage is usually determined by the buffer size in the sorting (ransomization) step which can be controlled by <code>--sort-mem</code>. The recognized memory units are <code>K</code>, <code>M</code>, and <code>G</code>. By default the program uses your system's <code>sort</code> (GNU sort), unless you specify <code>--use-internal-sort</code>.</p>"},{"location":"modules/","title":"Punkst Modules","text":"<p>punkst provides several command-line tools for analyzing high resolution spatial (transcriptomics) data.</p>"},{"location":"modules/#available-modules","title":"Available Modules","text":"<ul> <li>pts2tiles: Groups pixels to tiles for faster processing</li> <li>tiles2hex: Groups pixels into non-overlapping hexagons for spot level analysis</li> <li>topic-model: Runs LDA on the spot level data</li> <li>pixel-decode: Annotates each pixel with the top factors and their probabilities</li> <li>Visualization: Visualizes the pixel level analysis results</li> <li>DE tests: Differential expression tests</li> <li>tile-op: View and manipulate tiled data files (including merge and annotate)</li> <li>cooccurrence: Computes gene co-occurrence and/or extract marker genes from the co-occurrence matrix</li> </ul>"},{"location":"modules/#input-data-format","title":"Input Data Format","text":"<p>The input is a tsv file with the following columns: X, Y, feature, count. Whether the file contains headers or other columns is not relevant, as long as the above four columns are present.</p> <ul> <li> <p>X, Y coordinates can be either integer or float vlaues. (If your coordinates are integers and you would like to keep the original coordinates in the pixel level inference output, set <code>--coords-are-int</code> in <code>punkst pixel-decode</code>). The coordinates can be in arbitrary units, just make sure all scale/size related parameters you later provide should be in the same unit.</p> </li> <li> <p>\"feature\" can be a string or a nonnegative integer corresponding to the index in a feature list.</p> </li> <li> <p>\"count\" is a nonnegative integer. You could apply gene-specific non-negative real valued weights to the count later in analysis.</p> </li> </ul>"},{"location":"modules/coexp/","title":"Gene cooccurrence and marker selection","text":"<p><code>cooccurrence</code> computes gene co-occurrence within a specified radius and <code>coloc2markers</code> selects markers from the co-occurrence matrix.</p>"},{"location":"modules/coexp/#cooccurrence","title":"cooccurrence","text":"<p>The <code>cooccurrence</code> command computes how frequently each pair of genes appear together within a specified radius, either as a binary relation or weighted by distance with exponential decay. (We also have a <code>merge-mtx</code> command in case you want to add up multiple co-occurrence matrices potentially computed from different datasets.)</p>"},{"location":"modules/coexp/#usage","title":"Usage","text":"<pre><code>punkst cooccurrence --in-tsv tiles.tsv --in-index tiles.index \\\n--feature-dict features.tsv --out ./out \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--radius 15  --halflife 10 --binary --threads 4\n</code></pre>"},{"location":"modules/coexp/#required","title":"Required","text":"<p><code>--in-tsv</code> - The tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> The index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code> - Column index for x coordinate (0-based).</p> <p><code>--icol-y</code> - Column index for y coordinate (0-based).</p> <p><code>--icol-feature</code> - Column index for feature list (0-based).</p> <p><code>--icol-val</code> - Column index for the integer count values (0-based).</p> <p><code>--radius</code> - Radius within which to count co-occurrence.</p> <p><code>--out</code> - Output prefix for generated files.</p> <p><code>--feature-dict</code> - If feature column contains non-integer values, provide a dictionary/list of all feature names.</p>"},{"location":"modules/coexp/#optional","title":"Optional","text":"<p><code>--bounding-boxes</code> - Rectangular query regions specified as <code>xmin ymin xmax ymax</code> coordinates (plain numbers separated by spaces, without parenthesis). A single co-occurrence matrix is created using pixels in the union of the regions. Can specify multiple regions by adding multiple 4-tuples. e.g. <code>--bounding-boxes 5040 2300 8830 4640 100 200 1500 1600</code> for two rectangles.</p> <p><code>--weight-by-count</code> - Weight co-occurrence by the product of transcript counts at each pixel. Default: false. (This is unlikely to have a noticeable impact unless for very dense sequencing data)</p> <p><code>--halflife</code> - Half-life (starting from 1 for zero distance, the distance where the weight is 0.5) for exponential decay weighting. Default: -1, unweighted by distance within the radius.</p> <p><code>--min-neighbor</code> - Minimum number of neighboring pixels within radius for a pixel to be included (meant to be used to reduce the influence from sparse/non-tissue regions). Default: 1.</p> <p><code>--local-min</code> - Minimum co-occurrence value within a tile to record. Default: 0.</p> <p><code>--threads</code> - Number of threads to use. Default: 1.</p> <p><code>--binary</code> - Output results in binary format. Default: false (TSV output). Using the binary format is more efficient especially if you were to run the <code>coloc2markers</code> command later.</p>"},{"location":"modules/coexp/#output-files","title":"Output files","text":"<ul> <li> <p>Co-occurrence matrix named <code>{prefix}.mtx.bin</code> (if <code>--binary</code> is used) or <code>{prefix}.mtx.tsv</code> (otherwise).</p> </li> <li> <p>Marginal information per gene named <code>{prefix}.marginals.tsv</code>. The columns are feature index (corresponding to row \\&amp; column index in the matrix), name, total counts, total number of pixels, used pixels, used neighbors.</p> </li> </ul>"},{"location":"modules/coexp/#adding-up-multiple-matrices","title":"Adding up multiple matrices","text":"<p>For convenience</p> <pre><code>punkst merge-mtx --in-list mtx_list.txt --binary --binary-output --shared-nrows $M --out cooccur.merged\n</code></pre> <p>Just include the file paths of the input matrices in a file provided to <code>--in-list</code>.</p>"},{"location":"modules/coexp/#coloc2markers","title":"coloc2markers","text":"<p>The <code>coloc2markers</code> command selects optimal marker features from a co-occurrence matrix generated by the <code>cooccurrence</code> command. It can optionally find neighbors for selected or specified markers and recover the corresponding expression profiles.</p>"},{"location":"modules/coexp/#usage_1","title":"Usage","text":"<p>Just finding markers: <pre><code>punkst coloc2markers --input out.mtx.bin --binary --info out.marginals.tsv \\\n--out ./markers --K 24 --neighbors 10\n</code></pre></p> <p>Recovering gene expression profiles around each set of markers: <pre><code>punkst coloc2markers --input out.mtx.bin --binary --info out.marginals.tsv \\\n--out ./markers --K 24 --neighbors 10 --recover-factors --weight-by-counts --threads 4\n</code></pre></p>"},{"location":"modules/coexp/#required_1","title":"Required","text":"<p><code>--input</code> - Input co-occurrence matrix (binary or TSV format) from the <code>cooccurrence</code> command.</p> <p><code>--info</code> - Input gene/feature information file from the <code>cooccurrence</code> command.</p> <p><code>--K</code> - Number of markers to select. Note that marker selection is deterministic and sequential, so the set of markers generated with small <code>K</code> is almost certainly contained in the set generated with a larger <code>K</code>. Specifying a large <code>K</code> then trim down to the desired number is perhaps recommended.</p> <p><code>--out</code> - Output prefix for generated files.</p>"},{"location":"modules/coexp/#optional_1","title":"Optional","text":"<p><code>--binary</code> - Specify that input matrix is in binary format. Default: false (assumes TSV).</p> <p><code>--value-bytes</code> - Number of bytes per value in binary matrix. Default: 8, match with the output from <code>cooccurrence</code>. Only used with <code>--binary</code>.</p> <p><code>--min-count</code> - Minimum count for a feature to be considered as a marker. This should be a reaonably large number, otherwise the results are driven by rare genes.</p> <p><code>--fixed</code> - List of markers that must be included in the selection (strings, separated by spaces). Currently assume the input are distinct markers.</p> <p><code>--find-neighbors</code> - Find neighbors (often co-localized genes) for each selected marker.</p> <p><code>--neighbors</code> - Number of top neighbors to find for each marker. Default: 10. Can be used instead of flag <code>--find-neighbors</code>.</p> <p><code>--neighbor-max-rank-fraction</code> - Maximum rank (in terms of the quantile/fraction among all genes) to consider for mutual neighbors. Default: 0.1, meaning that if gene A is among the top 10% of genes to be around conditional on observing gene B, but gene B is not among the top 10%, say among the bottom 50% of genes to be observed around gene A, then they are not considered as neighbors.</p>"},{"location":"modules/coexp/#recover-factors","title":"Recover factors","text":"<p><code>--recover-factors</code> - Recover underlying factors from the co-occurrence matrix after marker selection.</p> <p><code>--threads</code> - Number of threads to use for factor recovery. Default: -1 (auto). Only used with <code>--recover-factors</code>.</p> <p><code>--max-iter</code> - Maximum iterations for factor recovery. Default: 500.</p> <p><code>--tol</code> - Convergence tolerance for factor recovery. Default: 1e-6.</p> <p><code>--weight-by-counts</code> - Weight factors by gene counts. Default: false.</p> <p><code>--verbose</code> - Verbosity level for output messages. Default: 0.</p>"},{"location":"modules/coexp/#output-files_1","title":"Output files","text":"<ul> <li> <p><code>{prefix}.top.tsv</code> - List of selected markers</p> </li> <li> <p>if <code>--find-neighbors</code> or <code>--neighbors</code> is used:</p> <ul> <li><code>{prefix}.pairs.tsv</code> - Detailed pairwise relationships. Columns: index, name of gene 1, total count of gene 1, name of gene 2, total count of gene 2, (weighted) proportion of gene 2 in gene 1's neighbors, vice versa, rank of gene 2 as gene 1's neighbor, vice versa.</li> <li><code>{prefix}.short.txt</code> - Compact neighbor lists. The first gene on each line is the selected marker, followed by its neighbors.</li> </ul> </li> <li> <p>if <code>--recover-factors</code> is used: <code>{prefix}.factors.tsv</code> for recovered factor matrix.</p> </li> </ul>"},{"location":"modules/de/","title":"Differential expression tests","text":"<ul> <li><code>punkst de-chisq</code>: Chi-squared DE on pseudobulk matrices.</li> <li><code>punkst multi-conditional-de-pixel</code>: Multi-sample (pairwise), cell type-specific DE using pixel-level annotations.</li> </ul>"},{"location":"modules/de/#de-chisq","title":"de-chisq","text":"<p><code>de-chisq</code> runs a Chi-squared test on a pseudobulk matrix. Provide one input for a 1-vs-rest test per factor, or two inputs for a pairwise comparison of each factor between datasets.</p> <p>1-vs-rest per factor: <pre><code>punkst de-chisq --input pseudobulk.tsv --out de.tsv --min-fc 1.5 --max-pval 1e-3\n</code></pre></p> <p>1-vs-nearest-neighbors per factor: <pre><code>punkst de-chisq --input pseudobulk.tsv --out de.tsv --neighbor-k 3 \\\n  --min-fc 1.5 --max-pval 1e-3\n</code></pre></p> <p>Pairwise comparison between two pseudobulk matrices: <pre><code>punkst de-chisq --input sampleA.pseudobulk.tsv sampleB.pseudobulk.tsv \\\n  --out de.pair.tsv --min-fc 1.5 --max-pval 1e-4\n</code></pre></p>"},{"location":"modules/de/#input-format","title":"Input Format","text":"<p>The input is a TSV file where the first row must be a header including an arbitrary label for the feature name column (e.g. \"Gene\") followed by K factor / cell type names. Then each row starts with the gene name followed by counts of that gene for each factor.</p> <p>Such matrices will be generated by <code>punkst topic-model</code> and <code>pixel-decode</code>.</p> <p>For pairwise input, the two matrices should have matching factor columns. Feature names are intersected between the two files.</p>"},{"location":"modules/de/#required-parameters","title":"Required Parameters","text":"<p><code>--input</code> - Input TSV file. Provide one file for 1-vs-rest tests, or two files for pairwise comparison. <code>--out</code> - Output TSV file.</p>"},{"location":"modules/de/#optional-parameters","title":"Optional Parameters","text":"<p><code>--min-count-per-feature</code> - Minimum total count for a feature to be considered. Default: 100.</p> <p><code>--max-pval</code> - Max p-value for output. Default: 1e-3.</p> <p><code>--min-fc</code> - Minimum fold change. Default: 1.5.</p> <p><code>--min-count</code> - Minimum observed count for a (feature, factor) pair to be tested. Default: 10.</p> <p><code>--pseudocount</code> - Pseudocount added to each cell for the Chi-squared test. Default: 0.5.</p> <p><code>--threads</code> - Number of threads. Default: 1. (normally unnecessary)</p> <p><code>--neighbor-k</code> - Number of nearest neighbor columns to aggregate as background (single input only). Default: 3. Must be smaller than K-1. When set, writes an additional output file with 1-vs-neighbors results.</p> <p><code>--confusion</code> - (Pairwise mode only; experimental heuristics) Two confusion matrices to deconvolve the two input pseudobulk matrices before comparison.</p>"},{"location":"modules/de/#output","title":"Output","text":"<p>Single input output columns:</p> <p><code>Feature</code>, <code>Factor</code>, <code>Chi2</code>, <code>FoldChange</code>, <code>log10pval</code></p> <p>When <code>--neighbor-k &gt; 0</code>, an additional file is written with the same columns and the suffix <code>.1vsNeighbors.tsv</code> (if <code>--out</code> ends with <code>.tsv</code>, that extension is stripped first).</p> <p>Two-input output columns:</p> <p><code>Feature</code>, <code>Factor</code>, <code>Chi2</code>, <code>FoldChange</code>, <code>log10pval</code>, <code>Count1</code>, <code>Count2</code></p> <p>Rows are sorted by factor (ascending) and Chi2 (descending) within each factor.</p>"},{"location":"modules/de/#multi-conditional-de-pixel","title":"multi-conditional-de-pixel","text":"<p><code>multi-conditional-de-pixel</code> joins pixel-level annotation results with the original transcript data on the fly and performs cell-type-specific DE between dataset groups. It aggregates transcripts into grid units of size <code>--grid-size</code> stratified by pixel level cell type assignments then runs pairwise tests for each cell type using a Binomial model. By default it builds all pairwise contrasts from <code>--labels</code> (or numeric indices); for more general comparisons, use a contrast design file. The p-values are computed both by permutation and by robust sandwich estimators of the standard errors of the estimated effect sizes.</p> <p>\\(X^{(k)}_{im} \\sim\\) Binom \\((N^{(k)}_{im}, \\pi^{(k)}_{im})\\) for cell type \\(k\\), bin \\(i\\) and gene \\(m\\), where both \\(X\\) and \\(N\\) are soft-aggregated counts from pixel level cell type assignments.</p> <p>logit \\((\\pi^{(k)}_{im}) = a^{(k)}_m + y_i b^{(k)}_m\\), where \\(y_i\\) is a binary indicator for the dataset (0/1). The null hypothesis is \\(b^{(k)}_m = 0\\).</p> <p>Note: the intended use is when the cell type model is from external sources (e.g. apply <code>pixel-decode</code> with a reference-based cell type pseudobulk matrix). The interpretation is tricky if the cell types are learned from the same datasets. Either way, this is only a data exploration tool and we do not claim that it is statistically rigorous.</p> <p>Example usage (pairwise contrasts between input datasets): <pre><code>punkst multi-conditional-de-pixel \\\n  --anno sampleA/pixel sampleB/pixel --binary \\\n  --pts sampleA/transcripts.tiled sampleB/transcripts.tiled \\\n  --labels sampleA sampleB \\\n  --K 12 --features features.tsv \\\n  --grid-size 20 --min-count 10 --perm 5000 \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n  --out de/pixel_de --threads 8 --seed 1\n</code></pre></p> <p>Example usage with explicit contrast file: <pre><code>punkst multi-conditional-de-pixel \\\n  --contrast contrast.tsv \\\n  --K 12 --features features.tsv \\\n  --grid-size 20 --min-count 10 --perm 5000 \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n  --out de/pixel_de --threads 8 --seed 1\n</code></pre></p> <p><code>contrast.tsv</code> format (tab-delimited): <pre><code>anno_prefix pts_prefix  B_vs_A  C_vs_A\nsampleA/pixel   sampleA/transcripts.tiled   -1  -1\nsampleB/pixel   sampleB/transcripts.tiled   1   0\nsampleC/pixel   sampleC/transcripts.tiled   0   1\n</code></pre> Each contrast column uses <code>-1</code> (group 0), <code>1</code> (group 1), or <code>0</code> (exclude). Contrast names come from the header.</p>"},{"location":"modules/de/#required-parameters_1","title":"Required Parameters","text":"<p>Provide either <code>--anno</code>, <code>--pts</code> (or <code>--anno-data</code>/<code>--anno-index</code>, <code>--pts</code>) or a <code>--contrast</code> design file.</p> <p><code>--anno</code> - Prefixes of pixel annotation files (from <code>punkst pixel-decode</code>). For each prefix, the tool expects <code>&lt;prefix&gt;.tsv</code> (or <code>&lt;prefix&gt;.bin</code> if <code>--binary</code>) and <code>&lt;prefix&gt;.index</code>. Cannot be combined with <code>--contrast</code>.</p> <p><code>--anno-data</code>, <code>--anno-index</code> - Alternative to <code>--anno</code>. Provide explicit data and index files (same count for each).</p> <p><code>--pts</code> - Prefixes of transcript data files (from <code>punkst pts2tiles</code>). For each prefix, the tool expects <code>&lt;prefix&gt;.tsv</code> and <code>&lt;prefix&gt;.index</code>. Cannot be combined with <code>--contrast</code>.</p> <p><code>--contrast</code> - Contrast design TSV with columns: <code>anno_prefix</code>, <code>pts_prefix</code>, and one or more contrast columns (values -1/0/1). When provided, it replaces <code>--anno/--anno-data/--anno-index/--pts</code> and <code>--labels</code>.</p> <p><code>--K</code> - Number of factors in the annotation files.</p> <p><code>--features</code> - A list of features to test, one feature name per line (lines starting with <code>#</code> are ignored).</p> <p><code>--grid-size</code> - Grid size used to aggregate transcripts into units.</p> <p><code>--icol-x</code>, <code>--icol-y</code>, <code>--icol-feature</code>, <code>--icol-val</code> - Column indices for X/Y coordinates, feature name, and count in the transcript files (0-based).</p> <p><code>--out</code> - Output prefix.</p> <p>Annotation and transcript tiles must use the same tile size for each dataset pair (this is guaranteed if the pixel level decoding results are generated from the corresponding transcript tiles by <code>punkst pixel-decode</code>).</p>"},{"location":"modules/de/#optional-parameters_1","title":"Optional Parameters","text":"<p><code>--labels</code> - Labels for datasets used in pairwise output; defaults to <code>0..N-1</code>. Ignored when <code>--contrast</code> is supplied.</p> <p><code>--binary</code> - Indicates the annotation data files are binary (<code>.bin</code>).</p> <p><code>--min-count-per-feature</code> - Minimum total count (in each pairwise comparison) for a feature to be considered. Default: 100.</p> <p><code>--max-pval</code> - Max p-value for output. Default: 1 (output all).</p> <p><code>--max-pval-deconv</code> - If at least two cell types (slices) have a p-value less than this threshold, the tool runs a deconvolution to estimate effects adjusted for other cell types. Default: 1e-3.</p> <p><code>--min-or</code> - Minimum odds ratio for output (bidirectional, so OR\\(&gt;x\\) and OR\\(&lt;1/x\\) are kept). Default: 1 (output all).</p> <p><code>--min-count</code> - Minimum observed cell-type-specific count for a unit to be included. Default: 10.</p> <p><code>--perm</code> - Number of permutations for beta calibration (two-group contrasts). Default: 0 (model-based p-values only).</p> <p><code>--min-or-perm</code> - Minimum odds ratio for permutation testing (bidirectional). Default: 1.2.</p> <p><code>--seed</code> - Random seed for permutation.</p> <p><code>--threads</code> - Number of threads. Default: 1. Almost the entire runtime is spent in data loading, so multi-threading is very useful for large datasets since tiles of data are loaded simultaneously.</p> <p><code>--debug</code> - Enable debug logging.</p>"},{"location":"modules/de/#output-files","title":"Output Files","text":"<p>Given <code>--out PREFIX</code>, the tool generates one main output file per contrast, plus auxiliary files.</p> <ul> <li>When permutation is enabled (<code>--perm &gt; 0</code>), for each contrast:</li> <li> <p><code>PREFIX.CONTRAST.perm_N.tsv</code> is generated, where <code>CONTRAST</code> is from the <code>--contrast</code> file header or <code>labelA_vs_labelB</code> for auto-pairwise, and <code>N</code> is the number of permutations. It contains permutation-based p-values for significant associations. Columns are: <code>Slice</code>, <code>Feature</code>, <code>Beta</code>, <code>Pi0</code>, <code>Pi1</code>, <code>TotalCount</code>, <code>log10p</code>, <code>p_perm</code>.</p> </li> <li> <p>When permutation is not enabled (<code>--perm 0</code>), for each contrast:</p> </li> <li><code>PREFIX.CONTRAST.tsv</code> is generated.</li> <li> <p>Columns:</p> <ul> <li><code>Slice</code>: Cell type index (0-based).</li> <li><code>Feature</code>: Feature name.</li> <li><code>Beta</code>: Marginal log odds ratio for the feature.</li> <li><code>log10p</code>: -log10(p-value) for <code>Beta</code>.</li> <li><code>Pi0</code>, <code>Pi1</code>: Estimated feature prevalence in group 0 and 1.</li> <li><code>TotalCount</code>: Total feature count in this cell type and contrast.</li> <li><code>Beta_global</code>: Weighted average of <code>Beta</code> across cell types.</li> <li><code>log10p_global</code>: -log10(p-value) for <code>Beta_global</code>.</li> <li><code>log10p_deviation</code>: -log10(p-value) for the deviation of <code>Beta</code> from <code>Beta_global</code>.</li> <li><code>Beta_deconv</code>: Deconvolved <code>Beta</code>, adjusted for other cell types. This is experimental and computed only when deconvolution is triggered (see <code>--max-pval-deconv</code>).</li> <li><code>log10p_deconv</code>: -log10(p-value) for <code>Beta_deconv</code>.</li> </ul> </li> <li> <p>Auxiliary files:</p> </li> <li><code>PREFIX.nobs.tsv</code>: The number of units and total pixel counts per cell type and dataset.</li> <li><code>PREFIX.sums.tsv</code>: Total feature counts per cell type and dataset for units passing filters.</li> </ul>"},{"location":"modules/lda4hex/","title":"topic-model","text":"<p><code>topic-model</code> (alias for <code>lda4hex</code>) fits top model on the hexagon/spot/single cell data.</p>"},{"location":"modules/lda4hex/#input-format","title":"Input format","text":""},{"location":"modules/lda4hex/#custom-sparse-matrix-input-from-tiles2hex","title":"Custom sparse matrix input (from <code>tiles2hex</code>)","text":"<p>This format is required for training in streaming (minibatch) mode with low memory usage.</p> <p>The input data is a plain text file where each line containing the sparse encoding of the gene counts for one unit (hexagon, cell, etc.). The orders of the units should be randomized (it would be if you set <code>--randomize</code> in <code>tiles2hex</code>, or if you sorted the file by the first column (random keys)).</p> <p>If generated hexagon data from <code>tiles2hex</code> you probably don't need to know the following details.</p> <p>If you are converting from 10X single cell DGE, see <code>convert-10X-SC</code> for details.</p>"},{"location":"modules/lda4hex/#required-data","title":"Required data","text":"<p>The required structure of each line is as follows (entries are separated by tabs): - one integer (m) for the number of unique genes in this unit - one integer for the total count of all genes in this unit - followed by m pairs of integers, each pair consisting of a gene index (0-based) and the count of that gene in this unit (separated by a single space). In a cell-by-gene count matrix, the pairs are the (column, value) pairs of all non-zero entries in one row corresponding to a cell.</p> <p>There could be other fields in the input before the above required (m+2) fields, the number of data fields before the required fields should be specified under the key \"offset_data\" in the json metadata file.</p>"},{"location":"modules/lda4hex/#required-metadata","title":"Required metadata","text":"<p>We require a json file with at least the following information: - \"dictionary\": a dictionary that contains key: value pairs where each key is a gene name and each value is the corresponding index of that gene in the sparse encoding in the input data file. (You could skip this dictionary if you provides all and only the present genes' information in the order consistent with the indices (the column names and column sums in a cell-by-gene matrix) by <code>--features</code> in <code>lda2hex</code> (see below)) - \"offset_data\": an integer that specifies the number of fields before the required fields in the input data file. - \"header_info\": a list of size <code>offset_data</code> that contains the names of the fields before the required fields in the input data file. We will carry over these fields to the output files.</p>"},{"location":"modules/lda4hex/#10x-dge-input-single-cell","title":"10X DGE input (single-cell)","text":"<p>You can train directly from the 10X MEX format by passing <code>--in-dge-dir</code> or the explicit triplet <code>--in-barcodes</code>, <code>--in-features</code>, <code>--in-matrix</code>.</p> <p>When using 10X input, <code>--features</code> is required and must include per-feature totals (second column). These totals are used for prior scaling and background initialization.</p> <p>Due to the limit of the 10X DGE format, we load the entire dataset in memory and perform shuffling \\&amp; minibatching internally. For very large datasets, it would be memory intensive. You could use punkst convert-10X-SC to convert the 10X DGE to the custom format once if you plan to train multiple models on a very large dataset.</p>"},{"location":"modules/lda4hex/#usage","title":"Usage","text":"<pre><code>punkst topic-model --n-topics 12 --sort-topics \\\n--in-data ${path}/hex_12.randomized.txt --in-meta ${path}/hex_12.json \\\n--out-prefix ${path}/hex_12 --transform \\\n--min-count-train 50 --minibatch-size 512 --threads ${threads} --seed 1\n</code></pre>"},{"location":"modules/lda4hex/#required","title":"Required","text":"<p><code>--n-topics</code> - Specifies the number of topics to learn.</p> <p><code>--out-prefix</code> - Specifies the prefix for the output files.</p> <p>Input in one of the following two formats is required:</p>"},{"location":"modules/lda4hex/#for-the-custom-format","title":"For the custom format","text":"<p><code>--in-data</code> - Specifies the input data file (created by <code>tiles2hex</code>)</p> <p><code>--in-meta</code> - Specifies the metadata (file created by <code>tiles2hex</code>)</p>"},{"location":"modules/lda4hex/#for-10x-dge-format","title":"For 10X DGE format","text":"<p><code>--in-dge-dir</code> - Specifies the input directory that contains the 10X DGE files: <code>barcodes.tsv.gz</code>, <code>features.tsv.gz</code>, and <code>matrix.mtx.gz</code>.</p> <p>Alternatively, you can specify the three files directly by <code>--in-barcodes</code>, <code>--in-features</code>, and <code>--in-matrix</code>.</p> <p><code>--features</code> - A tsv file containing at least two columns: the feature names and the total feature count in the dataset. (You will get it if you used <code>pts2tiles</code> or <code>multisample-prepare</code>) It is required for 10X input though optional for the custom format.</p>"},{"location":"modules/lda4hex/#optional","title":"Optional","text":""},{"location":"modules/lda4hex/#feature-filtering","title":"Feature Filtering","text":"<p><code>--features</code> - Required and used only when either of the following three parameters are specified. Path to a file where the first column contains gene names and the second column contains the total count of that gene. It is also required for 10X input.</p> <p><code>--min-count-per-feature</code> - Minimum total count for features to be included. Require <code>--features</code> to be specified. Default: 1.</p> <p><code>--include-feature-regex</code> - Regular expression (modified ECMAScript grammar) to include only features matching this pattern. Default: include all features.</p> <p><code>--exclude-feature-regex</code> - Regular expression (modified ECMAScript) to exclude features matching this pattern. Default: exclude no features.</p> <p>Feature Selection Logic: the above three filters are applied jointly, so only genes with at least the minimum count, matching the include regex (if provided), and not matching the exclude regex (if provided) will be included in the model.</p>"},{"location":"modules/lda4hex/#feature-weighting","title":"Feature Weighting","text":"<p><code>--feature-weights</code> - Path to a file containing a weight for each gene. Format should be gene name (first column) and weight (second column). If the json metadata file does not contain a dictionary, the first column should be the gene index (0-based) instead.</p> <p><code>--default-weight</code> - Default weight for features not present in the weights file. Set to 0 to ignore features not in the weights file. Default: 1.0.</p>"},{"location":"modules/lda4hex/#lda-training-parameters","title":"LDA Training Parameters","text":"<p><code>--threads</code> - Number of threads to use. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p> <p><code>--minibatch-size</code> - Size of the minibatches to use during training. Default: 512.</p> <p><code>--min-count-train</code> - Minimum total count for a hexagon to be included in the training set. Default: 20.</p> <p><code>--n-epochs</code> - Number of epochs to train for. Default: 1.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in the e-step in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--max-iter</code> - Maximum number of iterations for each document. Default: 100.</p> <p><code>--kappa</code> - Learning decay parameter for online LDA. Default: 0.7.</p> <p><code>--tau0</code> - Learning offset parameter for online LDA. Default: 10.0.</p> <p><code>--alpha</code> - Document-topic prior. Default: 1/K (where K is the number of topics).</p> <p><code>--eta</code> - Topic-word prior. Default: 1/K (where K is the number of topics).</p>"},{"location":"modules/lda4hex/#model-initialization","title":"Model Initialization","text":"<p><code>--model-prior</code> - File that contains the initial model matrix.</p> <p><code>--prior-scale</code> - Scale the initial model matrix uniformly by this value. Default: use the matrix as is.</p> <p><code>--prior-scale-rel</code> - Scale the initial model matrix relative to the total feature counts in the data.</p>"},{"location":"modules/lda4hex/#output-control","title":"Output Control","text":"<p><code>--transform</code> - Transform the data to the LDA space after training. If set, an output file <code>&lt;prefix&gt;.results.tsv</code> will be created. For 10X input, identifiers are 0-based barcode indices (in the input <code>barcodes.tsv.gz</code>).</p> <p><code>--projection-only</code> - Transform the data using the prior model without further training. Implies <code>--transform</code>.</p> <p><code>--sort-topics</code> - Order the topics with decreasing abundance.</p> <p><code>--verbose</code> - Control the verbosity level of output messages.</p>"},{"location":"modules/pixel-decode/","title":"pixel-decode","text":""},{"location":"modules/pixel-decode/#overview","title":"Overview","text":"<p><code>pixel-decode</code> takes a trained LDA model and tiled pixel-level data to annotate each pixel with the top factors and their probabilities. This module enables spatial mapping of gene expression patterns at single-pixel resolution.</p> <pre><code>punkst pixel-decode --model ${path}/hex_12.model.tsv \\\n--in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--temp-dir ${tmpdir} --out-pref ${path}/pixel \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 \\\n--pixel-res 0.5 --threads ${threads} --seed 1 --output-original\n</code></pre> <p>The pixel-level inference result (in this case <code>${path}/pixel.tsv</code>, see below \"Output Parameters\" for writing binary files and other options) contains the coordinates and the inferred top factors and their posterior probabilities for each pixel. The module also creates a pseudobulk file (<code>${path}/pixel.pseudobulk.tsv</code>) where each row is a gene and each column is a factor.</p>"},{"location":"modules/pixel-decode/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - Specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> - Specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - Specify the columns with X and Y coordinates (0-based).</p> <p><code>--icol-feature</code> - Specifies the column index for feature (0-based).</p> <p><code>--icol-val</code> - Specifies the column index for count/value (0-based).</p> <p><code>--model</code> - Specifies the model file where the first column contains feature names and the subsequent columns contain the parameters for each factor. The format should match that created by <code>topic-model</code>.</p> <p><code>--temp-dir</code> - Specifies the directory to store temporary files.</p> <p>Output specification - One of these must be provided: <code>--out-pref</code> - Specifies the output prefix for all output files.</p> <p><code>--out</code> - (Deprecated, for backward compatibility) Specifies the output file.</p> <p>Hexagon grid parameters - One of these must be provided: <code>--hex-size</code> - Specifies the size (side length) of the hexagons for initializing anchors.</p> <p><code>--hex-grid-dist</code> - Specifies center-to-center distance in the axial coordinate system used to place anchors. Equals <code>hex-size * sqrt(3)</code>.</p> <p>Anchor spacing parameters - One of these must be provided: <code>--anchor-dist</code> - Specifies the distance between adjacent anchors.</p> <p><code>--n-moves</code> - Specifies the number of sliding moves in each axis to generate the anchors. If <code>--n-moves</code> is <code>n</code>, <code>anchor-dist</code> equals <code>hex-grid-dist</code> / <code>n</code>.</p>"},{"location":"modules/pixel-decode/#optional-parameters","title":"Optional Parameters","text":""},{"location":"modules/pixel-decode/#input-parameters","title":"Input Parameters","text":"<p><code>--coords-are-int</code> - If set, indicates that the coordinates are integers; otherwise, they are treated as floating point values.</p> <p><code>--feature-is-index</code> - If set, the values in <code>--icol-feature</code> are interpreted as feature indices. Otherwise, they are expected to be feature names.</p> <p><code>--feature-weights</code> - Specifies a file to weight each feature. The first column should contain the feature names, and the second column should contain the weights.</p> <p><code>--default-weight</code> - Specifies the default weight for features not present in the weights file (only if <code>--feature-weights</code> is specified). Default: 0.</p> <p><code>--anchor</code> - Specifies a file containing anchor points to use in addition to evenly spaced lattice points.</p>"},{"location":"modules/pixel-decode/#data-annotation-parameters","title":"Data Annotation Parameters","text":"<p><code>--ext-col-ints</code> - Additional integer columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-floats</code> - Additional float columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-strs</code> - Additional string columns to carry over to the output file. Format: \"idx1:name1:len1 idx2:name2:len2 ...\" where 'idx' are 0-based column indices and 'len' are maximum lengths of strings.</p>"},{"location":"modules/pixel-decode/#processing-parameters","title":"Processing Parameters","text":"<p><code>--pixel-res</code> - Resolution for the analysis, in the same unit as the input coordinates. Default: 1 (each pixel treated independently). Setting the resolution equivalent to 0.5-1\u03bcm is recommended, but it could be smaller if your data is very dense.</p> <p><code>--radius</code> - Specifies the radius within which to search for anchors. Default: <code>anchor-dist * 1.2</code>.</p> <p><code>--min-init-count</code> - Minimum total count within the hexagon around an anchor for it to be included. Filters out regions outside tissues with sparse noise. Default: 10.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--threads</code> - Number of threads to use for parallel processing. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p>"},{"location":"modules/pixel-decode/#output-parameters","title":"Output Parameters","text":"<p><code>--output-binary</code> - If set, the output files will be in binary format for more efficient post-processing, otherwise in tsv format. If set, <code>--output-original</code> cannot be used, but you can use <code>punkst tile-op</code> (see tile-op) to annotate the transcript data with the inference results or to convert the binary output back to tsv.</p> <p><code>--output-original</code> - If set, the original data including the feature names and counts will be included in the output. If <code>pixel-res</code> is not 1 and <code>--output-original</code> is not set, the output contains results per collapsed pixel.</p> <p><code>--use-ticket-system</code> - If set, the order of pixels in the output file is deterministic across runs (though not necessarily the same as the input order). May incur a small performance penalty.</p> <p><code>--top-k</code> - Number of top factors to include in the output. Default: 3.</p> <p><code>--output-coord-digits</code> - Number of decimal digits to output for coordinates (only used if input coordinates are float or <code>--output-original</code> is not set). Default: 4.</p> <p><code>--output-prob-digits</code> - Number of decimal digits to output for probabilities. Default: 4.</p> <p><code>--verbose</code> - Increase verbosity of output messages.</p> <p><code>--debug</code> - Enable debug mode for additional diagnostic information.</p>"},{"location":"modules/pixel-decode/#process-multiple-samples","title":"Process multiple samples","text":"<p>If you want to project the same model onto multiple datasets/samples, you can use <code>--sample-list</code> to pass a tsv file containing all samples' information. (In this case, <code>--in-tsv</code> and <code>--in-index</code> are ignored.) Note: all other parameters are shared across samples, so the input files should have the same structure.</p> <p>If your multi-sample data are generated by <code>punkst multisample-prepare</code>, it has created a file named <code>*.persample_file_list.tsv</code>. You can just pass this file to <code>--sample-list</code> and optionally use <code>--out-pref</code> to specify an identifier (e.g., the model information) to add to each output file name.</p> <p>If you created the input file for each sample manually, you can create a tsv file with at least three columns: sample_id, path to the transcript file created by <code>pts2tiles</code>, path to the index file created by <code>pts2tiles</code>.</p> <p>Optional fourth column: output prefix.</p> <p>Optional fifth column: anchor file path.</p> <p>If there are headers, all header lines should start with \"#\".</p>"},{"location":"modules/pixel-decode/#output-files","title":"Output Files","text":"<p>The following files are generated (with prefix specified by <code>--out-pref</code>):</p> <p><code>&lt;prefix&gt;.tsv</code> - Main output file with pixel-level factor assignments</p> <p><code>&lt;prefix&gt;.pseudobulk.tsv</code> - Gene-by-factor matrix showing feature distribution across topics</p>"},{"location":"modules/pixel-decode/#example-usage-scenarios","title":"Example Usage Scenarios","text":""},{"location":"modules/pixel-decode/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8\n</code></pre>"},{"location":"modules/pixel-decode/#with-data-annotations","title":"With Data Annotations","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8 \\\n--ext-col-ints 4:celltype 5:cluster --ext-col-strs 6:sample_id:20 --output-original\n</code></pre>"},{"location":"modules/pixel-decode/#with-multiple-input-created-by-punkst-multisample-prepare","title":"With multiple input created by <code>punkst multisample-prepare</code>","text":"<pre><code>punkst pixel-decode --model model.tsv --sample-list multi.persample_file_list.tsv \\\n--temp-dir /tmp --out-pref results \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 --threads 8\n</code></pre>"},{"location":"modules/poisnmf/","title":"Poisson NMF with log(1+x) link","text":"<p><code>nmf-pois-log1p</code> fits a non-negative matrix factorization model with Poisson likelihood to single cell or spot level count data. It fits the model \\(y_{im} \\sim Pois(\\lambda_{im}), \\ \\ g(\\lambda_{im}) = \\theta_i^T \\beta_m\\) where \\(g(\\lambda_{im}) = \\log(1+\\lambda_{im}/c_i)\\) with unit (cell) specific scaling \\(c_i\\).</p>"},{"location":"modules/poisnmf/#input-format","title":"Input format","text":"<p>The input data format is the same as for <code>topic-model</code>/<code>lda4hex</code>, (could be enerated from pixel level data by <code>tiles2hex</code>), including a data file and a corresponding metadata JSON file.</p>"},{"location":"modules/poisnmf/#usage","title":"Usage","text":"<pre><code>punkst nmf-pois-log1p --in-data hex_data.txt --in-meta hex_meta.json \\\n--K 15 --out-prefix nmf_results --threads 8 --seed 1984\n</code></pre>"},{"location":"modules/poisnmf/#required","title":"Required","text":"<p><code>--in-data</code> - Input data file (created by <code>tiles2hex</code>).</p> <p><code>--in-meta</code> - Metadata file created by <code>tiles2hex</code>.</p> <p><code>--K</code> - The number of factors (topics) to learn.</p> <p><code>--out-prefix</code> - Prefix for the output files.</p>"},{"location":"modules/poisnmf/#optional","title":"Optional","text":""},{"location":"modules/poisnmf/#data-and-feature-filtering","title":"Data and Feature Filtering","text":"<p>(Similar to <code>topic-model</code>/<code>lda4hex</code>)</p> <p><code>--features</code> - Path to a file where the first column contains gene names and the second column contains the total count of that gene. Used for filtering.</p> <p><code>--min-count-per-feature</code> - Minimum total count for a feature to be included. Requires <code>--features</code>. Default: 100.</p> <p><code>--include-feature-regex</code> - Regular expression to include only features matching this pattern.</p> <p><code>--exclude-feature-regex</code> - Regular expression to exclude features matching this pattern.</p> <p><code>--min-count-train</code> - Minimum total count for a unit (hexagon/cell) to be included in training. Default: 50.</p>"},{"location":"modules/poisnmf/#model-and-training-parameters","title":"Model and Training Parameters","text":"<p><code>--mode</code> - Optimization algorithm to use in each Poisson regression subproblem. 1 for TRON (trust-region Newton-CG, default), 2 for Monotone FISTA (an accelerated gradient algorithm), 3 for a Newton's method with line search, 0 for Newton's method without line search.</p> <p><code>--size-factor</code> - A constant to scale the total counts per unit (matching \\(L\\) in Seurat), used to calculate the per-observation scaling parameter \\(c_i=\\frac{\\sum_m y_{im}}{L}\\) in \\(g(\\lambda_{im}) = \\log(1+\\lambda_{im}/c_i)\\). Default: 10000.</p> <p><code>--c</code> - If specified, use a constant <code>c</code> for all units instead of calculating it from <code>size-factor</code>. Default: -1 (per-unit <code>c</code>).</p> <p><code>--feature-residuals</code> - If set, output per-feature residuals for diagnosis.</p> <p><code>--max-iter-outer</code> - Maximum number of outer loop iterations (alternating between updating \\(\\theta\\) and \\(\\beta\\)). Default: 50, but it often approximates convergence in \\(\\lt 10\\) terations.</p> <p><code>--max-iter-inner</code> - Maximum number of iterations for each Poisson regression optimization. Default: 20.</p> <p><code>--tol-outer</code> - Convergence tolerance for the outer loop. Default: 1e-5.</p> <p><code>--tol-inner</code> - Convergence tolerance for the inner optimization problems. Default: 1e-6.</p> <p><code>--exact</code> - Use the exact likelihood for all observations. If not set, uses a second-order approximatio for zero-count observations to speed up computation.</p> <p><code>--seed</code> - Random seed for reproducibility.</p> <p><code>--threads</code> - Number of threads to use. Default: 1.</p>"},{"location":"modules/poisnmf/#experimental-including-covariates","title":"Experimental: including covariates","text":"<p>We fit the model \\(g(\\lambda_{im}) = \\theta_i^T \\beta_m + x_i^T b_m\\) where \\(x_i\\) contains covariates for unit \\(i\\) provided in <code>--in-covar</code>.</p> <p>Note: covariates and their effects are not constrained to be non-negative unless you specify so. We tried to make the optimization numerically stable but it may not always be interpretable if the non-negative \\(\\theta_i^T \\beta_m\\) part is dominated by the covariate effects.</p> <p><code>--in-covar</code> - Path to a file containing covariates, with a header line. The number of rows and the order of units should exactly match that in the input count data <code>--in-data</code>. Currently only numerical covariates are supported, you might try to fit categorical covariates as one-hot encoded binary variables.</p> <p><code>--icol-covar</code> - Column indices (0-based) in the covariate file to use. If not specified, all columns except the first are used.</p> <p><code>--allow-na</code> - If set, non-numerical values in covariate columns are replaced with 0. Otherwise, all values must be numerical.</p> <p><code>--covar-coef-min</code> - Lower bound for covariate coefficients. Default: -1e6.</p> <p><code>--covar-coef-max</code> - Upper bound for covariate coefficients. Default: 1e6.</p> <p>Example usage with covariates:</p> <pre><code>punkst nmf-pois-log1p --in-data hex_data.txt --in-meta hex_meta.json \\\n--K 15 --out-prefix nmf_results --threads 8 --seed 1984 \\\n--in-covar covars.tsv --icol-covar 1 3 5\n</code></pre>"},{"location":"modules/poisnmf/#experimental-compute-de-statistics-from-covhat-beta","title":"Experimental: compute DE statistics from Cov(\\(\\hat \\beta\\))","text":"<p><code>--detest-vs-avg</code> - Compute DE statistics for each factor vs the average, for each feature. Set to <code>1</code> to use Fisher's information based covariance, <code>2</code> to use the robust sandwich estimator, or <code>3</code> to compute both. Default: 0 (skip covariance computation so no DE output).</p> <p><code>--min-fc</code> - Minimum approximated fold change to report. Note: due to the nonlinear link, we don't have an exact fold change interpretation, the reported value is \\((\\exp(\\beta_{km})-1)/(\\exp(\\beta^0_{m})-1)\\) where \\(\\beta^0_m\\) is a weighted average of \\(\\beta_m\\) across factors and \\(\\theta\\) is scaled to have column sums equal to \\(N/K\\). Default: 1.5.</p> <p><code>--max-p</code> - Maximum p-value to report. Default: 0.05.</p> <p><code>--min-ct</code> - Minimum total count for a feature to be included in DE testing. Default: 100.</p>"},{"location":"modules/poisnmf/#output-files","title":"Output files","text":"<ul> <li> <p><code>{prefix}.model.tsv</code>: The feature-factor matrix (\\(\\beta\\)), where rows are features (genes) and columns are factors. It is scaled such that the column sums (\\(\\sum_m \\beta_{km}\\) for each \\(k\\)) all equal to \\(M\\), the number of features.</p> </li> <li> <p><code>{prefix}.theta.tsv</code>: The unit-factor matrix (\\(\\theta\\)), where rows are units (cells/hexagons) and columns are factors. The relative magnitudes of values within each row are meaningful, and the total magnitude of each row is correlated with the total counts of that unit.</p> </li> <li> <p><code>{prefix}.fit_stats.tsv</code>: Per-unit (cell/hexagon) statistics including total counts, log-likelihood, residuals, and the approximated variance of the Poisson rate estimates.</p> </li> <li> <p><code>{prefix}.covar.tsv</code>: (If covariates are used) The feature-covariate coefficient matrix containing \\(b_{jm}\\) for each covariate \\(j\\) and feature \\(m\\).</p> </li> <li> <p><code>{prefix}.de.{method}.tsv</code>: (If <code>--detest-vs-avg</code> is set) Differential expression statistics for each factor vs the average, for each feature. <code>{method}</code> is either <code>fisher</code>, <code>robust</code>, depending on <code>--se-method</code>.</p> </li> <li> <p><code>{prefix}.feature.residuals.tsv</code>: (If <code>--feature-residuals</code> is set) Per-feature total residual (averaged over all data points).</p> </li> </ul>"},{"location":"modules/pts2tiles/","title":"pts2tiles","text":""},{"location":"modules/pts2tiles/#group-pixels-to-tiles-for-faster-processing","title":"Group pixels to tiles for faster processing","text":"<p><code>pts2tiles</code> creates a plain tsv file that reorders the lines in the input file so that coordinates are grouped into non-overlapping square tiles. The ordering of lines within a tile is not guaranteed. It also creates an index file storing the offset of each tile to support fast access.</p> <p>Example usage <pre><code>punkst pts2tiles --in-tsv ${path}/transcripts.tsv \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 0 --tile-size 500 \\\n--temp-dir ${tmpdir} --out-prefix ${path}/transcripts.tiled --threads ${threads}\n</code></pre></p>"},{"location":"modules/pts2tiles/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - The input TSV file containing spatial data points. It could be a gzipped file ending with <code>.gz</code>. If the input is a stream from stdin, use <code>-</code> as the filename.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - The column indices for X and Y coordinates (0-based).</p> <p><code>--tile-size</code> - The size (side length) of the square tiles. The unit is the same as the coordinates in the input file.</p> <p><code>--out-prefix</code> - The prefix for all output files.</p> <p><code>--temp-dir</code> - The directory for storing temporary files during processing.</p> <p><code>--icol-feature</code> - The column index for feature names/IDs (0-based). If provided, the module will generate a file including feature names and counts. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features.)</p> <p><code>--icol-int</code> - Column indices for integer values to aggregate per feature. Can be specified multiple times to track multiple integer columns. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features, preferably excluding the extremely low count features.)</p>"},{"location":"modules/pts2tiles/#optional-parameters","title":"Optional Parameters","text":"<p><code>--skip</code> - The number of lines to skip in the input file (if your input file contains headers, set it to the number of header lines). Default: 0.</p> <p><code>--tile-buffer</code> - The per-thread per-tile buffer size in terms of the number of lines before writing to disk. Default: 1000. If the number of tiles may be huge and you are using a large number of threads so that the total memory usage is too high, choose a smaller number.</p> <p><code>--threads</code> - The number of threads to use for parallel processing. Default: 1.</p> <p><code>--verbose</code> - Controls the verbosity level of output messages.</p> <p><code>--debug</code> - Enables additional debug output.</p>"},{"location":"modules/pts2tiles/#output-files","title":"Output Files","text":"<ul> <li><code>prefix.tsv</code>: the tiled tsv file.</li> <li><code>prefix.index</code>: an index file that stores the offsets of each tile in the tiled tsv file. This will be used for fast access.</li> <li><code>prefix.coord_range.tsv</code>: a text file that contains the range of coordinates (xmin, xmax, ymin, ymax).</li> <li><code>prefix.features.tsv</code>: a tsv file containing the feature names and their aggregated values. This file is only generated if <code>--icol-feature</code> is specified.</li> </ul>"},{"location":"modules/tileop/","title":"tile-op","text":"<p><code>tile-op</code> provides utilities to view and manipulate the tiled data files created by <code>punkst pixel-decode</code> or <code>punkst pts2tiles</code>.</p> <p>Functionalities:</p> <ul> <li> <p>inspecting the index</p> </li> <li> <p>converting binary tiled files to TSV</p> </li> <li> <p>reorganizing fragmented tiles to a regular grid</p> </li> <li> <p>merging multiple inference results</p> </li> <li> <p>annotating (tiled) point level file with inference results</p> </li> <li> <p>compute joint probability distributions of factors</p> </li> <li> <p>compute confusion matrix among factors at a given resolution</p> </li> <li> <p>denoise and keep only the top predicted factor per pixel</p> </li> <li> <p>aggregate pixel level inference results by cell and subcellular compartments based in transcript/pixel level annotations</p> </li> </ul> <p>(Except for printing the index, all operations are intended to be used separately)</p>"},{"location":"modules/tileop/#usage","title":"Usage","text":""},{"location":"modules/tileop/#main-input-output","title":"Main input &amp; output","text":"<p>The main input are the tiled pixel level files created by <code>punkst pixel-decode</code>, either in the custom binary format or in plain TSV format.</p> <p>You can specify the pair of data and index files using <code>--in-data</code> and <code>--in-index</code>, or specify the prefix using <code>--in</code>. When using <code>--in</code>, without <code>--binary</code>, the tool assumes the data file is <code>&lt;in&gt;.tsv</code> and the index file is <code>&lt;in&gt;.index</code>, and with <code>--binary</code> it assumes the data file is <code>&lt;in&gt;.bin</code> and the index file is <code>&lt;in&gt;.index</code>.</p> <p>Use <code>--out</code> to specify the output prefix. In some operations use <code>--binary-out</code> to specify that the output is to be written in binary format.</p>"},{"location":"modules/tileop/#basic-inspection-and-conversion","title":"Basic Inspection and Conversion","text":"<p>To inspect the index of a tiled file:</p> <pre><code>punkst tile-op --print-index --in path/prefix [--binary]\n</code></pre> <p>To dump a binary tiled file to a plain TSV file:</p> <pre><code>punkst tile-op --dump-tsv --in path/prefix --binary --out path/prefix.dump\n</code></pre> <p>The output include <code>path/prefix.dump.tsv</code> and <code>path/prefix.dump.index</code>.</p>"},{"location":"modules/tileop/#fix-fragmented-tiles","title":"Fix fragmented Tiles","text":"<p>The output of <code>punkst pixel-decode</code> is organized into non-overlapping rectangular tiles that jointly cover the entire space, but the tiles do not fit into a regular grid.</p> <p>If we would need to merge multiple sets of inference results or want to join the inference results with point level data, currently we have to reorganize the data to a regular grid first. (The tile size shoud be already stored in the input's index file (<code>path/prefix.index</code>), currently we don't support generic reorganization)</p> <p>Note that this is not required for visualization <code>draw-pixel-factors</code>.</p> <pre><code>punkst tile-op --reorganize --in path/prefix [--binary] --out path/reorg_prefix\n</code></pre>"},{"location":"modules/tileop/#merge-multiple-inference-results","title":"Merge Multiple Inference Results","text":"<p>You can merge multiple inference files (e.g., from different models) into a single file. This finds the intersection of tiles and concatenates the results ((factor, probability) pairs) for each pixel.</p> <pre><code>punkst tile-op --in path/result1 [--binary] \\\n  --merge-emb path/result2.tsv path/result3.bin --k2keep 3 1 2 \\\n  --out path/merged_result --binary-out\n</code></pre> <p><code>--merge-emb</code> - One or more other inference files (created by <code>pixel-decode</code>) to merge with the main input file. They can be in either TSV or binary format, but have to have proper index files stored ad <code>&lt;prefix&gt;.index</code>.</p> <p><code>--k2keep</code> - (Optional) A list of integers specifying how many top factors to keep from each source file (including the main input). If not provided, all factors are kept.</p> <p><code>--binary-out</code> - (Optional) Save the merged output in binary format instead of TSV.</p> <p>In the above example, from file <code>result1.bin</code> (or <code>.tsv</code>) we keep top 3 factors, from <code>result2.tsv</code> we keep top 1 factor, and from <code>result3.bin</code> we keep top 2 factors. If the specified number exceeds the number of factors available in the corresponding file, all factors in the file are kept.</p>"},{"location":"modules/tileop/#annotate-points-with-inference-results","title":"Annotate Points with Inference Results","text":"<p>You can annotate a transcript file with the inference results. The query file is required to be generated by <code>punkst pts2tiles</code> with the same tile structure as the result file so that the tool can efficiently join it with the inference results, but you can apply <code>pts2tiles</code> to any tsv file that contains X, Y coordinates as two of its columns.</p> <pre><code>punkst tile-op --in path/prefix [--binary] \\\n  --annotate-pts path/transcripts --icol-x 0 --icol-y 1 \\\n  --out path/merged\n</code></pre> <p><code>--annotate-pts</code> - Prefix of the points file (the tool expects <code>&lt;prefix&gt;.tsv</code> and <code>&lt;prefix&gt;.index</code>) to be annotated.</p> <p><code>--icol-x</code> - 0-based column index for X coordinate in the points file.</p> <p><code>--icol-y</code> - 0-based column index for Y coordinate in the points file.</p>"},{"location":"modules/tileop/#compute-joint-probability-distributions","title":"Compute Joint Probability Distributions","text":"<p>You can compute the correlations or co-occurrences between factors, either from a single model or between inference results from multiple models applied to the same dataset. This is approximated by the sum of products of posterior probabilities across all pixels, although for each pixel only the top-K factors are considered (those stored in the inference result file). To compute co-occurrence between factors in a single model at different spatial resolutions, see the confusion matrix operation below.</p>"},{"location":"modules/tileop/#single-input","title":"Single Input","text":"<p>For a single inference result file:</p> <p><pre><code>punkst tile-op --prob-dot --in path/result [--binary] --out path/out_prefix\n</code></pre> Output:</p> <ul> <li> <p><code>path/out_prefix.marginal.tsv</code>: Marginal sums of posterior probabilities for each factor.</p> </li> <li> <p><code>path/out_prefix.joint.tsv</code>: Sum of products for each pair of factors.</p> </li> </ul> <p>If the file contains multiple sets of results (e.g. a merged), the output is the same as the multi-input case below, where it stores marginal and within-model joint output for each source separately, and produces cross-source products (e.g., <code>path/out_prefix.0v1.cross.tsv</code>).</p>"},{"location":"modules/tileop/#merging-and-computing-on-the-fly","title":"Merging and Computing on the Fly","text":"<p>You can also compute these statistics while merging multiple inference result files on the fly, without writing the large merged file to disk.</p> <pre><code>punkst tile-op --prob-dot --in path/result1 [--binary] \\\n  --merge-emb path/result2.tsv path/result3.bin \\\n  --out path/out_prefix\n</code></pre> <p>This supports <code>--k2keep</code> to reduce the number of top-K factors used in each source before computing the products.</p> <p>Output:</p> <ul> <li> <p><code>path/out_prefix.0.marginal.tsv</code>, <code>path/out_prefix.1.marginal.tsv</code>, ... (one per input source)</p> </li> <li> <p><code>path/out_prefix.0.joint.tsv</code>, ... (internal dot products for each source)</p> </li> <li> <p><code>path/out_prefix.0v1.cross.tsv</code>, <code>path/out_prefix.0v2.cross.tsv</code>, ... (cross-source dot products with <code>log10pval</code> from a naive chi-squared 2x2 enrichment test)</p> </li> </ul>"},{"location":"modules/tileop/#compute-confusion-matrix","title":"Compute Confusion Matrix","text":"<p>This operation computes a confusion matrix of factors at a given spatial resolution. It divides the space into squares of a specified size, identifies the top factor for each square, and then builds a matrix of co-occurrences.</p> <pre><code>punkst tile-op --confusion 10 --in path/result [--binary] --out path/out_prefix\n</code></pre> <p><code>--confusion</code> - The resolution (side length of square bins in microns) for computing the confusion matrix.</p> <p>Output:</p> <ul> <li><code>path/out_prefix.confusion.tsv</code>: A matrix of co-occurrence counts between factors.</li> </ul>"},{"location":"modules/tileop/#aggregate-results-by-cell","title":"Aggregate Results by Cell","text":"<p>This operation aggregates pixel-level inference results at cell and subcellular compartment level, based on the tailed transcript file that contains cell/compartment annotations per transcript/pixel. If your data is from CosMx, Xenium, or Visium MERSCOPE, you should have run <code>punkst pts2tiles</code> on the raw transcript file which contains cell ID and possibly a column indicating if the transcript is nuclear or cytoplasmic. Then the tailed file already contains the necessary information.</p> <pre><code>punkst tile-op --annotate-cell --in path/result [--binary] \\\n  --annotate-pts path/transcripts_with_cells \\\n  --icol-x 0 --icol-y 1 --icol-c 5 --icol-s 6 \\\n  --out path/cellular_results\n</code></pre> <p>This command will summarize the factors for each cell ID found in <code>path/transcripts_with_cells.tsv</code>.</p> <p><code>--annotate-cell</code> - Flag to enable aggregation by cell.</p> <p><code>--annotate-pts</code> - Prefix of the points file (e.g. transcripts) containing cell annotations.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - 0-based column indices for X and Y coordinates.</p> <p><code>--icol-z</code> - (Optional) 0-based column index for Z coordinate.</p> <p><code>--icol-c</code> - 0-based column index for the cell ID.</p> <p><code>--icol-s</code> - (Optional) 0-based column index for subcellular component annotations. If provided, results will be aggregated per-cell and per-component.</p> <p><code>--k-out</code> - (Optional) Number of top factors to include in the output for each cell/component. If not provided, the same number of in the input file is used.</p> <p><code>--max-cell-diameter</code> - (Optional) The maximum expected diameter of a cell in microns. Used for avoiding boundary effects as we process by tiles. Default is 50.</p> <p>Output:</p> <p>A TSV file <code>path/cellular_results.tsv</code> containing aggregated factor probabilities for each cell (and component, if specified).</p> <p>A TSV file <code>path/cellular_results.pseudobulk.tsv</code> containing the sum of factor probabilities across each subcellular component. Useful for comparing global factor abundance between components.</p>"},{"location":"modules/tileop/#denoise-top-labels","title":"Denoise Top Labels","text":"<p>This is a heuristic denoising operation on the top-predicted factor labels for each pixel. It replace pixels where the predicted factor differs from most of its neighbors with the majority vote among its neighbors. It is meant for the case where you projected categorical cell types at high resolution data where you do not expect to see much mixing of cell types at single pixel level. The output is a new tiled data file where for each pixel, only the smoothed top factor is kept. (The output can be used as input for <code>tile-op</code>, so you can dump it to a tsv file or do other operations)</p> <pre><code>punkst tile-op --smooth-top-labels 2 --in path/result [--binary] --out path/smoothed_result\n</code></pre> <p><code>--smooth-top-labels</code> - The number of rounds to perform the denoising operation. A value greater than 0 enables the operation. One or two rounds is usually sufficient.</p>"},{"location":"modules/tiles2hex/","title":"tiles2hex","text":"<p><code>tiles2hex</code> groups pixels into nonoverlapping hexagons for spot level analysis.</p> <p>The input is the tiled data created by <code>pts2tiles</code>. The output is a plain tab-delimited text file, each line representing one hexagon intended for internal use. It also writes metadata to a json file.</p>"},{"location":"modules/tiles2hex/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--feature-dict ${path}/features.txt \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n--min-count 20 --hex-grid-dist 12 \\\n--out ${path}/hex_12.txt --randomize \\\n--temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code>, <code>--icol-feature</code> specify the column indices corresponding to X and Y coordinates and feature (0-based).</p> <p><code>--icol-int</code> specifies the column index for count/value (0-based). You can specify multiple count columns with <code>--icol-int</code>, separated by space.</p> <p><code>--hex-size</code> specifies the side length of the hexagons. The unit is the same as the coordinates in the input file.</p> <p><code>--out</code> specifies the output file.</p>"},{"location":"modules/tiles2hex/#optional-parameters","title":"Optional Parameters","text":"<p><code>--feature-dict</code> specifies a file with the names of features, one per line. It is used only if the values in <code>--icol-feature</code> are to be interpreted as feature names not indices. Features not present in the file will be ignored. (If the input file contains feature indices instead of names, all features will be included in the output)</p> <p><code>--min-count</code> specifies the minimum count for a hexagon to be included in the output.</p> <p><code>--randomize</code> if set, the order of hexagons in the output will be randomized.</p> <p><code>--temp-dir</code> specifies the directory for temporary files.</p> <p><code>--threads</code> specifies the number of threads to use.</p>"},{"location":"modules/tiles2hex/#output-format","title":"Output Format","text":"<p>The output is a plain tab-delimited text file. It is not a table: each line contains data for one unit and lines have different number of tokens.</p> <p>The first element in each line of the output is a random key, which is used to shuffle the data before model training. When <code>--randomize</code> is not set when you run <code>tiles2hex</code>, you can do the following <pre><code>sort -k1,1 --parallel ${threads} -S 1G ${path}/hex.txt -o ${path}/hex.txt\n</code></pre> If you use <code>topic-model</code>, you should always shuffle the hexagon file.</p> <p>The remaining of each line is structured as follows:</p> <p>In the basic case, the next two integers after the random key are coordinates (horizontal and vertical) in the axial hexagonal coordinate system.</p> <p>The next 2K tokens (K pairs of non-negative integers) are the number of unique features (\\(M_k\\)) and the total count (\\(C_k\\)) for each modality. The number of modalities (K) is the same as the number of column indices specified in <code>--icol-int</code>.</p> <p>Then there are K chunks of feature values, the k-th chunk containing \\(2M_k\\) (or \\(M_k\\) values) of non-negative integers where \\(M_k\\) is what you read from the previous tokens. The first number in each pair is the indices of the feature, the second is the count of that feature in the hexagon. The indices are 0-based and correspond to the order of features in the <code>--feature-dict</code> file. If <code>--feature-dict</code> is not provided (so the input already codes features as indices), the indices are the same as those in the input file.</p>"},{"location":"modules/tiles2hex/#advanced-usage-spatial-stratification-by-anchor-points","title":"Advanced Usage: Spatial Stratification By Anchor Points","text":"<p><code>tiles2hex</code> can also create multiple sets of units that group pixels that are close to user-provided anchor points. This is useful for creating units stratified by known biological structures for downstream clustering or factorization. A tested use case is to provide nuclear centers as anchor points so likely-nuclear and likely-cytoplasmic pixels are grouped separately.</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index transcripts.tiled.index --feature-dict ${path}/features.txt --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --min-count 20 --hex-size ${hex_size} --anchor-files ${path}/anchors1.txt ${path}/anchors2.txt --radius ${radius1} ${radius2} --out ${path}/hex.txt --temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#additional-parameters-for-anchor-based-analysis","title":"Additional Parameters for Anchor-Based Analysis","text":"<p><code>--anchor-files</code> specifies one or more files containing anchor points. Each anchor file should contain coordinates (x, y) separated by space, one anchor point per line. You can provide multiple anchor files to define different sets of anchor points, separated by space.</p> <p><code>--radius</code> specifies the radius around each anchor point within which pixels will be associated with that anchor. The unit is the same as the coordinates in the input file. You must provide one radius value for each anchor file, in the matched order.</p> <p><code>--ignore-background</code> if set, pixels that are not within the radius of any anchor point will be ignored. By default, these background pixels are included as a separate layer.</p>"},{"location":"modules/tiles2hex/#output-format-for-anchor-based-analysis","title":"Output Format for Anchor-Based Analysis","text":"<p>The output format is similar to the basic usage, but each hexagon also includes a non-negative index as the second token, indicating which anchor set it belongs to. The metadata JSON file includes an additional integer field <code>n_layers</code> recording the number of layers, or the number of anchor sets used (plus one if background is included).</p>"},{"location":"modules/visualization/","title":"Visualization","text":""},{"location":"modules/visualization/#high-resolution-image-of-pixel-level-factorization-results","title":"High resolution image of pixel level factorization results","text":"<p><code>draw-pixel-factors</code> visualizes the results of <code>pixel-decode</code></p> <pre><code>punkst draw-pixel-factors --in ${path}/pixel.decode --in-color ${path}/color.rgb.tsv --out ${path}/pixel.png --scale 1 --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre> <p><code>--in</code> specifies the prefix of the input data and index files created by <code>pixel-decode</code>. The tool will look for <code>${in}.bin</code> (or <code>${in}.tsv</code>) and <code>${in}.index</code>.</p> <p><code>--binary</code> indicates the input data is in binary format (if you used <code>punkst pixel-decode</code> with <code>--output-binary</code>)</p> <p><code>--out</code> specifies the output png file.</p> <p><code>--in-color</code> specifies a tsv file with the colors for each factor. The first three columns will be interpreted as R, G, B values in the range \\(0-255\\). The valid lines will be assigned to factors in the order they appear in this file. An example color table can be found in <code>punkst/ext/py/cmap.48.tsv</code>.</p> <p><code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code> specify the range of the coordinates. If your data is generated by <code>punkst pixel-decode</code> and you want to visualize the whole area, these parameters are unnecessary.</p> <p><code>--scale</code> (default 1) scales input coordinates to pixels in the output image. <code>int((x-xmin)/scale)</code> equals the horizontal pixel coordinate in the image. Since coordinates are normally in micrometer and the analysis resolution is typically 0.5 or 1, <code>--scale 1</code> (default) is usually a good choice. For Visium HD data where the data resolution is 2um, use <code>--scale 2</code>.</p> <p><code>--min-prob</code> (default: 1e-3) Minimum probability to consider a pixel.</p> <p><code>--top-only</code> Use only the top predicted factor per pixel.</p> <p>If your specified <code>--transform</code> in <code>topic-model</code>, one way to create the color table is to use the helper python script <pre><code>python punkst/ext/py/color_helper.py --input ${path}/prefix.results.tsv --output ${path}/color\n</code></pre></p> <p>Visualize selected factors</p> <p>Alternatively, you can specify a few factors and their corresponding colors directly:</p> <p><code>--channel-list</code> A list of factors to visualize</p> <p><code>--color-list</code> A list of colors in hex code (#RRGGBB).</p> <p>Denoise</p> <p>When <code>--top-only</code> is used, you could optionally smooth out isolated noisy pixels. This is heuristic, and may only be meaningful when you projected categorical cell types.</p> <p><code>--island-smooth</code> The number of iterations to remove isolated noisy pixels different from most of its neighbors. One or two iterations is recommended.</p> <p><code>--fill-empty-islands</code> Fill empty pixels surrounded by consistent neighbors (only activated when --island-smooth is set).</p>"},{"location":"modules/visualization/#html-report-for-factor-weights-and-top-genes","title":"HTML report for factor weights and top genes","text":"<p><code>factor_report.py</code> generates HTML reports summarizing factor characteristics and top genes</p> <p>The script generates an interactive HTML report (<code>${output_pref}.factor.info.html</code>) and a TSV summary (<code>${output_pref}.factor.info.tsv</code>) containing factor weights, top differentially expressed genes, and visualization colors.</p> <pre><code>python ext/py/factor_report.py --de ${path}/de_bulk.tsv --pseudobulk ${path}/pseudobulk.tsv --color_table ${path}/color.rgb.tsv --output_pref ${path}/report\n</code></pre> <p><code>--de</code> specifies the differential expression results file from <code>de-chisq</code></p> <p><code>--de_neighbor</code> optionally specifies a <code>de-chisq</code> neighbor-output file (e.g. <code>${out}.1vsNeighbors.tsv</code>) to add a column displaying high specific top genes.</p> <p><code>--pseudobulk</code> specifies the pseudobulk count table.</p> <p><code>--color_table</code> specifies the RGB color table for factors. This probably should be the same file as that used for <code>punkst draw-pixel-factors</code>.</p> <p><code>--output_pref</code> specifies the output prefix for generated files.</p> <p><code>--feature_label</code> specifies the column name for features (default: \"Feature\").</p> <p><code>--n_top_gene</code> maximum number of top genes to include in report (default: 20).</p> <p><code>--min_top_gene</code> minimum number of top genes to show per factor (default: 10).</p> <p><code>--max_pval</code> maximum p-value threshold for significant genes (default: 0.001).</p> <p><code>--min_fc</code> minimum fold change threshold for significant genes (default: 1.5).</p> <p><code>--annotation</code> optional file with factor annotations to display instead of the factor IDs. It is a tsv file where the first column contains factor IDs as appear in the header of the pseudobulk table, and the second column contains the annotation.</p> <p><code>--anchor</code> optional file with anchor genes chosen to represent each factor. It is a tsv file where the first column contains factor IDs as appear in the header of the pseudobulk table, and the second column contains the anchor gene names (separated by things other than tabs).</p>"},{"location":"workflows/","title":"Pixel Level Factor Analysis Example","text":"<p>This example demonstrates how to perform pixel level factor analysis with punkst, achieving similar results to FICTURE (2024) with improved efficiency.</p> <p>We will explain how to generate a full workflow using a template Makefile, then explains the steps taken inside the workflow.</p>"},{"location":"workflows/#generic-input-format-and-example-data","title":"Generic input format and example data","text":"<p>There is a small example data <code>transcripts.tsv.gz</code> in <code>punkst/examples/data</code>.</p> <p>See Input for details on starting from raw data from different platforms.</p> <p>We only need one input file storing the pixel/transcript information: a TSV file with X coordinate, Y coordinate, feature, and count columns. (<code>punkst/examples/data/transcripts.tsv.gz</code>)</p> <p>It can be gzipped or uncompressed. It can have other columns (which will be ignored in analysis but can be optionally carried over to the pixel level output), and it may or may not have headers (see Step 1). We will only use this file directly in step 1.</p>"},{"location":"workflows/#use-the-example-makefile-template","title":"Use the example Makefile template","text":"<p>We provide template Makefile and config files in <code>punkst/examples</code> to generate the full workflows.</p>"},{"location":"workflows/#basic-workflow","title":"Basic workflow","text":"<p>You can copy <code>punkst/examples/basic/config.json</code> to your own directory and modify the data path and parameters, then use <code>punkst/ext/py/generate_workflow.py</code> to generate a data-specific Makefile for your task.</p> <p>The python script also generates a bash script that can be submitted as a slurm job. If you are not using slurm just ignore the parameters in the \"job\"  section of the config and run the generation script without the <code>-o</code> option.</p> <pre><code># set repopath to the path of the punkst repo\npython ${repopath}/ext/py/generate_workflow.py \\\n  -c config.json -o run.sh -m Makefile \\\n  -t ${repopath}/examples/basic/Makefile\n</code></pre> <p>You can check the generated workflow before execution by <pre><code>make -f Makefile --dry-run\n</code></pre></p> <p>Then <code>make -f Makefile</code> exectutes the workflow.</p>"},{"location":"workflows/#parameters-in-configjson","title":"Parameters in config.json","text":"<p>(The parameters in the example config file works for the example data, where the coordinates are in microns.)</p> <p><code>\"datadir\"</code>: the path to store all output</p> <p><code>\"tmpdir\"</code>: the path to store temporary files (those files will be deleted automatically by the program). This directory must be empty or creatable.</p> <p><code>\"transcripts\"</code>: a tsv file with X coordinate, Y coordinate, gene/transcript name, and count columns. There could be other columns but they will be ignored.</p> <p>Specify the 0-based column indices in \"transcripts\" for X coordinate, Y coordinate, feature, and count: <code>\"icol_x\"</code>, <code>\"icol_y\"</code>, <code>\"icol_feature\"</code>, and <code>\"icol_count\"</code>. If the input file contains headers, set \"skip\" to the number of lines to skip.</p> <p><code>\"exclude_feature_regex\"</code>: a regular expression to exclude features from the analysis. For example, to exclude negative control probes and/or mitochondrial genes.</p> <p><code>\"tilesize\"</code>: we store and process data by square tiles, this parameter specifies the size length of the tiles in the same unit as your coordinates. Tile sizes affect the memory usage and (perhaps less so) run time, we've been using 500\\(\\mu\\)m for all of our experiments.</p> <p><code>\"hexgrids\"</code> (list): this is center-to-center distance of the hexagonal grid used for training the model. The best value depends on your data density. We've been using \\(12\\sim 18\\mu m\\) for most dataset, but you might want to use a larger value if your data has very low transcript density.</p> <p><code>\"topics\"</code> (list): the number of topics (factors) to learn.</p> <p><code>\"pixhex\"</code>: often set to be the same as \"hexgrids\" or slightly smaller.</p> <p><code>\"nmove\"</code>: \"pixhex\" divided by \"nmove\" is the distance between adjacent anchor points in the algorithm. We recommend pixhex/nmove to be around \\(4~6\\mu m\\) for high resolution results.</p> <p><code>\"res\"</code>: the resolution for pixel level inference (pixels within this distance will be grouped together in inference). We've been using \\(0.5\\mu m\\).</p> <p><code>\"scale\"</code>: this only controls the visualization of pixel level results. The coordinate values divided by scale will be the \"pixel\" indices in the image. If your coordinates are in microns and you want \\(0.5 \\mu m\\) to be one pixel in the image, set scale to 0.5. For Visium HD where the data resolution is \\(2 \\mu m\\), you probably want to set scale to 2.</p> <p>Section <code>\"job\"</code>: only for slurm users. Those are just slurm job parameters to create a job script to wrap aroun the Makefile. You probably don't need this, just for convenience. You can include additional commands by setting \"extra_lines\".</p>"},{"location":"workflows/#step-by-step","title":"Step by step","text":""},{"location":"workflows/#setup","title":"Setup","text":"<p>First, set up the environment variables:</p> <pre><code>threads=4 # Number of threads for parallel processing\ntmpdir=/path/to/tmp # Directory for temporary files (must be empty or creatable)\npath=/path/to/your_data # Path to your data directory\n</code></pre>"},{"location":"workflows/#step-1-group-pixels-to-tiles","title":"Step 1: Group pixels to tiles","text":"<p>Group pixels into non-overlapping square tiles for faster processing:</p> <pre><code>punkst pts2tiles --in-tsv transcripts.tsv \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 1 \\\n  --tile-size 500 \\\n  --temp-dir ${tmpdir} --threads ${threads} \\\n  --out-prefix ${path}/transcripts.tiled\n</code></pre> <p>Key parameters:</p> <p><code>--icol-x</code>, <code>--icol-y</code>: Column indices for X and Y coordinates (0-based)</p> <p><code>--skip</code>: If your input file has a header, use <code>--skip 1</code> to skip the first (or more) lines</p> <p><code>--tile-size</code>: Size (side length) of the square tiles</p> <p>Detailed documentation for pts2tiles</p>"},{"location":"workflows/#step-2-create-hexagonal-units","title":"Step 2: Create hexagonal units","text":"<p>Group pixels into non-overlapping hexagons:</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --feature-dict ${path}/transcripts.tiled.features.tsv \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n  --min-count 20 --hex-size 7 \\\n  --out ${path}/hex_12.txt --randomize \\\n  --temp-dir ${tmpdir} --threads ${threads}\n</code></pre> <p>Key parameters:</p> <p><code>--icol-feature</code>, <code>--icol-int</code>: Column indices for feature and count(s)</p> <p><code>--hex-size</code>: Side length of the hexagons</p> <p><code>--min-count</code>: Minimum count for a hexagon to be included</p> <p><code>--randomize</code>: If set, the order of hexagons in the output will be randomized. You should always shuffle hexagons before running <code>topic-model</code>.</p> <p>Detailed documentation for tiles2hex</p>"},{"location":"workflows/#step-3-run-lda-on-hexagon-data","title":"Step 3: Run LDA on hexagon data","text":"<p>Perform Latent Dirichlet Allocation on the hexagon data:</p> <pre><code>punkst topic-model --in-data ${path}/hex_12.randomized.txt \\\n  --in-meta ${path}/hex_12.json \\\n  --n-topics 12 \\\n  --n-epochs 2 --min-count-train 50 \\\n  --out-prefix ${path}/hex_12 --transform \\\n  --threads ${threads} --seed 1\n</code></pre> <p>Key parameters:</p> <p><code>--n-topics</code>: Number of topics (factors) to learn</p> <p><code>--transform</code>: Generate transform results after model fitting</p> <p>Detailed documentation for topic-model</p>"},{"location":"workflows/#step-4-decode-pixels-with-the-model","title":"Step 4: Decode pixels with the model","text":"<p>Annotate each pixel with top factors and their probabilities:</p> <pre><code>punkst pixel-decode --model ${path}/hex_12.model.tsv \\\n  --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n  --hex-grid-dist 12 --n-moves 2 \\\n  --pixel-res 0.5 \\\n  --out-pref ${path}/pixel.decode \\\n  --temp-dir ${tmpdir} \\\n  --threads ${threads} --seed 1 --output-original\n</code></pre> <p>Key parameters:</p> <p><code>--model</code>: Model file created by <code>topic-model</code></p> <p><code>--hex-grid-dist</code>: Center-to-center distance of the hexagonal grid</p> <p><code>--n-moves</code>: Number of sliding moves to generate anchors</p> <p><code>--pixel-res</code>: Resolution for the analysis (in the same unit as coordinates)</p> <p><code>--output-original</code>: Write each transcript/input pixel as a separate line in the output. This will be slower and generates a bigger file, so only use it if matching the inference with the original input is useful. (Excluding this flag for Visium HD data is more sensible)</p> <p>Detailed documentation for pixel-decode</p>"},{"location":"workflows/#step-5-visualize-the-results","title":"Step 5: Visualize the results","text":"<p>Visualize the pixel decoding results:</p> <p>Optional: choose a color table based on the intermediate results. Otherwise, you need to create a RGB table with the following columns: R, G, B (including the header). So each row represents the RGB color for a factor, with integer values from 0 to 255. (Python dependency: jinja2, pandas, matplotlib.)</p> <pre><code>python punkst/ext/py/color_helper.py --input ${path}/hex_12.results.tsv --output ${path}/color\n</code></pre> <p>Generate an image for the pixel level factor assignment <pre><code>punkst draw-pixel-factors --in-tsv ${path}/pixel.decode.tsv \\\n  --in-color ${path}/color.rgb.tsv \\\n  --out ${path}/pixel.png \\\n  --scale 1 \\\n  --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre></p> <p>Key parameters:</p> <p><code>--in-color</code>: TSV file with RGB colors for each factor</p> <p><code>--scale</code>: Scales input coordinates to pixels in the output image (2 means 2 coordinate units = 1 pixel in the image). If the coordinates are in microns, 1 or 0.5 is suitable for high resolution data (imaging-based, Stereo-seq, Seq-scope, etc.); 2 is suitable for Visium HD.</p> <p><code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code>: Range of coordinates to visualize. If you specified <code>--icol-feature</code> and <code>--icol-int</code> in <code>pts2tiles</code>, you can either find the range in the <code>transcripts.tiled.coord_range.tsv</code> file or pass it directly with <code>--range transcripts.tiled.coord_range.tsv</code>.</p> <p>Compute naive differential expression statistics <pre><code>python punkst/ext/py/de_bulk.py --input ${path}/pixel.decode.pseudobulk.tsv \\\n  --output ${path}/de_bulk.tsv --thread ${threads}\n</code></pre></p> <p>Generate a html to display the color and top enriched genes for each factor <pre><code>python punkst/ext/py/factor_report.py --de ${path}/de_bulk.tsv \\\n  --pseudobulk ${path}/pixel.decode.pseudobulk.tsv \\\n  --color_table ${path}/color.rgb.tsv \\\n  --output_pref ${path}/report\n</code></pre></p> <p>Optional: add <code>--de_neighbor ${path}/de_bulk.1vsNeighbors.tsv</code> if you included <code>--neighbor-k</code> when running <code>punkst de-chisq</code> to display top \"highly specific\" genes that are enriched even when comparing with k most similar factors.</p> <p>Detailed documentation for visualization</p>"},{"location":"workflows/multisample/","title":"Multi-Sample Analysis Utilities","text":"<p>There are two main utilities for handling multiple transcriptomics inputs to help multi-sample analysis.</p> <p><code>multisample-prepare</code> processes raw data from multiple samples in a unified way. The output are ready for model training and later sample-specific pixel level projection. You also have the option to run only the first or the second step of the pipeline, see below for details.</p> <p><code>merge-units</code> merges multiple binned datasets, for example single cells or hexagons, in our customized sparse matrix format to a single dataset while harmonizing the sample-specific feature lists.</p> <p>(<code>pixel-decode</code> also allows processing multiple samples using the same model and parameters (see option <code>--sample-list</code> in <code>pixel-decode</code>) )</p>"},{"location":"workflows/multisample/#processing-multiple-samples-from-raw-data","title":"Processing multiple samples from raw data","text":"<p>The <code>multisample-prepare</code> command processes multiple raw spatial transcriptomics datasets into a merged hexagonal binned data suitable for joint model training (by <code>punkst topic-model</code>) and sample-specific tiled pixel level data for pixel level projection.</p> <p>It runs <code>pts2tiles</code> and <code>tiles2hex</code> for each sample then merge the binned level data.</p> <p>(if neither <code>--hex-grid-dist</code> nor <code>--hex-size</code> is provided, it only runs <code>pts2tiles</code>; if <code>--tiles2hex-only</code> is set (see the third section below) it only runs <code>tiles2hex</code> and merges the output files.)</p>"},{"location":"workflows/multisample/#usage","title":"Usage","text":"<pre><code>punkst multisample-prepare --in-tsv-list input_file_list.tsv \\\n    --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --skip 1 \\\n    --tile-size 500 \\\n    --min-total-count-per-sample 100 \\\n    --hex-grid-dist 12 --min-count 10 \\\n    --out-dir ./out --out-joint-pref merged \\\n    --temp-dir ./tmp --threads ${threads}\n</code></pre>"},{"location":"workflows/multisample/#input-file-list-in-tsv-list","title":"Input File List (<code>--in-tsv-list</code>)","text":"<p>The pipeline requires an input TSV file that lists the information for each sample to be processed. Each line should contain two tab-separated columns: 1.  A unique Sample ID. 2.  The path to the raw transcript file for that sample.</p> <p>The raw transcript file should be in the format expected by <code>pts2tiles</code>.</p> <p>Example <code>input_file_list.tsv</code>: <pre><code>sample_A    /path/to/sample_A_transcripts.tsv\nsample_B    /path/to/sample_B_transcripts.tsv\n</code></pre></p>"},{"location":"workflows/multisample/#key-options","title":"Key Options","text":"<p>(Other options are available, see <code>pts2tiles</code> and <code>tiles2hex</code> for details.)</p> <p><code>--in-tsv-list &lt;file&gt;</code>: (Required) Path to the input TSV file describing the samples.</p> <p><code>--min-total-count-per-sample &lt;int&gt;</code>: The minimum sample-specific total count a feature must have (across all samples) to be included in the merged file. Default: 1. Setting it to 0 to use the union of features across samples.</p> <p><code>--exclude-feature-regex</code>: Regular expression (modified ECMAScript grammar) to exclude features matching this pattern. Default: exclude no feature.</p> <p><code>--include-feature-regex</code>: Regular expression to include only features matching this pattern. Default: include all features. (e.g. to exclude features that contain \"Unassigned\" or \"NegControl\" as substrings in any part of the feature name, you can use <code>--exclude-feature-regex \".*(Unassigned|NegControl).*\"</code>)</p> <p><code>--threads &lt;int&gt;</code>: Number of threads to use. [Default: 1]</p> <p><code>--out-dir &lt;dir&gt;</code>: (Required) The base output directory where all results will be stored. Merged outputs will be placed directly under this directory, sample-specific outputs will be in subdirectories named <code>samples/[sample_id]/</code>.</p> <p><code>--out-joint-pref &lt;prefix&gt;</code>: (Required) A prefix for all merged output (the merged hexagon file and merged feature list). For example, the</p> <p><code>--temp-dir &lt;dir&gt;</code>: (Required) A directory for storing temporary files (will be created if it doesn't exist).</p> <p><code>--overwrite</code>: If set, overwrite existing sample-specific output files.</p> <p><code>pts2tiles</code> options:</p> <p><code>--icol-x &lt;int&gt;</code>, <code>--icol-y &lt;int&gt;</code>, <code>--icol-feature &lt;int&gt;</code>, <code>--icol-int &lt;int&gt;</code>: (Required) 0-based column indices for X/Y coordinates, the feature name, and the count/value.</p> <p><code>--skip</code>: If your input file has a header, use <code>--skip 1</code> to skip the first (or more) lines.</p> <p><code>--tile-size &lt;int&gt;</code>: (Required) The size of the square tiles for pre-processing. Should be big enough, say 500 microns.</p> <p><code>tiles2hex</code> options:</p> <p><code>--hex-grid-dist &lt;float&gt;</code>: The center-to-center distance for the hexagonal grid. Alternatively, provide <code>--hex-size &lt;float&gt;</code>, side length of the hexagons (exactly one of the two options must be provided). Multiple values can be provided, separated by spaces.</p> <p><code>--min-count &lt;int&gt;</code>: The minimum total count for a hexagon (unit) to be included in the output.</p>"},{"location":"workflows/multisample/#output-files","title":"Output Files","text":"<p>All outputs are under the specified <code>--out-dir</code></p> <p>In the main output directory (<code>--out-dir</code>):</p> <p><code>[--out-joint-pref].persample_file_list.tsv</code>: A list of paths to the tiled pixel level files for each sample. These are the input for <code>pixel-decode</code>.</p> <p><code>[--out-joint-pref].union_features.tsv</code>: A list of all features found in any of the samples, with total and sample-specific counts.</p> <p><code>[--out-joint-pref].features.tsv</code>: The final list of features used for the merged output.</p> <p><code>[--out-joint-pref].hex_[dist].txt</code> and <code>.json</code>: The final merged hexagon data and its corresponding metadata file, ready for <code>topic-model</code>.</p> <p>In per-sample subdirectories (<code>--out-dir/samples/[sample_id]/</code>):</p> <p>Intermediate tiled transcript files (<code>.tiled.tsv</code>, <code>.tiled.index</code>).</p> <p>Per-sample feature counts (<code>.tiled.features.tsv</code>).</p> <p>Per-sample randomized hexagon data (<code>.hex_[dist].txt</code>, <code>.hex_[dist].json</code>).</p>"},{"location":"workflows/multisample/#merge-hexagon-units-from-pre-processed-samples","title":"Merge hexagon units from pre-processed samples","text":"<p>The <code>merge-units</code> command merges multiple bin level data in the format of the output from <code>punkst tiles2hex</code>.</p> <p>The input files can have different extra information, as long as the metadata (<code>.json</code>) are recognized and includes a key <code>offset_data</code> indicating the starting index (0-based) of the sparsely coded count data.</p> <p>(In each row, tokens are separate by tabs. Starting from the index specified by <code>offset_data</code>, each row contains two integers for the number of unique features and the total count of all features, followed by feature_index and count (separated by a single space) pairs. See <code>tiles2hex</code> for more details.) (In each row, tokens are separate by tabs. Starting from the index specified by <code>offset_data</code>, each row contains two integers for the number of unique features and the total count of all features, followed by feature_index and count (separated by a single space) pairs. See <code>tiles2hex</code> for more details.)</p>"},{"location":"workflows/multisample/#usage_1","title":"Usage","text":"<p>This example merges two pre-processed samples.</p> <p>Two optional input specifications are demonstrated: It tells the tool to either use the existing random keys or generate new random keys based on the input data (<code>-2</code> on the 5-th column) and to carry over the data from column index (0-based) <code>4</code> of sample 1 and column <code>3</code> from sample 2 into the new \"info\" column.</p> <pre><code># Create the input specification\ninput_list=\"input.tsv\"\necho -e \"1\\t./1/1.tiled.features.tsv\\t./1/1.hex_12.txt\\t./1/1.hex_12.json\\t-2\\t4\" &gt; ${input_list}\necho -e \"2\\t./2/2.tiled.features.tsv\\t./2/2.hex_12.txt\\t./2/2.hex_12.json\\t-2\\t3\" &gt;&gt; ${input_list}\n\n# Run the merge command\npunkst merge-units \\\n    --in-list ${input_list} \\\n    --out-pref ./merged.hex_12 \\\n    --min-total-count-per-sample 100 \\\n    --temp-dir ./tmp --threads 4\n</code></pre>"},{"location":"workflows/multisample/#input-file-in-list","title":"Input File (<code>--in-list</code>)","text":"<p>The command requires a TSV file specifying the input for each sample. Each line must contain at least four columns:</p> <ol> <li> <p>Sample ID: A unique identifier for the sample.</p> </li> <li> <p>Features Path: Path to the sample-specific feature file. (TSV with feature name and count, like that created by <code>pts2tiles</code>. We only read the first two columns; lines where the second token is not a non-negative integer are ignored.)</p> </li> <li> <p>Hexagon Data Path: Path to the sample's hexagon data file (<code>.txt</code>).</p> </li> <li> <p>Hexagon Metadata Path: Path to the sample's hexagon metadata file (<code>.json</code>).</p> </li> <li> <p>Random Key Column Index (Optional): An integer specifying how to handle the random key for each unit.</p> <ul> <li> <p><code>&gt;= 0</code>: The column index in the input hexagon file to use as the random key for shuffling the merged output.</p> </li> <li> <p><code>-1</code>: Generate a new random key for each unit.</p> </li> <li> <p><code>-2</code>: Try to find the key column from the metadata (<code>.json</code>) file first. (Default)</p> </li> </ul> </li> <li> <p>Info Columns (Optional): A comma-delimited string of column indices (e.g., <code>2,5,6</code>) from the input hexagon file to carry over into a single \"info\" column in the merged output. Each sample can have different info columns. Put an \".\" to indicate no info columns to carry over for that sample.</p> </li> </ol> <p>Example <code>input.tsv</code>: <pre><code>1   /path/to/1/1.tiled.features.tsv /path/to/1/1.hex_12.txt /path/to/1/1.hex_12.json    0   4\n2   /path/to/2/2.tiled.features.tsv /path/to/2/2.hex_12.txt /path/to/2/2.hex_12.json    -2  3\n</code></pre></p>"},{"location":"workflows/multisample/#options","title":"Options","text":"<p><code>--in-list &lt;file&gt;</code>: (Required) Path to the input TSV file listing the pre-processed samples.</p> <p><code>--out-pref &lt;prefix&gt;</code>: (Required) Prefix for all output files (e.g., <code>/path/to/output/merged</code>).</p> <p><code>--temp-dir &lt;dir&gt;</code>: (Required) A directory for storing temporary files (will try to create it if it doesn't exist).</p> <p><code>--min-total-count-per-sample &lt;int&gt;</code>: Minimum per-sample count a feature must have across all samples to be included in the final merged feature set. (Default: 1. Set to 0 to use the union of features).</p> <p><code>--min-count &lt;int&gt;</code>: Minimum total count a unit/hexagon must have after feature filtering to be included in the merged output. [Default: 1]</p> <p><code>--threads &lt;int&gt;</code>: Number of threads. [Default: 1]</p>"},{"location":"workflows/multisample/#output-files_1","title":"Output Files","text":"<p><code>[--out-pref].txt</code> and <code>.json</code>: The merged hexagon data file and its corresponding metadata, ready for <code>topic-model</code>.</p> <p><code>[--out-pref].features.tsv</code>: The list of features and their total counts in the merged dataset.</p> <p><code>[--out-pref].union_features.tsv</code>: A list of all features found across all samples and their per-sample counts.</p>"},{"location":"workflows/multisample/#generate-sample-specific-and-merged-hexagons","title":"Generate sample-specific and merged hexagons","text":"<p>The <code>multisample-prepare</code> command has another mode to only perform the second step: run <code>tiles2hex</code> for each sample then merge the binned level data. This mode is activated by <code>--tiles2hex-only</code>, and a different input should be provided in <code>--in-tsv-list</code>. Most likely use case is when you have already run <code>pts2tiles</code> for each sample separately.</p>"},{"location":"workflows/multisample/#usage_2","title":"Usage","text":"<pre><code>punkst multisample-prepare --in-tsv-list input_file_list.tsv \\\n    --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n    --tiles2hex-only \\\n    --min-total-count-per-sample 100 \\\n    --hex-grid-dist 12 --min-count 10 \\\n    --out-dir ./out --out-joint-pref merged \\\n    --temp-dir ./tmp --threads ${threads} \\\n</code></pre>"},{"location":"workflows/multisample/#input-file-list-in-tsv-list_1","title":"Input File List (<code>--in-tsv-list</code>)","text":"<p>The pipeline requires an input TSV file that lists the information for each sample to be processed. Each line should contain two tab-separated columns: 1.  A unique Sample ID. 2.  The path to the tiled transcript file for that sample. 3.  The path to the corresponding index file. 4.  The path to the per-sample feature count file.</p> <p>The three input files per sample should be in the same formats as those output by <code>pts2tiles</code>.</p> <p>Example <code>input_file_list.tsv</code>: <pre><code>sample_A    /path/to/sample_A_transcripts.tiled.tsv /path/to/sample_A_transcripts.tiled.index   /path/to/sample_A.features.tsv\nsample_B    /path/to/sample_B_transcripts.tiled.tsv /path/to/sample_B_transcripts.tiled.index   /path/to/sample_B.features.tsv\n</code></pre></p>"}]}