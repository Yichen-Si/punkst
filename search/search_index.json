{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"punkst","text":""},{"location":"#pixel-level-factor-analysis","title":"Pixel level factor analysis","text":"<p>The punkst toolkit provides a pipeline for efficient pixel level factor analysis, which achieves the same result as FICTURE (2024) with improved efficiency. (Previous python package is still functioning.)</p> <p>Check the quick start page for help on generating the full workflow using Makefile or running each command step by step.</p> <p>Check the Input for help on preparing your data. Current we have worked exampled for 10X Genomics Visium HD, Xenium, NanoString CosMx SMI, and Vizgen MERSCOPE data. We've also applied punkst to Seq-scope and Stereo-seq data, we are working on providing more information.</p>"},{"location":"#workflow-overview","title":"Workflow Overview","text":"<ol> <li>pts2tiles: Group pixels to tiles for faster processing</li> <li>tiles2hex: Group pixels into non-overlapping hexagons</li> <li>lda4hex: Run factorization on the hexagon data</li> <li>pixel-decode: Annotate each pixel with the top factors and their probabilities</li> <li>Visualization: Create a high resolution visualization of the results</li> </ol> <p>All analyses are parallelized and some steps need to write temporary files. Choose the number of threads and the temporary directory (must be empty or can be created) to match your system capabilities.</p> <p>Check the Modules for detailed documentation of each command.</p>"},{"location":"basic/","title":"Pixel Level Factor Analysis Example","text":"<p>This example demonstrates how to perform pixel level factor analysis with punkst, achieving similar results to FICTURE (2024) with improved efficiency.</p> <p>We will explain how to generate a full workflow using a template Makefile, then explains the steps taken inside the workflow.</p>"},{"location":"basic/#generic-input-format-and-example-data","title":"Generic input format and example data","text":"<p>There is a small example data in <code>punkst/examples/data</code>. Decompress the the data to your local directory <code>tar xvzf example_data.tar.gz -C /path/to/parent/dir</code>.</p> <p>See Input for details on starting from raw data from different platforms.</p> <p>We need three pieces of information:</p> <ul> <li> <p>The main input file is a TSV file with X coordinate, Y coordinate, feature, and count columns. (<code>example_data/transcripts.tsv</code>) It can have other columns (which will be ignored), and it may or may not have headers (see Step 1). We will only use this file directly in step 1.</p> </li> <li> <p>A file containing a list of genes to use, one per line. If your file contains multiple columns, the first column delimited by any whitespace will be used. (<code>example_data/features.txt</code>)</p> </li> <li> <p>The coordinate range: xmin, xmax, ymin, ymax. (For the example data you can find them in <code>example_data/coordinate_minmax.tsv</code>)</p> </li> </ul>"},{"location":"basic/#use-the-example-makefile-template","title":"Use the example Makefile template","text":"<p>We provide a template Makefile and config file in <code>punkst/examples</code> to generate the full workflow of FICTURE.</p> <p>You can copy <code>punkst/examples/basic/generic/config.json</code> to your own directory modify the data path and parameters, then use <code>punkst/ext/py/generate_workflow.py</code> to generate a data-specific Makefile for your task.</p> <p>The python script also generates a bash script that can be submitted as a slurm job. If you are not using slurm just ignore the parameters in the \"job\"  section of the config and run the generation script without the <code>-o</code> option.</p> <pre><code># set repopath to the path of the punkst repo\npython ${repopath}/ext/py/generate_workflow.py \\\n  -c config.json -o run.sh -m Makefile \\\n  -t ${repopath}/examples/basic/generic/Makefile\n</code></pre> <p>You can check the generated workflow before execution by <pre><code>make -f Makefile --dry-run\n</code></pre></p> <p>Then <code>make -f Makefile</code> exectutes the workflow.</p>"},{"location":"basic/#parameters-in-configjson","title":"Parameters in config.json","text":"<p>(The parameters in the example config file works for the example data, where the coordinates are in microns.)</p> <ul> <li> <p><code>\"datadir\"</code>: the path to store all output</p> </li> <li> <p><code>\"tmpdir\"</code>: the path to store temporary files (those files will be deleted automatically by the program). This directory must be empty or creatable.</p> </li> <li> <p><code>\"transcripts\"</code>: a tsv file with X coordinate, Y coordinate, gene/transcript name, and count columns. There could be other columns but they will be ignored.</p> </li> <li> <p>Specify the 0-based column indices in \"transcripts\" for X coordinate, Y coordinate, feature, and count: <code>\"icol_x\"</code>, <code>\"icol_y\"</code>, <code>\"icol_feature\"</code>, and <code>\"icol_count\"</code>. If the input file contains headers, set \"skip\" to the number of lines to skip.</p> </li> <li> <p><code>\"features\"</code>: a file that contains a list of genes to use. Rows in the transcripts file where the gene name on the column specified by <code>icol_feature</code> does not match any names in this file will be ignored. (If your feature list contains multiple columns, the first column delimited by any whitespace will be used.)</p> </li> <li> <p><code>\"tilesize\"</code>: we store and process data by square tiles, this parameter specifies the size length of the tiles in the same unit as your coordinates. Tile sizes affect the memory usage and (perhaps less so) run time, we've been using 500\\(\\mu\\)m for all of our experiments.</p> </li> <li> <p><code>\"hexgrids\"</code> (list): this is center-to-center distance of the hexagonal grid used for training the model. The best value depends on your data density. We've been using 12~18\\(\\mu\\)m, but you might want to use a larger value if your data has very low transcript density.</p> </li> <li> <p><code>\"topics\"</code> (list): the number of topics (factors) to learn.</p> </li> <li> <p><code>\"pixhex\"</code>: often set to be the same as \"hexgrids\" or slightly smaller.</p> </li> <li> <p><code>\"nmove\"</code>: \"pixhex\" divided by \"nmove\" is the distance between adjacent anchor points in the algorithm. We recommend pixhex/nmove to be around \\(4~6\\mu m\\) for high resolution results.</p> </li> <li> <p><code>\"res\"</code>: the resolution for pixel level inference (pixels within this distance will be grouped together in inference). We've been using \\(0.5\\mu m\\).</p> </li> <li> <p><code>\"scale\"</code>: this only controls the visualization of pixel level results. The coordinate values divided by scale will be the \"pixel\" indices in the image. If your coordinates are in microns and you want \\(0.5 \\mu m\\) to be one pixel in the image, set scale to 0.5. For Visium HD where the data resolution is \\(2 \\mu m\\), you probably want to set scale to 2.</p> </li> <li> <p><code>\"xmin\", \"xmax\", \"ymin\", \"ymax\"</code>: the range of coordinates.</p> </li> <li> <p>Section <code>\"job\"</code>: only for slurm users. Those are just slurm job parameters to create a job script to wrap aroun the Makefile. You probably don't need this, just for convenience. You can include additional commands by setting \"extra_lines\".</p> </li> </ul>"},{"location":"basic/#step-by-step","title":"Step by step","text":""},{"location":"basic/#setup","title":"Setup","text":"<p>First, set up the environment variables:</p> <pre><code>threads=4 # Number of threads for parallel processing\ntmpdir=/path/to/tmp # Directory for temporary files (must be empty or creatable)\npath=/path/to/your_data # Path to your data directory\n</code></pre>"},{"location":"basic/#step-1-group-pixels-to-tiles","title":"Step 1: Group pixels to tiles","text":"<p>Group pixels into non-overlapping square tiles for faster processing:</p> <pre><code>punkst pts2tiles --in-tsv ${path}/transcripts.tsv \\\n  --icol-x 0 --icol-y 1 --skip 0 \\\n  --tile-size 500 \\\n  --temp-dir ${tmpdir} --threads ${threads} \\\n  --out-prefix ${path}/transcripts.tiled\n</code></pre> <p>Key parameters: - <code>--icol-x</code>, <code>--icol-y</code>: Column indices for X and Y coordinates (0-based) - If your input file has a header, use <code>--skip 1</code> to skip the first (or more) lines - <code>--tile-size</code>: Size (side length) of the square tiles</p> <p>Detailed documentation for pts2tiles</p>"},{"location":"basic/#step-2-create-hexagonal-units","title":"Step 2: Create hexagonal units","text":"<p>Group pixels into non-overlapping hexagons:</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --feature-dict ${path}/features.txt \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n  --min-count 20 --hex-size 7 \\\n  --out ${path}/hex.txt \\\n  --temp-dir ${tmpdir} --threads ${threads}\n</code></pre> <p>Key parameters: - <code>--icol-feature</code>, <code>--icol-int</code>: Column indices for feature and count(s) - <code>--hex-size</code>: Side length of the hexagons - <code>--min-count</code>: Minimum count for a hexagon to be included</p> <p>Shuffle the output for training: <pre><code>sort -k1,1 --parallel ${threads} ${path}/hex.txt &gt; ${path}/hex.randomized.txt\nrm ${path}/hex.txt\n</code></pre></p> <p>Detailed documentation for tiles2hex</p>"},{"location":"basic/#step-3-run-lda-on-hexagon-data","title":"Step 3: Run LDA on hexagon data","text":"<p>Perform Latent Dirichlet Allocation on the hexagon data:</p> <pre><code>punkst lda4hex --in-data ${path}/hex.randomized.txt \\\n  --in-meta ${path}/hex.json \\\n  --n-topics 12 \\\n  --n-epochs 2 --min-count-train 50 \\\n  --out-prefix ${path}/hex.lda --transform \\\n  --threads ${threads} --seed 1\n</code></pre> <p>Key parameters: - <code>--n-topics</code>: Number of topics (factors) to learn - <code>--transform</code>: Generate transform results after model fitting</p> <p>Detailed documentation for lda4hex</p>"},{"location":"basic/#step-4-decode-pixels-with-the-model","title":"Step 4: Decode pixels with the model","text":"<p>Annotate each pixel with top factors and their probabilities:</p> <pre><code>punkst pixel-decode --model ${path}/hex.lda.model.tsv \\\n  --in-tsv ${path}/transcripts.tiled.tsv \\\n  --in-index ${path}/transcripts.tiled.index \\\n  --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n  --hex-grid-dist 12 --n-moves 2 \\\n  --pixel-res 0.5 \\\n  --out-pref ${path}/pixel.decode \\\n  --temp-dir ${tmpdir} \\\n  --threads ${threads} --seed 1 --output-original\n</code></pre> <p>Key parameters: - <code>--model</code>: Model file created by lda4hex - <code>--hex-grid-dist</code>: Center-to-center distance of the hexagonal grid - <code>--n-moves</code>: Number of sliding moves to generate anchors - <code>--pixel-res</code>: Resolution for the analysis (in the same unit as coordinates) - <code>--output-original</code>: Write each transcript/input pixel as a separate line in the output. This will be slower and generates a bigger file, so only use it if matching the inference with the original input is useful. (Excluding this flag for Visium HD data is more sensible)</p> <p>Detailed documentation for pixel-decode</p>"},{"location":"basic/#step-5-visualize-the-results","title":"Step 5: Visualize the results","text":"<p>Visualize the pixel decoding results:</p> <p>Optional: choose a color table based on the intermediate results. Otherwise, you need to create a RGB table with the following columns: R, G, B (including the header). So each row represents the RGB color for a factor, with integer values from 0 to 255. (Python dependency: jinja2, pandas, matplotlib.)</p> <pre><code>python punkst/ext/py/color_helper.py --input ${path}/hex.lda.results.tsv --output ${path}/color\n</code></pre> <p>Generate an image for the pixel level factor assignment <pre><code>punkst draw-pixel-factors --in-tsv ${path}/pixel.decode.tsv \\\n  --in-color ${path}/color.rgb.tsv \\\n  --out ${path}/pixel.png \\\n  --scale 1 \\\n  --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre></p> <p>Key parameters: - <code>--in-color</code>: TSV file with RGB colors for each factor - <code>--scale</code>: Scales input coordinates to pixels in the output image (2 means 2 coordinate units = 1 pixel) - <code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code>: Range of coordinates to visualize</p> <p>Detailed documentation for visualization</p>"},{"location":"install/","title":"Install","text":""},{"location":"install/#installation-guide-for-punkst","title":"Installation Guide for punkst","text":"<p>This guide walks you through building punkst on Linux and macOS, including environments without root access.</p>"},{"location":"install/#build","title":"Build","text":"<p>Prerequisites</p> <ul> <li>Git</li> <li>CMake: 3.15 to 3.23</li> <li>C++17 compiler* (GCC \u22658, Clang \u22655, MSVC 2017+)</li> <li>TBB, OpenCV</li> </ul> <p>*We do assume your compiler properly supports C++17. Consider updating your compiler if you encounter issues.</p> <pre><code># 1) Clone the repository\ngit clone --recursive https://github.com/your-org/punkst.git\ncd punkst\n# 2) Create and enter a build directory\nmkdir build &amp;&amp; cd build\n# 3) Configure\ncmake ..\n# 4) Build\ncmake --build . --parallel # or make\n</code></pre> <p>If you did not clone the submodule (Eigen) initially, you can do <pre><code>git submodule update --init\n</code></pre></p> <p>If TBB is not found, you can either install it yourself (see below) or add a flag <code>cmake .. -DFETCH_TBB=ON</code> to let cmake fetch and build it from oneTBB (which will take a while).</p> <p>If you installed some dependencies locally, you may need to specify their paths like <pre><code>cmake .. \\\n  -DOpenCV_DIR=$HOME/.local/lib/cmake/opencv4 \\\n  -DCMAKE_PREFIX_PATH=\"$HOME/.local\"\n  ```\n(On mac, if CMake fails to locate OpenCV (installed with brew), pass: `-DOpenCV_DIR=$(brew --prefix opencv)/lib/cmake/opencv4` or wherever OpenCV is installed.)\n\nThe `punkst` binary will be placed in `bin/` under the project root.\n\nVerifying the Build\n\n```bash\npunkst/bin/punkst --help\n</code></pre></p> <p>You should see a message starting with <pre><code>Available Commands\nThe following commands are available:\n</code></pre></p>"},{"location":"install/#required-libraries","title":"Required Libraries","text":"<ul> <li>TBB</li> </ul> <p>System: <code>sudo apt-get install libtbb-dev</code> or <code>yum install tbb-devel</code> on linux and <code>brew install tbb</code> on macOS.</p> <p>Local: <code>git clone</code> from oneTBB then build locally.</p> <ul> <li>OpenCV</li> </ul> <p><code>sudo apt-get install libopencv-dev</code>or <code>sudo yum install opencv-devel</code> on linux, <code>brew install opencv</code> on macOS. See OpenCV installation guide for more details on how to install from source.</p> <ul> <li>Other dependencies</li> </ul> Library Ubuntu / Debian CentOS / RHEL macOS (Homebrew) zlib <code>sudo apt-get install zlib1g-dev</code> <code>sudo yum install zlib-devel</code> <code>brew install zlib</code> BZip2 <code>sudo apt-get install libbz2-dev</code> <code>sudo yum install bzip2-devel</code> <code>brew install bzip2</code> LibLZMA <code>sudo apt-get install liblzma-dev</code> <code>sudo yum install xz-devel</code> <code>brew install xz</code>"},{"location":"blog/","title":"Blog","text":""},{"location":"input/","title":"Notes on processing data from different technologies/platforms","text":"<p>Here are instructions on how to convert raw data from different platforms to the generic input format for FICTURE</p> <p>Except for Visium HD (see below), we provide a pair of template <code>Makefile</code> and <code>config_prepare.json</code> files in the <code>examples/format_input</code> directory for each platform. You can copy <code>config_prepare.json</code> to your directory and modify the parameters, then generate the concrete <code>Makefile</code> by <pre><code>python /path/to/punkst/ext/py/generate_workflow.py \\\n  -t /path/to/punkst/examples/format_input/cosmx/Makefile \\\n  -c config_prepare.json -m Makefile\n</code></pre> then run <code>make</code>.</p> <p>After the conversion, you can follow the standard workflow as described in the quick start page to run the pipeline (and specify all coordinate/size related parameters in microns). For platforms that provide cell coordinates, we also extracted the cell centers and you can try the experimental workflow in <code>examples/with_cell_centers</code>.</p>"},{"location":"input/#visium-hd","title":"Visium HD","text":"<p>First we need to locate the \"binned output\" directory and the subdirectory with the original resolution. For the data downloaded from 10X website it is called <code>binned_outputs/square_002um</code>. Let's call it <code>RAWDIR</code>. In this directory, you should find the subdirecotries, <code>spatial</code> and <code>filtered_feature_bc_matrix</code> (I guess you could also use data in <code>raw_feature_bc_matrix</code> but I've not tried it yet).</p> <p>In the <code>spatial</code> directory, you should have a json file that contains the scaling factor of the coordinates, named as <code>scalefactors_json.json</code>. Let's grep the scaling factor (or set it manually). <pre><code>microns_per_pixel=$(grep -w microns_per_pixel ${RAWDIR}/spatial/scalefactors_json.json | perl -lane '$_ =~ m/.*\"microns_per_pixel\": ([0-9.]+)/; print $1' )\n#  \"microns_per_pixel\": 0.2737129726047599 in an example data\n</code></pre></p> <p>The spatial coordinates for each barcode are stored in <code>parguet</code> format, we do not support this format directly. Let's convert it to plain tsv file. duckdb seems fast and easy to use:</p> <pre><code>cd ${RAWDIR}/spatial/\nduckdb -c \"COPY (SELECT * FROM read_parquet('tissue_positions.parquet')) TO 'tissue_positions.tsv' (HEADER, DELIMITER '\\t');\"\n</code></pre> <p>Alternatively, you can try to use pyarrow and pandas in python.</p> <p>Next, punkst has a command to merge the 10X style dge files (and the spatial coordinates) into a single file as our standard input:</p> <pre><code>brc_raw=${RAWDIR}/spatial/tissue_positions.tsv # the one converted from parquet\nmtx_path=${RAWDIR}/filtered_feature_bc_matrix # path to the 10X style dge files\npunkst convert-dge \\\n--microns-per-pixel ${microns_per_pixel} \\\n--exclude-regex '^mt-' --in-tissue-only \\\n--in-positions ${brc_raw} \\\n--in-dge-dir ${mtx_path} \\\n--output-dir ${path} \\\n--coords-precision 4\n</code></pre> <p>Here the optional flag <code>--exclude-regex</code> takes a regular expression to exclude genes matching with the regex. In the above example we exclude all mitochondrial genes.</p> <p>The optional flag <code>--in-tissue-only</code> will exclude all barcodes that are labeled as not in the tissue.</p> <p>It wrote the main data <code>transcripts.tsv</code>, a file <code>coordinate_minmax.tsv</code> containing the coordinate range (xmin, xmax, ymin, ymax), and <code>features.tsv</code> listing the gene names and their total counts. We probably want to filter out some very rare probs/genes by <code>awk '$2 &gt; 100' ${path}/features.tsv &gt; ${path}/features_min100.tsv</code>. The coordinates in <code>transcripts.tsv</code> and the range in <code>coordinate_minmax.tsv</code> are all in microns.</p>"},{"location":"input/#cosmx-smi","title":"CosMx SMI","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/cosmx</code> to conver CosMx raw output files to the generic input format. Copy the config file to your directory and set the raw file names. For example, here is an example for the public mouse half brain data:</p> <pre><code>{\n    \"workflow\": {\n      \"raw_tx\" : \"Run1000_S1_Half_tx_file.csv\",\n      \"raw_meta\": \"Run1000_S1_Half_metadata_file.csv\",\n      \"raw_mtx\": \"Run1000_S1_Half_exprMat_file.csv\",\n      \"microns_per_pixel\": 0.12,\n      \"datadir\": \"/output/test\"\n    }\n  }\n</code></pre> <p>You can find <code>\"microns_per_pixel\"</code> in the ReadMe.html, it may say something like \"To convert to microns multiply the pixel value by 0.12 um per pixel\".</p>"},{"location":"input/#merscope","title":"MERSCOPE","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/merscope</code> to conver MERSCOPE raw output files to the generic input format.</p> <p>Set <code>\"rawdir\"</code> to be the path that contains the MERSCOPE output files. We will need the following files: <code>cell_by_gene.csv.gz</code>, <code>cell_metadata.csv.gz</code>, and <code>detected_transcripts.csv.gz</code>. If your data is compressed, set <code>\"compressed\"</code> to 1, otherwise (plain csv) set it to 0. Set <code>\"datadir\"</code> to the output directory.</p>"},{"location":"input/#xenium","title":"Xenium","text":"<p>You can use the template <code>Makefile</code> and <code>config_prepare.json</code> in <code>punkst/examples/format_input/xenium</code> to conver Xenium raw output files to the generic input format.</p> <p>In the <code>config.json</code>, you need specify <code>\"raw_transcripts\"</code>, the path of the transcript file <code>transcripts.csv.gz</code>, and <code>\"rawdir\"</code> that contains the directory <code>cell_feature_matrix</code> (decompressed from <code>cell_feature_matrix.tar.gz</code>) and a cell metadata file <code>cells.csv.gz</code>.</p> <p>The following is what happens in the Makefile:</p> <p>Basic on public datasets in 10X data release, there will be a <code>transcripts.parquet.csv.gz</code> or <code>transcripts.csv.gz</code> file in the output bundle. We just need to decompress it and extract the columns we need: <code>x_location</code>, <code>y_location</code>, and <code>feature_name</code> (and add a dummy <code>count</code> as always 1).</p> <pre><code>cat transcripts.parquet.csv.gz | cut -d',' -f 4-6 | sed 's/\"//g' | awk -F',' -v OFS=$\"\\t\" ' {print $2, $3, $1, \"1\"} ' &gt; transcripts.tsv\n</code></pre> <p>Decompress <code>cell_feature_matrix.tar.gz</code>, you can find the list of genes in <code>cell_feature_matrix/features.tsv.gz</code>. Unfortunately we need the first column contains the gene names in the transcript file, so let's decompress and extract the second column (excluding the negative control probes): <pre><code>zcat cell_feature_matrix/features.tsv.gz | grep \"Gene Expression\" | cut -f 2 &gt; features.txt\n</code></pre></p> <p>If you don't know the range of the coordinates (needed in the standard Makefile workflow, though we only need that to make the final image), we could get it by parsing the cell metadata <code>cells.csv.gz</code> (just because it may be the smallest file with all coordinates): <pre><code>zcat cells.csv.gz | cut -d',' -f 2-3 | tail -n +2 | head -n 1000 | awk -F',' -v OFS=$\"\\t\" -v out=\"test_cell_centers.tsv\" -v range=\"test_coord_range.txt\" '\\\nNR==1{xmin=$1; xmax=$1; ymin=$2; ymax=$2} \\\n{if ($1 &lt; xmin) {xmin=$1}; if ($1 &gt; xmax) {xmax=$1}; \\\nif ($2 &lt; ymin) {ymin=$2}; if ($2 &gt; ymax) {ymax=$2}; \\\nprint $1, $2 &gt; out } \\\nEND{print \"XMIN:=\" xmin &gt; range; \\\nprint \"XMAX:=\" xmax &gt;&gt; range; \\\nprint \"YMIN:=\" ymin &gt;&gt; range; \\\nprint \"YMAX:=\" ymax &gt;&gt; range}'\n</code></pre></p>"},{"location":"modules/","title":"Punkst Modules","text":"<p>Punkst provides several command-line tools for analyzing high resolution spatial (transcriptomics) data. Each module can be used individually or as part of a pipeline.</p>"},{"location":"modules/#available-modules","title":"Available Modules","text":"<ul> <li>pts2tiles: Groups pixels to tiles for faster processing</li> <li>tiles2hex: Groups pixels into non-overlapping hexagons for spot level analysis</li> <li>lda4hex: Runs LDA on the spot level data</li> <li>pixel-decode: Annotates each pixel with the top factors and their probabilities</li> <li>Visualization: Visualizes the pixel level analysis results</li> </ul>"},{"location":"modules/#input-data-format","title":"Input Data Format","text":"<p>The input is a tsv file with the following columns: X, Y, feature, count. Whether the file contains headers or other columns is not relevant, as long as the above four columns are present.</p> <ul> <li> <p>X, Y coordinates can be either integer or float vlaues. (If your coordinates are integers and you would like to keep the original coordinates in the pixel level inference output, set <code>--coords-are-int</code> in <code>punkst pixel-decode</code>). The coordinates can be in arbitrary units, just make sure all scale/size related parameters you later provide should be in the same unit.</p> </li> <li> <p>\"feature\" can be a string or a nonnegative integer corresponding to the index in a feature list.</p> </li> <li> <p>\"count\" is a nonnegative integer. You could apply gene-specific non-negative real valued weights to the count later in analysis.</p> </li> </ul>"},{"location":"modules/lda4hex/","title":"lda4hex","text":"<p><code>lda4hex</code> runs LDA on the hexagon data.</p>"},{"location":"modules/lda4hex/#input-format","title":"Input format","text":"<p>The input data is a plain text file where each line containing the sparse encoding of the gene counts for one unit (hexagon, cell, etc.). The orders of the units should be randomized.</p> <p>If generated hexagon data from <code>tiles2hex</code> you probably don't need to know the following details.</p>"},{"location":"modules/lda4hex/#required-data","title":"Required data","text":"<p>The required structure of each line is as follows (entries are separated by tabs): - one integer (m) for the number of unique genes in this unit - one integer for the total count of all genes in this unit - followed by m pairs of integers, each pair consisting of a gene index (0-based) and the count of that gene in this unit (separated by a single space). In a cell-by-gene count matrix, the pairs are the (column, value) pairs of all non-zero entries in one row corresponding to a cell.</p> <p>There could be other fields in the input before the above required (m+2) fields, the number of data fields before the required fields should be specified under the key \"offset_data\" in the json metadata file.</p>"},{"location":"modules/lda4hex/#required-metadata","title":"Required metadata","text":"<p>We require a json file with at least the following information: - \"dictionary\": a dictionary that contains key: value pairs where each key is a gene name and each value is the corresponding index of that gene in the sparse encoding in the input data file. (You could skip this dictionary if you provides all and only the present genes' information in the order consistent with the indices (the column names and column sums in a cell-by-gene matrix) by <code>--features</code> in <code>lda2hex</code> (see below)) - \"offset_data\": an integer that specifies the number of fields before the required fields in the input data file. - \"header_info\": a list of size <code>offset_data</code> that contains the names of the fields before the required fields in the input data file. We will carry over these fields to the output files.</p>"},{"location":"modules/lda4hex/#usage","title":"Usage","text":"<pre><code>punkst lda4hex --in-data ${path}/hex_12.randomized.txt --in-meta ${path}/hex_12.json \\\n--n-topics 12 --out-prefix ${path}/hex_12 --transform \\\n--min-count-train 50 --minibatch-size 512 --threads ${threads} --seed 1\n</code></pre>"},{"location":"modules/lda4hex/#required","title":"Required","text":"<p><code>--in-data</code> - Specifies the input data file (created by <code>tiles2hex</code> then shuffled).</p> <p><code>--in-meta</code> - Specifies the metadata file created by <code>tiles2hex</code>.</p> <p><code>--n-topics</code> - Specifies the number of topics to learn.</p> <p><code>--out-prefix</code> - Specifies the prefix for the output files.</p>"},{"location":"modules/lda4hex/#optional","title":"Optional","text":""},{"location":"modules/lda4hex/#feature-filtering","title":"Feature Filtering","text":"<p><code>--features</code> - Required and used only when either of the following three parameters are specified. Path to a file where the first column contains gene names and the second column contains the total count of that gene.</p> <p><code>--min-count-per-feature</code> - Minimum total count for features to be included. Require <code>--features</code> to be specified. Default: 1.</p> <p><code>--include-feature-regex</code> - Regular expression (POSIX extended) to include only features matching this pattern. Default: include all features.</p> <p><code>--exclude-feature-regex</code> - Regular expression (POSIX extended) to exclude features matching this pattern. Default: exclude no features.</p> <p>Feature Selection Logic: the above three filters are applied jointly, so only genes with at least the minimum count, matching the include regex (if provided), and not matching the exclude regex (if provided) will be included in the model.</p>"},{"location":"modules/lda4hex/#feature-weighting","title":"Feature Weighting","text":"<p><code>--feature-weights</code> - Path to a file containing a weight for each gene. Format should be gene name (first column) and weight (second column). If the json metadata file does not contain a dictionary, the first column should be the gene index (0-based) instead.</p> <p><code>--default-weight</code> - Default weight for features not present in the weights file. Set to 0 to ignore features not in the weights file. Default: 1.0.</p>"},{"location":"modules/lda4hex/#lda-training-parameters","title":"LDA Training Parameters","text":"<p><code>--threads</code> - Number of threads to use. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p> <p><code>--minibatch-size</code> - Size of the minibatches to use during training. Default: 512.</p> <p><code>--min-count-train</code> - Minimum total count for a hexagon to be included in the training set. Default: 20.</p> <p><code>--n-epochs</code> - Number of epochs to train for. Default: 1.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in the e-step in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--max-iter</code> - Maximum number of iterations for each document. Default: 100.</p> <p><code>--kappa</code> - Learning decay parameter for online LDA. Default: 0.7.</p> <p><code>--tau0</code> - Learning offset parameter for online LDA. Default: 10.0.</p> <p><code>--alpha</code> - Document-topic prior. Default: 1/K (where K is the number of topics).</p> <p><code>--eta</code> - Topic-word prior. Default: 1/K (where K is the number of topics).</p>"},{"location":"modules/lda4hex/#model-initialization","title":"Model Initialization","text":"<p><code>--model-prior</code> - File that contains the initial model matrix.</p> <p><code>--prior-scale</code> - Scale the initial model matrix uniformly by this value. Default: use the matrix as is.</p>"},{"location":"modules/lda4hex/#output-control","title":"Output Control","text":"<p><code>--transform</code> - Transform the data to the LDA space after training. If set, an output file <code>&lt;prefix&gt;.results.tsv</code> will be created.</p> <p><code>--projection-only</code> - Transform the data using the prior model without further training. Implies <code>--transform</code>.</p> <p><code>--verbose</code> - Control the verbosity level of output messages.</p>"},{"location":"modules/pixel-decode/","title":"pixel-decode","text":""},{"location":"modules/pixel-decode/#overview","title":"Overview","text":"<p><code>pixel-decode</code> takes a trained LDA model and tiled pixel-level data to annotate each pixel with the top factors and their probabilities. This module enables spatial mapping of gene expression patterns at single-pixel resolution.</p> <pre><code>punkst pixel-decode --model ${path}/hex_12.model.tsv \\\n--in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--temp-dir ${tmpdir} --out-pref ${path}/pixel.decode \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 \\\n--hex-grid-dist 12 --n-moves 2 \\\n--pixel-res 0.5 --threads ${threads} --seed 1 --output-original\n</code></pre> <p>The pixel-level inference result (in this case <code>${path}/pixel.decode.tsv</code>) contains the coordinates and the inferred top factors and their posterior probabilities for each pixel. The module also creates a pseudobulk file (<code>${path}/pixel.decode.pseudobulk.tsv</code>) where each row is a gene and each column is a factor.</p>"},{"location":"modules/pixel-decode/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - Specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> - Specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - Specify the columns with X and Y coordinates (0-based).</p> <p><code>--icol-feature</code> - Specifies the column index for feature (0-based).</p> <p><code>--icol-val</code> - Specifies the column index for count/value (0-based).</p> <p><code>--model</code> - Specifies the model file where the first column contains feature names and the subsequent columns contain the parameters for each factor. The format should match that created by <code>lda4hex</code>.</p> <p><code>--temp-dir</code> - Specifies the directory to store temporary files.</p> <p>Output specification - One of these must be provided: - <code>--out</code> - Specifies the output file (for backward compatibility). - <code>--out-pref</code> - Specifies the output prefix for all output files.</p> <p>Hexagon grid parameters - One of these must be provided: - <code>--hex-size</code> - Specifies the size (side length) of the hexagons for initializing anchors. - <code>--hex-grid-dist</code> - Specifies center-to-center distance in the axial coordinate system used to place anchors. Equals <code>hex-size * sqrt(3)</code>.</p> <p>Anchor spacing parameters - One of these must be provided: - <code>--anchor-dist</code> - Specifies the distance between adjacent anchors. - <code>--n-moves</code> - Specifies the number of sliding moves in each axis to generate the anchors. If <code>--n-moves</code> is <code>n</code>, <code>anchor-dist</code> equals <code>hex-grid-dist</code> / <code>n</code>.</p>"},{"location":"modules/pixel-decode/#optional-parameters","title":"Optional Parameters","text":""},{"location":"modules/pixel-decode/#input-parameters","title":"Input Parameters","text":"<p><code>--coords-are-int</code> - If set, indicates that the coordinates are integers; otherwise, they are treated as floating point values.</p> <p><code>--feature-is-index</code> - If set, the values in <code>--icol-feature</code> are interpreted as feature indices. Otherwise, they are expected to be feature names.</p> <p><code>--feature-weights</code> - Specifies a file to weight each feature. The first column should contain the feature names, and the second column should contain the weights.</p> <p><code>--default-weight</code> - Specifies the default weight for features not present in the weights file (only if <code>--feature-weights</code> is specified). Default: 0.</p> <p><code>--anchor</code> - Specifies a file containing anchor points to use in addition to evenly spaced lattice points.</p>"},{"location":"modules/pixel-decode/#data-annotation-parameters","title":"Data Annotation Parameters","text":"<p><code>--ext-col-ints</code> - Additional integer columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-floats</code> - Additional float columns to carry over to the output file. Format: \"idx1:name1 idx2:name2 ...\" where 'idx' are 0-based column indices.</p> <p><code>--ext-col-strs</code> - Additional string columns to carry over to the output file. Format: \"idx1:name1:len1 idx2:name2:len2 ...\" where 'idx' are 0-based column indices and 'len' are maximum lengths of strings.</p>"},{"location":"modules/pixel-decode/#processing-parameters","title":"Processing Parameters","text":"<p><code>--pixel-res</code> - Resolution for the analysis, in the same unit as the input coordinates. Default: 1 (each pixel treated independently). Setting the resolution equivalent to 0.5-1\u03bcm is recommended, but it could be smaller if your data is very dense.</p> <p><code>--radius</code> - Specifies the radius within which to search for anchors. Default: <code>anchor-dist * 1.2</code>.</p> <p><code>--min-init-count</code> - Minimum total count within the hexagon around an anchor for it to be included. Filters out regions outside tissues with sparse noise. Default: 10.</p> <p><code>--mean-change-tol</code> - Tolerance for convergence in terms of the mean absolute change in the topic proportions of a document. Default: 1e-3.</p> <p><code>--threads</code> - Number of threads to use for parallel processing. Default: 1.</p> <p><code>--seed</code> - Random seed for reproducibility. If not set or \u22640, a random seed will be generated.</p>"},{"location":"modules/pixel-decode/#output-parameters","title":"Output Parameters","text":"<p><code>--output-original</code> - If set, the original data including the feature names and counts will be included in the output. If <code>pixel-res</code> is not 1 and <code>--output-original</code> is not set, the output contains results per collapsed pixel.</p> <p><code>--use-ticket-system</code> - If set, the order of pixels in the output file is deterministic across runs (though not necessarily the same as the input order). May incur a small performance penalty.</p> <p><code>--top-k</code> - Number of top factors to include in the output. Default: 3.</p> <p><code>--output-coord-digits</code> - Number of decimal digits to output for coordinates (only used if input coordinates are float or <code>--output-original</code> is not set). Default: 4.</p> <p><code>--output-prob-digits</code> - Number of decimal digits to output for probabilities. Default: 4.</p> <p><code>--verbose</code> - Increase verbosity of output messages.</p> <p><code>--debug</code> - Enable debug mode for additional diagnostic information.</p>"},{"location":"modules/pixel-decode/#output-files","title":"Output Files","text":"<p>When using <code>--out-pref</code>, the following files are generated: - <code>&lt;prefix&gt;.tsv</code> - Main output file with pixel-level factor assignments - <code>&lt;prefix&gt;.pseudobulk.tsv</code> - Gene-by-factor matrix showing feature distribution across topics</p>"},{"location":"modules/pixel-decode/#example-usage-scenarios","title":"Example Usage Scenarios","text":""},{"location":"modules/pixel-decode/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index --temp-dir /tmp --out-pref results --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 --hex-grid-dist 500 --n-moves 4 --threads 8\n</code></pre>"},{"location":"modules/pixel-decode/#with-data-annotations","title":"With Data Annotations","text":"<pre><code>punkst pixel-decode --model model.tsv --in-tsv data.tsv --in-index data.index --temp-dir /tmp --out-pref results --icol-x 0 --icol-y 1 --icol-feature 2 --icol-val 3 --hex-grid-dist 500 --n-moves 4 --ext-col-ints 4:celltype 5:cluster --ext-col-strs 6:sample_id:20 --output-original\n</code></pre>"},{"location":"modules/pts2tiles/","title":"pts2tiles","text":""},{"location":"modules/pts2tiles/#group-pixels-to-tiles-for-faster-processing","title":"Group pixels to tiles for faster processing","text":"<p><code>pts2tiles</code> creates a plain tsv file that reorders the lines in the input file so that coordinates are grouped into non-overlapping square tiles. The ordering of lines within a tile is not guaranteed. It also creates an index file storing the offset of each tile to support fast access.</p> <p>Example usage <pre><code>punkst pts2tiles --in-tsv ${path}/transcripts.tsv \\\n--icol-x 0 --icol-y 1 --skip 0 --tile-size 500 \\\n--temp-dir ${tmpdir} --out-prefix ${path}/transcripts.tiled --threads ${threads}\n</code></pre></p>"},{"location":"modules/pts2tiles/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> - The input TSV file containing spatial data points.</p> <p><code>--icol-x</code>, <code>--icol-y</code> - The column indices for X and Y coordinates (0-based).</p> <p><code>--tile-size</code> - The size (side length) of the square tiles. The unit is the same as the coordinates in the input file.</p> <p><code>--out-prefix</code> - The prefix for the output files.</p> <p><code>--temp-dir</code> - The directory for storing temporary files during processing.</p> <p><code>--icol-feature</code> - The column index for feature names/IDs (0-based). If provided, the module will generate a feature counts file. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features.)</p> <p><code>--icol-int</code> - Column indices for integer values to aggregate per feature. Can be specified multiple times to track multiple integer columns. (Not strictly required, but otherwise you will need to prepare your own list of (filtered) features, preferably excluding the extremely low count features.)</p>"},{"location":"modules/pts2tiles/#optional-parameters","title":"Optional Parameters","text":"<p><code>--skip</code> - The number of lines to skip in the input file (if your input file contains headers, set it to the number of header lines). Default: 0.</p> <p><code>--tile-buffer</code> - The per-thread per-tile buffer size in terms of the number of lines before writing to disk. Default: 1000. If the number of tiles may be huge and you are using a large number of threads so that the total memory usage is too high, choose a smaller number.</p> <p><code>--threads</code> - The number of threads to use for parallel processing. Default: 1.</p> <p><code>--verbose</code> - Controls the verbosity level of output messages.</p> <p><code>--debug</code> - Enables additional debug output.</p>"},{"location":"modules/pts2tiles/#output-files","title":"Output Files","text":"<ul> <li><code>${path}/transcripts.tiled.tsv</code>: the tiled tsv file.</li> <li><code>${path}/transcripts.tiled.index</code>: an index file that stores the offsets of each tile in the tiled tsv file. This will be used for fast access.</li> <li><code>${path}/coord_range.tsv</code>: a text file that contains the range of coordinates (xmin, xmax, ymin, ymax).</li> <li><code>${path}/features.tsv</code>: a tsv file containing the feature names and their aggregated values. This file is only generated if <code>--icol-feature</code> is specified.</li> </ul>"},{"location":"modules/tiles2hex/","title":"tiles2hex","text":"<p><code>tiles2hex</code> groups pixels into nonoverlapping hexagons for spot level analysis.</p> <p>The input is the tiled data created by <code>pts2tiles</code>. The output is a plain tab-delimited text file, each line representing one hexagon intended for internal use. It also writes metadata to a json file.</p>"},{"location":"modules/tiles2hex/#basic-usage","title":"Basic Usage","text":"<pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index ${path}/transcripts.tiled.index \\\n--feature-dict ${path}/features.txt \\\n--icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 \\\n--min-count 20 --hex-grid-dist 12 \\\n--out ${path}/hex_12.txt --temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#required-parameters","title":"Required Parameters","text":"<p><code>--in-tsv</code> specifies the tiled data created by <code>pts2tiles</code>.</p> <p><code>--in-index</code> specifies the index file created by <code>pts2tiles</code>.</p> <p><code>--icol-x</code>, <code>--icol-y</code>, <code>--icol-feature</code> specify the column indices corresponding to X and Y coordinates and feature (0-based).</p> <p><code>--icol-int</code> specifies the column index for count/value (0-based). You can specify multiple count columns with <code>--icol-int</code>, separated by space.</p> <p><code>--hex-size</code> specifies the side length of the hexagons. The unit is the same as the coordinates in the input file.</p> <p><code>--out</code> specifies the output file.</p>"},{"location":"modules/tiles2hex/#optional-parameters","title":"Optional Parameters","text":"<p><code>--feature-dict</code> specifies a file with the names of features, one per line. It is used only if the values in <code>--icol-feature</code> are to be interpreted as feature names not indices. Features not present in the file will be ignored. (If the input file contains feature indices instead of names, all features will be included in the output)</p> <p><code>--min-count</code> specifies the minimum count for a hexagon to be included in the output.</p> <p><code>--temp-dir</code> specifies the directory for temporary files.</p> <p><code>--threads</code> specifies the number of threads to use.</p>"},{"location":"modules/tiles2hex/#output-format","title":"Output Format","text":"<p>The output is a plain tab-delimited text file. It is not a table: each line contains data for one unit and lines have different number of tokens.</p> <p>The first element in each line of the output is a random key, which can be used to shuffle the data before model training (if you use <code>lda4hex</code> you should always do this):</p> <pre><code>sort -k1,1 --parallel ${threads} -S 1G ${path}/hex.txt &gt; ${path}/hex.randomized.txt\n</code></pre> <p>The remaining of each line is structured as follows:</p> <p>In the basic case, the next two integers after the random key are coordinates (horizontal and vertical) in the axial hexagonal coordinate system.</p> <p>The next 2K tokens (K pairs of non-negative integers) are the number of unique features (\\(M_k\\)) and the total count (\\(C_k\\)) for each modality. The number of modalities (K) is the same as the number of column indices specified in <code>--icol-int</code>.</p> <p>Then there are K chunks of feature values, the k-th chunk containing \\(2M_k\\) (or \\(M_k\\) values) of non-negative integers where \\(M_k\\) is what you read from the previous tokens. The first number in each pair is the indices of the feature, the second is the count of that feature in the hexagon. The indices are 0-based and correspond to the order of features in the <code>--feature-dict</code> file. If <code>--feature-dict</code> is not provided (so the input already codes features as indices), the indices are the same as those in the input file.</p>"},{"location":"modules/tiles2hex/#advanced-usage-spatial-stratification-by-anchor-points","title":"Advanced Usage: Spatial Stratification By Anchor Points","text":"<p><code>tiles2hex</code> can also create multiple sets of units that group pixels that are close to user-provided anchor points. This is useful for creating units stratified by known biological structures for downstream clustering or factorization. A tested use case is to provide nuclear centers as anchor points so likely-nuclear and likely-cytoplasmic pixels are grouped separately.</p> <pre><code>punkst tiles2hex --in-tsv ${path}/transcripts.tiled.tsv --in-index transcripts.tiled.index --feature-dict ${path}/features.txt --icol-x 0 --icol-y 1 --icol-feature 2 --icol-int 3 --min-count 20 --hex-size ${hex_size} --anchor-files ${path}/anchors1.txt ${path}/anchors2.txt --radius ${radius1} ${radius2} --out ${path}/hex.txt --temp-dir ${tmpdir} --threads ${threads}\n</code></pre>"},{"location":"modules/tiles2hex/#additional-parameters-for-anchor-based-analysis","title":"Additional Parameters for Anchor-Based Analysis","text":"<p><code>--anchor-files</code> specifies one or more files containing anchor points. Each anchor file should contain coordinates (x, y) separated by space, one anchor point per line. You can provide multiple anchor files to define different sets of anchor points, separated by space.</p> <p><code>--radius</code> specifies the radius around each anchor point within which pixels will be associated with that anchor. The unit is the same as the coordinates in the input file. You must provide one radius value for each anchor file, in the matched order.</p> <p><code>--ignore-background</code> if set, pixels that are not within the radius of any anchor point will be ignored. By default, these background pixels are included as a separate layer.</p>"},{"location":"modules/tiles2hex/#output-format-for-anchor-based-analysis","title":"Output Format for Anchor-Based Analysis","text":"<p>The output format is similar to the basic usage, but each hexagon also includes a non-negative index as the second token, indicating which anchor set it belongs to. The metadata JSON file includes an additional integer field <code>n_layers</code> recording the number of layers, or the number of anchor sets used (plus one if background is included).</p>"},{"location":"modules/visualization/","title":"Visualization","text":""},{"location":"modules/visualization/#draw-pixel-factors","title":"draw-pixel-factors","text":"<p><code>draw-pixel-factors</code> visualizes the results of <code>pixel-decode</code></p> <pre><code>punkst draw-pixel-factors --in-tsv ${path}/pixel.decode.tsv --header-json ${path}/pixel.decode.json --in-color ${path}/color.rgb.tsv --out ${path}/pixel.png --scale 100 --xmin ${xmin} --xmax ${xmax} --ymin ${ymin} --ymax ${ymax}\n</code></pre> <p><code>--in-tsv</code> specifies the input data file created by <code>pixel-decode</code>.</p> <p><code>--header-json</code> specifies the header created by <code>pixel-decode</code>.</p> <p><code>--in-color</code> specifies a tsv file with the colors for each factor. The first three columns will be interpreted as R, G, B values in the range \\(0-255\\). The valid lines will be assigned to factors in the order they appear in this file.</p> <p><code>--xmin</code>, <code>--xmax</code>, <code>--ymin</code>, <code>--ymax</code> specify the range of the coordinates.</p> <p><code>--scale</code> scales input coordinates to pixels in the output image. <code>int((x-xmin)/scale)</code> equals the horizontal pixel coordinate in the image.</p> <p><code>--out</code> specifies the output png file.</p> <p>If your specified <code>--transform</code> in <code>lda4hex</code>, one way to create the color table is to use the helper python script <pre><code>python punkst/ext/py/color_helper.py --input ${path}/prefix.results.tsv --output ${path}/color\n</code></pre></p>"}]}